{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66274d1",
   "metadata": {},
   "source": [
    "# Self Consistent Training Model For The Hybrid Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702be75",
   "metadata": {},
   "source": [
    "### Library imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6a2621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 12:23:47.763761: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-03 12:23:47.763781: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strawberry Fields: a Python library for continuous-variable quantum circuits.\n",
      "Copyright 2018-2020 Xanadu Quantum Technologies Inc.\n",
      "\n",
      "Python version:            3.10.4\n",
      "Platform info:             Linux-5.4.0-121-generic-x86_64-with-glibc2.31\n",
      "Installation path:         /home/bartu/miniconda3/envs/qml/lib/python3.10/site-packages/strawberryfields\n",
      "Strawberry Fields version: 0.23.0\n",
      "Numpy version:             1.22.4\n",
      "Scipy version:             1.8.1\n",
      "SymPy version:             1.10.1\n",
      "NetworkX version:          2.8.4\n",
      "The Walrus version:        0.19.0\n",
      "Blackbird version:         0.4.0\n",
      "XCC version:               0.2.1\n",
      "TensorFlow version:        2.9.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import strawberryfields as sf\n",
    "from strawberryfields import ops\n",
    "sf.about()\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 6), dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf3aef",
   "metadata": {},
   "source": [
    "### Training parameter definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5c5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of modes\n",
    "modes = 1\n",
    "\n",
    "# Cutoff dimension (number of Fock states)\n",
    "cutoff_dim = 3\n",
    "\n",
    "# Input vector length.\n",
    "input_len = 3\n",
    "\n",
    "# Number of layers (depth)\n",
    "Qlayers = 25\n",
    "\n",
    "# Number of steps in optimization routine performing gradient descent for the quantum decoder.\n",
    "reps = 300\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.05\n",
    "\n",
    "# Standard deviation of initial parameters\n",
    "passive_sd = 0.2\n",
    "active_sd = 0.01\n",
    "\n",
    "# The gamma parameter in the penalty function, given by the reference paper.\n",
    "norm_weight = 200\n",
    "\n",
    "# Seeds for the RNG functions to be able to reproduce results.\n",
    "tf.random.set_seed(155)\n",
    "np.random.seed(155)\n",
    "\n",
    "# Number of iterations to train the classical autoencoder.\n",
    "classical_epochs = 100\n",
    "# Number of feedback loop iterations.\n",
    "sctm_iterations = 7\n",
    "\n",
    "# Phase space circile restriciton radius.\n",
    "alpha_clip = 5\n",
    "\n",
    "save_file_name = str(input_len) + '_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5d28b",
   "metadata": {},
   "source": [
    "### Initialization of the classical encoder and decoder networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5d7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(input_len, activation =\"elu\"),\n",
    "      layers.Dense(5, activation=\"elu\"),\n",
    "      layers.Dense(5, activation=\"elu\"),\n",
    "      layers.Dense(4, activation=\"elu\"),\n",
    "      layers.Dense(3, activation=\"elu\"),\n",
    "      layers.Dense(latent_dim, activation=\"elu\"),\n",
    "    ])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "        layers.Dense(input_len, activation=\"elu\"),\n",
    "        layers.Dense(4, activation=\"elu\"),\n",
    "        layers.Dense(5, activation=\"elu\"),\n",
    "        layers.Dense(5, activation=\"elu\"),\n",
    "        layers.Dense(input_len, activation=\"elu\")\n",
    "    ])\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b5803",
   "metadata": {},
   "source": [
    "### Initialization of layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38596bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(modes, layers, active_sd=0.0001, passive_sd=0.1):\n",
    "    \"\"\"Initialize a 2D TensorFlow Variable containing normally-distributed\n",
    "    random weights for an ``N`` mode quantum neural network with ``L`` layers.\n",
    "\n",
    "    Args:\n",
    "        modes (int): the number of modes in the quantum neural network\n",
    "        layers (int): the number of layers in the quantum neural network\n",
    "        active_sd (float): the standard deviation used when initializing\n",
    "            the normally-distributed weights for the active parameters\n",
    "            (displacement, squeezing, and Kerr magnitude)\n",
    "        passive_sd (float): the standard deviation used when initializing\n",
    "            the normally-distributed weights for the passive parameters\n",
    "            (beamsplitter angles and all gate phases)\n",
    "\n",
    "    Returns:\n",
    "        tf.Variable[tf.float32]: A TensorFlow Variable of shape\n",
    "        ``[layers, 2*(max(1, modes-1) + modes**2 + modes)]``, where the Lth\n",
    "        row represents the layer parameters for the Lth layer.\n",
    "    \"\"\"\n",
    "    # Number of interferometer parameters:\n",
    "    M = int(modes * (modes - 1)) + max(1, modes - 1)\n",
    "\n",
    "    # Create the TensorFlow variables\n",
    "    int1_weights = tf.random.normal(shape=[layers, M], stddev=passive_sd)\n",
    "    s_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
    "    int2_weights = tf.random.normal(shape=[layers, M], stddev=passive_sd)\n",
    "    dr_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
    "    dp_weights = tf.random.normal(shape=[layers, modes], stddev=passive_sd)\n",
    "    k_weights = tf.random.normal(shape=[layers, modes], stddev=active_sd)\n",
    "\n",
    "    weights = tf.concat(\n",
    "        [int1_weights, s_weights, int2_weights, dr_weights, dp_weights, k_weights], axis=1\n",
    "    )\n",
    "\n",
    "    weights = tf.Variable(weights)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9278c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 12:23:50.758670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-03 12:23:50.758714: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-03 12:23:50.758747: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BartuDesktop): /proc/driver/nvidia/version does not exist\n",
      "2022-07-03 12:23:50.759179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b8c5a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target state for the training is chosen to be [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def GenerateTargetState(input_len, f):\n",
    "    state = np.zeros(input_len)\n",
    "    state[f] = 1.0\n",
    "    train = np.array([state])\n",
    "    return train, state\n",
    "    \n",
    "    \n",
    "x_train, target_state = GenerateTargetState(input_len, 0)\n",
    "print('The target state for the training is chosen to be ' + str(target_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0f231",
   "metadata": {},
   "source": [
    "### Compile the classical autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09be67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a80bd4",
   "metadata": {},
   "source": [
    "### Initialization of the feedback loop that contains the training of the classical and quantum layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c1899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 9.1365e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.7085e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9152e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9471e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.0367e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3486e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8349e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3082e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2258e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5870e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4351e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2435e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.2236e-06\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1589e-06\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3880e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7762e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0115e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.3112e-06\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3060e-06\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7823e-06\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0847e-06\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6408e-06\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7697e-06\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5320e-06\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6697e-06\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0036e-07\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6808e-06\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6734e-06\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5258e-06\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5926e-06\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9733e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0943e-06\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9168e-07\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.9086e-07\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4658e-06\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3425e-06\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8274e-07\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5024e-07\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1407e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0035e-07\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8265e-07\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0677e-06\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0225e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0590e-07\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5125e-07\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4756e-07\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7531e-07\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.1535e-07\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.4671e-07\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.4871e-07\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4665e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6495e-07\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9105e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9365e-07\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8970e-08\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3225e-07\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9459e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3925e-07\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9669e-07\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9689e-07\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.5177e-08\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4168e-08\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3792e-07\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0967e-07\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.7847e-08\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.1545e-08\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.6350e-08\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0410e-08\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5340e-08\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5585e-08\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2117e-08\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9020e-08\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9832e-08\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0539e-08\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3542e-08\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9000e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2423e-09\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6980e-08\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5249e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7812e-08\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0848e-08\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3408e-08\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.8146e-09\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2885e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8377e-08\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5229e-09\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.4606e-09\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0070e-08\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8392e-09\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5878e-09\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.8282e-09\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7404e-09\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0809e-09\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0069e-09\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0569e-09\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3629e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0753e-09\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1452e-09\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 4.1786e-09\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5277e-09\n",
      "[[-0.35088533 -0.51118636]]\n",
      "[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29437/3474690789.py:54: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 0 Cost: 1.2391 Fidelity: 0.0038 Trace: 0.0477\n",
      "Rep: 1 Cost: 1.1643 Fidelity: 0.0163 Trace: 0.0531\n",
      "Rep: 2 Cost: 0.9995 Fidelity: 0.0234 Trace: 0.0328\n",
      "Rep: 3 Cost: 0.8832 Fidelity: 0.0307 Trace: 0.0325\n",
      "Rep: 4 Cost: 0.9747 Fidelity: 0.0180 Trace: 0.0246\n",
      "Rep: 5 Cost: 0.9650 Fidelity: 0.0188 Trace: 0.0264\n",
      "Rep: 6 Cost: 0.8928 Fidelity: 0.0301 Trace: 0.0320\n",
      "Rep: 7 Cost: 0.8911 Fidelity: 0.0330 Trace: 0.0358\n",
      "Rep: 8 Cost: 0.8736 Fidelity: 0.0392 Trace: 0.0416\n",
      "Rep: 9 Cost: 0.8425 Fidelity: 0.0468 Trace: 0.0485\n",
      "Rep: 10 Cost: 0.8435 Fidelity: 0.0561 Trace: 0.0594\n",
      "Rep: 11 Cost: 0.8027 Fidelity: 0.0677 Trace: 0.0692\n",
      "Rep: 12 Cost: 0.7906 Fidelity: 0.0712 Trace: 0.0728\n",
      "Rep: 13 Cost: 0.7815 Fidelity: 0.0720 Trace: 0.0734\n",
      "Rep: 14 Cost: 0.7911 Fidelity: 0.0756 Trace: 0.0773\n",
      "Rep: 15 Cost: 0.7478 Fidelity: 0.0823 Trace: 0.0829\n",
      "Rep: 16 Cost: 0.7523 Fidelity: 0.0887 Trace: 0.0900\n",
      "Rep: 17 Cost: 0.7434 Fidelity: 0.0936 Trace: 0.0947\n",
      "Rep: 18 Cost: 0.7604 Fidelity: 0.0997 Trace: 0.1026\n",
      "Rep: 19 Cost: 0.6887 Fidelity: 0.1200 Trace: 0.1209\n",
      "Rep: 20 Cost: 0.7150 Fidelity: 0.1343 Trace: 0.1388\n",
      "Rep: 21 Cost: 0.7076 Fidelity: 0.1533 Trace: 0.1592\n",
      "Rep: 22 Cost: 0.6271 Fidelity: 0.1795 Trace: 0.1812\n",
      "Rep: 23 Cost: 0.7417 Fidelity: 0.1796 Trace: 0.1938\n",
      "Rep: 24 Cost: 0.5658 Fidelity: 0.2363 Trace: 0.2375\n",
      "Rep: 25 Cost: 0.6710 Fidelity: 0.2720 Trace: 0.2947\n",
      "Rep: 26 Cost: 0.5106 Fidelity: 0.3339 Trace: 0.3376\n",
      "Rep: 27 Cost: 0.6814 Fidelity: 0.3160 Trace: 0.3549\n",
      "Rep: 28 Cost: 0.6156 Fidelity: 0.3813 Trace: 0.4116\n",
      "Rep: 29 Cost: 0.4452 Fidelity: 0.4999 Trace: 0.5137\n",
      "Rep: 30 Cost: 0.4483 Fidelity: 0.5335 Trace: 0.5593\n",
      "Rep: 31 Cost: 0.3468 Fidelity: 0.5882 Trace: 0.5967\n",
      "Rep: 32 Cost: 0.3644 Fidelity: 0.6488 Trace: 0.6699\n",
      "Rep: 33 Cost: 0.3382 Fidelity: 0.7041 Trace: 0.7313\n",
      "Rep: 34 Cost: 0.2199 Fidelity: 0.7631 Trace: 0.7682\n",
      "Rep: 35 Cost: 0.4244 Fidelity: 0.6933 Trace: 0.7376\n",
      "Rep: 36 Cost: 0.3402 Fidelity: 0.7550 Trace: 0.7839\n",
      "Rep: 37 Cost: 0.2481 Fidelity: 0.8265 Trace: 0.8386\n",
      "Rep: 38 Cost: 0.3487 Fidelity: 0.8335 Trace: 0.8516\n",
      "Rep: 39 Cost: 0.2621 Fidelity: 0.8407 Trace: 0.8573\n",
      "Rep: 40 Cost: 0.2109 Fidelity: 0.8567 Trace: 0.8676\n",
      "Rep: 41 Cost: 0.2167 Fidelity: 0.8702 Trace: 0.8857\n",
      "Rep: 42 Cost: 0.2248 Fidelity: 0.8968 Trace: 0.9033\n",
      "Rep: 43 Cost: 0.2060 Fidelity: 0.8917 Trace: 0.8954\n",
      "Rep: 44 Cost: 0.2678 Fidelity: 0.9004 Trace: 0.9078\n",
      "Rep: 45 Cost: 0.2566 Fidelity: 0.8953 Trace: 0.9014\n",
      "Rep: 46 Cost: 0.1496 Fidelity: 0.9233 Trace: 0.9247\n",
      "Rep: 47 Cost: 0.2355 Fidelity: 0.9257 Trace: 0.9361\n",
      "Rep: 48 Cost: 0.1684 Fidelity: 0.9270 Trace: 0.9329\n",
      "Rep: 49 Cost: 0.2087 Fidelity: 0.9177 Trace: 0.9274\n",
      "Rep: 50 Cost: 0.1247 Fidelity: 0.9457 Trace: 0.9504\n",
      "Rep: 51 Cost: 0.1936 Fidelity: 0.9436 Trace: 0.9527\n",
      "Rep: 52 Cost: 0.1428 Fidelity: 0.9410 Trace: 0.9464\n",
      "Rep: 53 Cost: 0.1527 Fidelity: 0.9358 Trace: 0.9462\n",
      "Rep: 54 Cost: 0.1167 Fidelity: 0.9499 Trace: 0.9569\n",
      "Rep: 55 Cost: 0.1375 Fidelity: 0.9494 Trace: 0.9543\n",
      "Rep: 56 Cost: 0.1084 Fidelity: 0.9502 Trace: 0.9524\n",
      "Rep: 57 Cost: 0.1609 Fidelity: 0.9389 Trace: 0.9482\n",
      "Rep: 58 Cost: 0.1461 Fidelity: 0.9570 Trace: 0.9612\n",
      "Rep: 59 Cost: 0.1599 Fidelity: 0.9536 Trace: 0.9633\n",
      "Rep: 60 Cost: 0.2092 Fidelity: 0.9525 Trace: 0.9625\n",
      "Rep: 61 Cost: 0.1447 Fidelity: 0.9586 Trace: 0.9621\n",
      "Rep: 62 Cost: 0.1176 Fidelity: 0.9611 Trace: 0.9659\n",
      "Rep: 63 Cost: 0.1279 Fidelity: 0.9566 Trace: 0.9615\n",
      "Rep: 64 Cost: 0.1128 Fidelity: 0.9517 Trace: 0.9547\n",
      "Rep: 65 Cost: 0.1308 Fidelity: 0.9475 Trace: 0.9541\n",
      "Rep: 66 Cost: 0.1113 Fidelity: 0.9583 Trace: 0.9641\n",
      "Rep: 67 Cost: 0.1255 Fidelity: 0.9570 Trace: 0.9616\n",
      "Rep: 68 Cost: 0.1490 Fidelity: 0.9523 Trace: 0.9569\n",
      "Rep: 69 Cost: 0.0967 Fidelity: 0.9635 Trace: 0.9652\n",
      "Rep: 70 Cost: 0.1381 Fidelity: 0.9661 Trace: 0.9691\n",
      "Rep: 71 Cost: 0.1127 Fidelity: 0.9647 Trace: 0.9656\n",
      "Rep: 72 Cost: 0.1580 Fidelity: 0.9337 Trace: 0.9407\n",
      "Rep: 73 Cost: 0.1125 Fidelity: 0.9463 Trace: 0.9494\n",
      "Rep: 74 Cost: 0.1631 Fidelity: 0.9601 Trace: 0.9663\n",
      "Rep: 75 Cost: 0.1555 Fidelity: 0.9567 Trace: 0.9658\n",
      "Rep: 76 Cost: 0.1345 Fidelity: 0.9617 Trace: 0.9634\n",
      "Rep: 77 Cost: 0.1716 Fidelity: 0.9460 Trace: 0.9533\n",
      "Rep: 78 Cost: 0.1786 Fidelity: 0.9375 Trace: 0.9521\n",
      "Rep: 79 Cost: 0.1147 Fidelity: 0.9515 Trace: 0.9559\n",
      "Rep: 80 Cost: 0.1130 Fidelity: 0.9459 Trace: 0.9490\n",
      "Rep: 81 Cost: 0.1178 Fidelity: 0.9457 Trace: 0.9513\n",
      "Rep: 82 Cost: 0.1014 Fidelity: 0.9506 Trace: 0.9515\n",
      "Rep: 83 Cost: 0.1517 Fidelity: 0.9447 Trace: 0.9504\n",
      "Rep: 84 Cost: 0.1389 Fidelity: 0.9468 Trace: 0.9517\n",
      "Rep: 85 Cost: 0.1111 Fidelity: 0.9491 Trace: 0.9500\n",
      "Rep: 86 Cost: 0.1449 Fidelity: 0.9324 Trace: 0.9395\n",
      "Rep: 87 Cost: 0.1638 Fidelity: 0.9465 Trace: 0.9526\n",
      "Rep: 88 Cost: 0.0916 Fidelity: 0.9589 Trace: 0.9599\n",
      "Rep: 89 Cost: 0.1675 Fidelity: 0.9339 Trace: 0.9421\n",
      "Rep: 90 Cost: 0.1824 Fidelity: 0.9346 Trace: 0.9420\n",
      "Rep: 91 Cost: 0.0905 Fidelity: 0.9507 Trace: 0.9521\n",
      "Rep: 92 Cost: 0.1527 Fidelity: 0.9404 Trace: 0.9452\n",
      "Rep: 93 Cost: 0.1953 Fidelity: 0.9351 Trace: 0.9434\n",
      "Rep: 94 Cost: 0.1113 Fidelity: 0.9496 Trace: 0.9524\n",
      "Rep: 95 Cost: 0.1007 Fidelity: 0.9490 Trace: 0.9506\n",
      "Rep: 96 Cost: 0.1543 Fidelity: 0.9339 Trace: 0.9382\n",
      "Rep: 97 Cost: 0.1108 Fidelity: 0.9335 Trace: 0.9356\n",
      "Rep: 98 Cost: 0.0835 Fidelity: 0.9372 Trace: 0.9382\n",
      "Rep: 99 Cost: 0.1008 Fidelity: 0.9418 Trace: 0.9431\n",
      "Rep: 100 Cost: 0.0628 Fidelity: 0.9449 Trace: 0.9453\n",
      "Rep: 101 Cost: 0.1659 Fidelity: 0.9366 Trace: 0.9404\n",
      "Rep: 102 Cost: 0.0929 Fidelity: 0.9472 Trace: 0.9477\n",
      "Rep: 103 Cost: 0.2065 Fidelity: 0.9193 Trace: 0.9307\n",
      "Rep: 104 Cost: 0.2209 Fidelity: 0.9152 Trace: 0.9285\n",
      "Rep: 105 Cost: 0.1004 Fidelity: 0.9481 Trace: 0.9499\n",
      "Rep: 106 Cost: 0.1503 Fidelity: 0.9340 Trace: 0.9427\n",
      "Rep: 107 Cost: 0.1900 Fidelity: 0.9134 Trace: 0.9296\n",
      "Rep: 108 Cost: 0.1460 Fidelity: 0.9325 Trace: 0.9392\n",
      "Rep: 109 Cost: 0.0772 Fidelity: 0.9460 Trace: 0.9474\n",
      "Rep: 110 Cost: 0.1235 Fidelity: 0.9429 Trace: 0.9472\n",
      "Rep: 111 Cost: 0.0823 Fidelity: 0.9452 Trace: 0.9468\n",
      "Rep: 112 Cost: 0.0973 Fidelity: 0.9472 Trace: 0.9494\n",
      "Rep: 113 Cost: 0.0859 Fidelity: 0.9532 Trace: 0.9564\n",
      "Rep: 114 Cost: 0.0865 Fidelity: 0.9559 Trace: 0.9581\n",
      "Rep: 115 Cost: 0.1069 Fidelity: 0.9506 Trace: 0.9532\n",
      "Rep: 116 Cost: 0.0650 Fidelity: 0.9580 Trace: 0.9591\n",
      "Rep: 117 Cost: 0.1011 Fidelity: 0.9601 Trace: 0.9621\n",
      "Rep: 118 Cost: 0.0827 Fidelity: 0.9620 Trace: 0.9634\n",
      "Rep: 119 Cost: 0.0994 Fidelity: 0.9601 Trace: 0.9622\n",
      "Rep: 120 Cost: 0.0788 Fidelity: 0.9570 Trace: 0.9584\n",
      "Rep: 121 Cost: 0.0828 Fidelity: 0.9534 Trace: 0.9548\n",
      "Rep: 122 Cost: 0.0962 Fidelity: 0.9514 Trace: 0.9533\n",
      "Rep: 123 Cost: 0.0834 Fidelity: 0.9548 Trace: 0.9569\n",
      "Rep: 124 Cost: 0.0841 Fidelity: 0.9594 Trace: 0.9615\n",
      "Rep: 125 Cost: 0.0669 Fidelity: 0.9661 Trace: 0.9672\n",
      "Rep: 126 Cost: 0.0676 Fidelity: 0.9657 Trace: 0.9675\n",
      "Rep: 127 Cost: 0.0430 Fidelity: 0.9680 Trace: 0.9683\n",
      "Rep: 128 Cost: 0.1092 Fidelity: 0.9590 Trace: 0.9631\n",
      "Rep: 129 Cost: 0.1006 Fidelity: 0.9585 Trace: 0.9635\n",
      "Rep: 130 Cost: 0.0550 Fidelity: 0.9682 Trace: 0.9686\n",
      "Rep: 131 Cost: 0.1204 Fidelity: 0.9603 Trace: 0.9664\n",
      "Rep: 132 Cost: 0.1225 Fidelity: 0.9572 Trace: 0.9662\n",
      "Rep: 133 Cost: 0.1011 Fidelity: 0.9627 Trace: 0.9657\n",
      "Rep: 134 Cost: 0.0556 Fidelity: 0.9668 Trace: 0.9678\n",
      "Rep: 135 Cost: 0.1055 Fidelity: 0.9661 Trace: 0.9688\n",
      "Rep: 136 Cost: 0.0614 Fidelity: 0.9691 Trace: 0.9700\n",
      "Rep: 137 Cost: 0.1098 Fidelity: 0.9685 Trace: 0.9707\n",
      "Rep: 138 Cost: 0.0951 Fidelity: 0.9691 Trace: 0.9712\n",
      "Rep: 139 Cost: 0.0883 Fidelity: 0.9662 Trace: 0.9671\n",
      "Rep: 140 Cost: 0.1074 Fidelity: 0.9606 Trace: 0.9626\n",
      "Rep: 141 Cost: 0.0619 Fidelity: 0.9645 Trace: 0.9652\n",
      "Rep: 142 Cost: 0.1116 Fidelity: 0.9558 Trace: 0.9593\n",
      "Rep: 143 Cost: 0.1012 Fidelity: 0.9608 Trace: 0.9642\n",
      "Rep: 144 Cost: 0.0569 Fidelity: 0.9730 Trace: 0.9740\n",
      "Rep: 145 Cost: 0.0784 Fidelity: 0.9722 Trace: 0.9740\n",
      "Rep: 146 Cost: 0.0356 Fidelity: 0.9752 Trace: 0.9755\n",
      "Rep: 147 Cost: 0.0555 Fidelity: 0.9698 Trace: 0.9706\n",
      "Rep: 148 Cost: 0.0536 Fidelity: 0.9714 Trace: 0.9722\n",
      "Rep: 149 Cost: 0.0325 Fidelity: 0.9751 Trace: 0.9753\n",
      "Rep: 150 Cost: 0.0889 Fidelity: 0.9722 Trace: 0.9749\n",
      "Rep: 151 Cost: 0.0935 Fidelity: 0.9716 Trace: 0.9747\n",
      "Rep: 152 Cost: 0.0415 Fidelity: 0.9721 Trace: 0.9725\n",
      "Rep: 153 Cost: 0.0836 Fidelity: 0.9719 Trace: 0.9754\n",
      "Rep: 154 Cost: 0.1116 Fidelity: 0.9718 Trace: 0.9745\n",
      "Rep: 155 Cost: 0.0881 Fidelity: 0.9624 Trace: 0.9644\n",
      "Rep: 156 Cost: 0.0944 Fidelity: 0.9707 Trace: 0.9719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 157 Cost: 0.0899 Fidelity: 0.9757 Trace: 0.9765\n",
      "Rep: 158 Cost: 0.0489 Fidelity: 0.9760 Trace: 0.9766\n",
      "Rep: 159 Cost: 0.0939 Fidelity: 0.9661 Trace: 0.9686\n",
      "Rep: 160 Cost: 0.0749 Fidelity: 0.9689 Trace: 0.9705\n",
      "Rep: 161 Cost: 0.0887 Fidelity: 0.9746 Trace: 0.9773\n",
      "Rep: 162 Cost: 0.0941 Fidelity: 0.9698 Trace: 0.9731\n",
      "Rep: 163 Cost: 0.0528 Fidelity: 0.9755 Trace: 0.9761\n",
      "Rep: 164 Cost: 0.1038 Fidelity: 0.9736 Trace: 0.9772\n",
      "Rep: 165 Cost: 0.1158 Fidelity: 0.9696 Trace: 0.9761\n",
      "Rep: 166 Cost: 0.0865 Fidelity: 0.9738 Trace: 0.9758\n",
      "Rep: 167 Cost: 0.0781 Fidelity: 0.9720 Trace: 0.9752\n",
      "Rep: 168 Cost: 0.1202 Fidelity: 0.9663 Trace: 0.9745\n",
      "Rep: 169 Cost: 0.1025 Fidelity: 0.9748 Trace: 0.9790\n",
      "Rep: 170 Cost: 0.0363 Fidelity: 0.9771 Trace: 0.9774\n",
      "Rep: 171 Cost: 0.0860 Fidelity: 0.9784 Trace: 0.9790\n",
      "Rep: 172 Cost: 0.0717 Fidelity: 0.9772 Trace: 0.9774\n",
      "Rep: 173 Cost: 0.0863 Fidelity: 0.9701 Trace: 0.9729\n",
      "Rep: 174 Cost: 0.0539 Fidelity: 0.9772 Trace: 0.9776\n",
      "Rep: 175 Cost: 0.1372 Fidelity: 0.9617 Trace: 0.9678\n",
      "Rep: 176 Cost: 0.1372 Fidelity: 0.9590 Trace: 0.9664\n",
      "Rep: 177 Cost: 0.0644 Fidelity: 0.9778 Trace: 0.9782\n",
      "Rep: 178 Cost: 0.1310 Fidelity: 0.9632 Trace: 0.9692\n",
      "Rep: 179 Cost: 0.1400 Fidelity: 0.9582 Trace: 0.9679\n",
      "Rep: 180 Cost: 0.1172 Fidelity: 0.9682 Trace: 0.9723\n",
      "Rep: 181 Cost: 0.1036 Fidelity: 0.9631 Trace: 0.9659\n",
      "Rep: 182 Cost: 0.1149 Fidelity: 0.9629 Trace: 0.9673\n",
      "Rep: 183 Cost: 0.1136 Fidelity: 0.9666 Trace: 0.9709\n",
      "Rep: 184 Cost: 0.0508 Fidelity: 0.9639 Trace: 0.9644\n",
      "Rep: 185 Cost: 0.1442 Fidelity: 0.9389 Trace: 0.9456\n",
      "Rep: 186 Cost: 0.1519 Fidelity: 0.9312 Trace: 0.9412\n",
      "Rep: 187 Cost: 0.1169 Fidelity: 0.9580 Trace: 0.9616\n",
      "Rep: 188 Cost: 0.0717 Fidelity: 0.9712 Trace: 0.9725\n",
      "Rep: 189 Cost: 0.1200 Fidelity: 0.9650 Trace: 0.9679\n",
      "Rep: 190 Cost: 0.1116 Fidelity: 0.9684 Trace: 0.9698\n",
      "Rep: 191 Cost: 0.0608 Fidelity: 0.9708 Trace: 0.9715\n",
      "Rep: 192 Cost: 0.1228 Fidelity: 0.9658 Trace: 0.9668\n",
      "Rep: 193 Cost: 0.1463 Fidelity: 0.9623 Trace: 0.9630\n",
      "Rep: 194 Cost: 0.1204 Fidelity: 0.9648 Trace: 0.9661\n",
      "Rep: 195 Cost: 0.0584 Fidelity: 0.9652 Trace: 0.9661\n",
      "Rep: 196 Cost: 0.1031 Fidelity: 0.9576 Trace: 0.9588\n",
      "Rep: 197 Cost: 0.0827 Fidelity: 0.9478 Trace: 0.9488\n",
      "Rep: 198 Cost: 0.0649 Fidelity: 0.9479 Trace: 0.9485\n",
      "Rep: 199 Cost: 0.0488 Fidelity: 0.9591 Trace: 0.9592\n",
      "Rep: 200 Cost: 0.0762 Fidelity: 0.9602 Trace: 0.9618\n",
      "Rep: 201 Cost: 0.0614 Fidelity: 0.9681 Trace: 0.9686\n",
      "Rep: 202 Cost: 0.0960 Fidelity: 0.9641 Trace: 0.9691\n",
      "Rep: 203 Cost: 0.1227 Fidelity: 0.9624 Trace: 0.9693\n",
      "Rep: 204 Cost: 0.0620 Fidelity: 0.9704 Trace: 0.9718\n",
      "Rep: 205 Cost: 0.0900 Fidelity: 0.9603 Trace: 0.9634\n",
      "Rep: 206 Cost: 0.1121 Fidelity: 0.9547 Trace: 0.9607\n",
      "Rep: 207 Cost: 0.0759 Fidelity: 0.9632 Trace: 0.9652\n",
      "Rep: 208 Cost: 0.0775 Fidelity: 0.9662 Trace: 0.9682\n",
      "Rep: 209 Cost: 0.0917 Fidelity: 0.9652 Trace: 0.9687\n",
      "Rep: 210 Cost: 0.0640 Fidelity: 0.9609 Trace: 0.9620\n",
      "Rep: 211 Cost: 0.0777 Fidelity: 0.9597 Trace: 0.9617\n",
      "Rep: 212 Cost: 0.0799 Fidelity: 0.9658 Trace: 0.9691\n",
      "Rep: 213 Cost: 0.0615 Fidelity: 0.9696 Trace: 0.9705\n",
      "Rep: 214 Cost: 0.0969 Fidelity: 0.9685 Trace: 0.9723\n",
      "Rep: 215 Cost: 0.1009 Fidelity: 0.9677 Trace: 0.9740\n",
      "Rep: 216 Cost: 0.0849 Fidelity: 0.9689 Trace: 0.9716\n",
      "Rep: 217 Cost: 0.0797 Fidelity: 0.9681 Trace: 0.9700\n",
      "Rep: 218 Cost: 0.0998 Fidelity: 0.9685 Trace: 0.9721\n",
      "Rep: 219 Cost: 0.0594 Fidelity: 0.9705 Trace: 0.9720\n",
      "Rep: 220 Cost: 0.0634 Fidelity: 0.9729 Trace: 0.9743\n",
      "Rep: 221 Cost: 0.0780 Fidelity: 0.9728 Trace: 0.9754\n",
      "Rep: 222 Cost: 0.0548 Fidelity: 0.9757 Trace: 0.9767\n",
      "Rep: 223 Cost: 0.0622 Fidelity: 0.9739 Trace: 0.9751\n",
      "Rep: 224 Cost: 0.0638 Fidelity: 0.9745 Trace: 0.9761\n",
      "Rep: 225 Cost: 0.0585 Fidelity: 0.9779 Trace: 0.9781\n",
      "Rep: 226 Cost: 0.0843 Fidelity: 0.9725 Trace: 0.9748\n",
      "Rep: 227 Cost: 0.1063 Fidelity: 0.9733 Trace: 0.9746\n",
      "Rep: 228 Cost: 0.0961 Fidelity: 0.9781 Trace: 0.9788\n",
      "Rep: 229 Cost: 0.0492 Fidelity: 0.9759 Trace: 0.9768\n",
      "Rep: 230 Cost: 0.1104 Fidelity: 0.9773 Trace: 0.9782\n",
      "Rep: 231 Cost: 0.1318 Fidelity: 0.9758 Trace: 0.9771\n",
      "Rep: 232 Cost: 0.0777 Fidelity: 0.9781 Trace: 0.9792\n",
      "Rep: 233 Cost: 0.1091 Fidelity: 0.9753 Trace: 0.9764\n",
      "Rep: 234 Cost: 0.1340 Fidelity: 0.9727 Trace: 0.9735\n",
      "Rep: 235 Cost: 0.1145 Fidelity: 0.9716 Trace: 0.9725\n",
      "Rep: 236 Cost: 0.0525 Fidelity: 0.9745 Trace: 0.9750\n",
      "Rep: 237 Cost: 0.1039 Fidelity: 0.9766 Trace: 0.9781\n",
      "Rep: 238 Cost: 0.0709 Fidelity: 0.9773 Trace: 0.9788\n",
      "Rep: 239 Cost: 0.1046 Fidelity: 0.9744 Trace: 0.9753\n",
      "Rep: 240 Cost: 0.1063 Fidelity: 0.9750 Trace: 0.9762\n",
      "Rep: 241 Cost: 0.0665 Fidelity: 0.9739 Trace: 0.9752\n",
      "Rep: 242 Cost: 0.0898 Fidelity: 0.9754 Trace: 0.9757\n",
      "Rep: 243 Cost: 0.1237 Fidelity: 0.9752 Trace: 0.9763\n",
      "Rep: 244 Cost: 0.0858 Fidelity: 0.9711 Trace: 0.9718\n",
      "Rep: 245 Cost: 0.0905 Fidelity: 0.9694 Trace: 0.9712\n",
      "Rep: 246 Cost: 0.0879 Fidelity: 0.9749 Trace: 0.9758\n",
      "Rep: 247 Cost: 0.0667 Fidelity: 0.9719 Trace: 0.9726\n",
      "Rep: 248 Cost: 0.1034 Fidelity: 0.9732 Trace: 0.9744\n",
      "Rep: 249 Cost: 0.0885 Fidelity: 0.9761 Trace: 0.9764\n",
      "Rep: 250 Cost: 0.1047 Fidelity: 0.9678 Trace: 0.9724\n",
      "Rep: 251 Cost: 0.1544 Fidelity: 0.9671 Trace: 0.9736\n",
      "Rep: 252 Cost: 0.1102 Fidelity: 0.9746 Trace: 0.9759\n",
      "Rep: 253 Cost: 0.1012 Fidelity: 0.9663 Trace: 0.9703\n",
      "Rep: 254 Cost: 0.1520 Fidelity: 0.9645 Trace: 0.9706\n",
      "Rep: 255 Cost: 0.1141 Fidelity: 0.9663 Trace: 0.9693\n",
      "Rep: 256 Cost: 0.1206 Fidelity: 0.9577 Trace: 0.9617\n",
      "Rep: 257 Cost: 0.1053 Fidelity: 0.9691 Trace: 0.9720\n",
      "Rep: 258 Cost: 0.0957 Fidelity: 0.9693 Trace: 0.9722\n",
      "Rep: 259 Cost: 0.1052 Fidelity: 0.9612 Trace: 0.9645\n",
      "Rep: 260 Cost: 0.0684 Fidelity: 0.9730 Trace: 0.9737\n",
      "Rep: 261 Cost: 0.0733 Fidelity: 0.9714 Trace: 0.9728\n",
      "Rep: 262 Cost: 0.0844 Fidelity: 0.9692 Trace: 0.9701\n",
      "Rep: 263 Cost: 0.0680 Fidelity: 0.9736 Trace: 0.9741\n",
      "Rep: 264 Cost: 0.0669 Fidelity: 0.9751 Trace: 0.9766\n",
      "Rep: 265 Cost: 0.0697 Fidelity: 0.9734 Trace: 0.9743\n",
      "Rep: 266 Cost: 0.0870 Fidelity: 0.9703 Trace: 0.9721\n",
      "Rep: 267 Cost: 0.0499 Fidelity: 0.9761 Trace: 0.9767\n",
      "Rep: 268 Cost: 0.1054 Fidelity: 0.9636 Trace: 0.9666\n",
      "Rep: 269 Cost: 0.1118 Fidelity: 0.9598 Trace: 0.9630\n",
      "Rep: 270 Cost: 0.0487 Fidelity: 0.9704 Trace: 0.9710\n",
      "Rep: 271 Cost: 0.0757 Fidelity: 0.9712 Trace: 0.9730\n",
      "Rep: 272 Cost: 0.0663 Fidelity: 0.9703 Trace: 0.9716\n",
      "Rep: 273 Cost: 0.0637 Fidelity: 0.9745 Trace: 0.9762\n",
      "Rep: 274 Cost: 0.0525 Fidelity: 0.9717 Trace: 0.9727\n",
      "Rep: 275 Cost: 0.0598 Fidelity: 0.9705 Trace: 0.9719\n",
      "Rep: 276 Cost: 0.0732 Fidelity: 0.9737 Trace: 0.9755\n",
      "Rep: 277 Cost: 0.0575 Fidelity: 0.9750 Trace: 0.9760\n",
      "Rep: 278 Cost: 0.0541 Fidelity: 0.9736 Trace: 0.9747\n",
      "Rep: 279 Cost: 0.0590 Fidelity: 0.9678 Trace: 0.9687\n",
      "Rep: 280 Cost: 0.0520 Fidelity: 0.9729 Trace: 0.9738\n",
      "Rep: 281 Cost: 0.0621 Fidelity: 0.9761 Trace: 0.9773\n",
      "Rep: 282 Cost: 0.0447 Fidelity: 0.9758 Trace: 0.9762\n",
      "Rep: 283 Cost: 0.0428 Fidelity: 0.9750 Trace: 0.9755\n",
      "Rep: 284 Cost: 0.0426 Fidelity: 0.9772 Trace: 0.9779\n",
      "Rep: 285 Cost: 0.0340 Fidelity: 0.9781 Trace: 0.9784\n",
      "Rep: 286 Cost: 0.0627 Fidelity: 0.9749 Trace: 0.9763\n",
      "Rep: 287 Cost: 0.0608 Fidelity: 0.9755 Trace: 0.9767\n",
      "Rep: 288 Cost: 0.0397 Fidelity: 0.9787 Trace: 0.9791\n",
      "Rep: 289 Cost: 0.0649 Fidelity: 0.9761 Trace: 0.9775\n",
      "Rep: 290 Cost: 0.0491 Fidelity: 0.9788 Trace: 0.9793\n",
      "Rep: 291 Cost: 0.0623 Fidelity: 0.9771 Trace: 0.9788\n",
      "Rep: 292 Cost: 0.0775 Fidelity: 0.9767 Trace: 0.9783\n",
      "Rep: 293 Cost: 0.0413 Fidelity: 0.9786 Trace: 0.9791\n",
      "Rep: 294 Cost: 0.0625 Fidelity: 0.9805 Trace: 0.9814\n",
      "Rep: 295 Cost: 0.0430 Fidelity: 0.9806 Trace: 0.9811\n",
      "Rep: 296 Cost: 0.0596 Fidelity: 0.9765 Trace: 0.9779\n",
      "Rep: 297 Cost: 0.0585 Fidelity: 0.9799 Trace: 0.9809\n",
      "Rep: 298 Cost: 0.0635 Fidelity: 0.9765 Trace: 0.9775\n",
      "Rep: 299 Cost: 0.0360 Fidelity: 0.9813 Trace: 0.9817\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 3.2677e-05\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.3697e-07\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6811e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0811e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0534e-07\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0370e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5119e-05\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8066e-07\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4460e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1122e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2769e-07\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0288e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7077e-06\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7877e-07\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1302e-06\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5604e-06\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9289e-08\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9739e-06\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1480e-06\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4120e-07\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4405e-06\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8046e-06\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.8246e-08\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4497e-06\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9214e-06\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0066e-07\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7406e-06\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1175e-06\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.3301e-08\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2876e-06\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5759e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.2517e-08\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6580e-07\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1169e-06\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5369e-08\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4410e-07\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0552e-07\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9364e-08\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8410e-07\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4360e-07\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0797e-09\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.6666e-07\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7402e-07\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2834e-08\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.6863e-07\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3137e-07\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6275e-08\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9541e-07\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4250e-07\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9290e-08\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2732e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4058e-08\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8834e-08\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7240e-07\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5759e-08\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9002e-08\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2242e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0574e-08\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2896e-08\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3228e-08\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8850e-09\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.3270e-08\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9446e-08\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9835e-10\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8711e-08\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6735e-08\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0130e-09\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9745e-08\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0600e-08\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.5922e-09\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9531e-08\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9383e-09\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3370e-08\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8568e-08\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4331e-11\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4318e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.8245e-09\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2315e-09\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2762e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.5209e-09\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.1662e-09\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.4367e-09\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.2476e-10\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8746e-09\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4579e-09\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7755e-11\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2209e-09\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3373e-09\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.4386e-10\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.2111e-09\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.5996e-10\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8665e-09\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5750e-09\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5275e-11\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2346e-09\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0459e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6864e-10\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8585e-09\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0992e-10\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9378e-10\n",
      "[[-0.34921306 -0.5099324 ]]\n",
      "[1. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29437/3474690789.py:54: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 0 Cost: 1.2639 Fidelity: 0.0011 Trace: 0.0515\n",
      "Rep: 1 Cost: 1.1887 Fidelity: 0.0439 Trace: 0.0562\n",
      "Rep: 2 Cost: 1.4274 Fidelity: 0.0676 Trace: 0.1002\n",
      "Rep: 3 Cost: 1.3077 Fidelity: 0.0634 Trace: 0.0807\n",
      "Rep: 4 Cost: 0.9735 Fidelity: 0.0396 Trace: 0.0495\n",
      "Rep: 5 Cost: 1.0278 Fidelity: 0.0309 Trace: 0.0375\n",
      "Rep: 6 Cost: 1.0998 Fidelity: 0.0230 Trace: 0.0302\n",
      "Rep: 7 Cost: 0.9861 Fidelity: 0.0223 Trace: 0.0245\n",
      "Rep: 8 Cost: 0.9053 Fidelity: 0.0204 Trace: 0.0216\n",
      "Rep: 9 Cost: 0.9317 Fidelity: 0.0212 Trace: 0.0220\n",
      "Rep: 10 Cost: 0.9469 Fidelity: 0.0211 Trace: 0.0214\n",
      "Rep: 11 Cost: 0.9530 Fidelity: 0.0185 Trace: 0.0197\n",
      "Rep: 12 Cost: 0.9069 Fidelity: 0.0181 Trace: 0.0191\n",
      "Rep: 13 Cost: 0.9095 Fidelity: 0.0182 Trace: 0.0190\n",
      "Rep: 14 Cost: 0.9467 Fidelity: 0.0165 Trace: 0.0180\n",
      "Rep: 15 Cost: 0.9600 Fidelity: 0.0166 Trace: 0.0185\n",
      "Rep: 16 Cost: 0.9174 Fidelity: 0.0197 Trace: 0.0206\n",
      "Rep: 17 Cost: 0.9056 Fidelity: 0.0223 Trace: 0.0236\n",
      "Rep: 18 Cost: 0.8991 Fidelity: 0.0244 Trace: 0.0261\n",
      "Rep: 19 Cost: 0.8890 Fidelity: 0.0290 Trace: 0.0296\n",
      "Rep: 20 Cost: 0.8984 Fidelity: 0.0320 Trace: 0.0331\n",
      "Rep: 21 Cost: 0.8812 Fidelity: 0.0350 Trace: 0.0365\n",
      "Rep: 22 Cost: 0.8369 Fidelity: 0.0400 Trace: 0.0407\n",
      "Rep: 23 Cost: 0.8503 Fidelity: 0.0436 Trace: 0.0452\n",
      "Rep: 24 Cost: 0.8656 Fidelity: 0.0449 Trace: 0.0467\n",
      "Rep: 25 Cost: 0.8421 Fidelity: 0.0479 Trace: 0.0490\n",
      "Rep: 26 Cost: 0.8042 Fidelity: 0.0533 Trace: 0.0540\n",
      "Rep: 27 Cost: 0.8149 Fidelity: 0.0606 Trace: 0.0618\n",
      "Rep: 28 Cost: 0.8306 Fidelity: 0.0663 Trace: 0.0679\n",
      "Rep: 29 Cost: 0.7531 Fidelity: 0.0697 Trace: 0.0698\n",
      "Rep: 30 Cost: 0.8095 Fidelity: 0.0660 Trace: 0.0680\n",
      "Rep: 31 Cost: 0.7814 Fidelity: 0.0725 Trace: 0.0736\n",
      "Rep: 32 Cost: 0.7748 Fidelity: 0.0787 Trace: 0.0801\n",
      "Rep: 33 Cost: 0.7742 Fidelity: 0.0882 Trace: 0.0899\n",
      "Rep: 34 Cost: 0.7442 Fidelity: 0.0941 Trace: 0.0953\n",
      "Rep: 35 Cost: 0.7355 Fidelity: 0.0952 Trace: 0.0961\n",
      "Rep: 36 Cost: 0.7419 Fidelity: 0.0998 Trace: 0.1018\n",
      "Rep: 37 Cost: 0.7414 Fidelity: 0.1046 Trace: 0.1070\n",
      "Rep: 38 Cost: 0.6911 Fidelity: 0.1132 Trace: 0.1134\n",
      "Rep: 39 Cost: 0.6892 Fidelity: 0.1135 Trace: 0.1138\n",
      "Rep: 40 Cost: 0.7136 Fidelity: 0.1148 Trace: 0.1166\n",
      "Rep: 41 Cost: 0.6940 Fidelity: 0.1188 Trace: 0.1199\n",
      "Rep: 42 Cost: 0.7078 Fidelity: 0.1185 Trace: 0.1198\n",
      "Rep: 43 Cost: 0.6955 Fidelity: 0.1258 Trace: 0.1268\n",
      "Rep: 44 Cost: 0.7078 Fidelity: 0.1315 Trace: 0.1342\n",
      "Rep: 45 Cost: 0.7117 Fidelity: 0.1294 Trace: 0.1328\n",
      "Rep: 46 Cost: 0.6699 Fidelity: 0.1313 Trace: 0.1318\n",
      "Rep: 47 Cost: 0.7231 Fidelity: 0.1307 Trace: 0.1350\n",
      "Rep: 48 Cost: 0.7289 Fidelity: 0.1388 Trace: 0.1439\n",
      "Rep: 49 Cost: 0.6608 Fidelity: 0.1491 Trace: 0.1499\n",
      "Rep: 50 Cost: 0.6994 Fidelity: 0.1466 Trace: 0.1506\n",
      "Rep: 51 Cost: 0.7064 Fidelity: 0.1486 Trace: 0.1540\n",
      "Rep: 52 Cost: 0.6558 Fidelity: 0.1638 Trace: 0.1659\n",
      "Rep: 53 Cost: 0.6512 Fidelity: 0.1757 Trace: 0.1763\n",
      "Rep: 54 Cost: 0.6236 Fidelity: 0.1807 Trace: 0.1815\n",
      "Rep: 55 Cost: 0.6076 Fidelity: 0.1803 Trace: 0.1806\n",
      "Rep: 56 Cost: 0.6373 Fidelity: 0.1851 Trace: 0.1870\n",
      "Rep: 57 Cost: 0.5749 Fidelity: 0.2016 Trace: 0.2019\n",
      "Rep: 58 Cost: 0.6204 Fidelity: 0.2230 Trace: 0.2249\n",
      "Rep: 59 Cost: 0.5695 Fidelity: 0.2207 Trace: 0.2215\n",
      "Rep: 60 Cost: 0.6740 Fidelity: 0.2234 Trace: 0.2318\n",
      "Rep: 61 Cost: 0.6011 Fidelity: 0.2521 Trace: 0.2588\n",
      "Rep: 62 Cost: 0.5530 Fidelity: 0.2753 Trace: 0.2757\n",
      "Rep: 63 Cost: 0.5373 Fidelity: 0.2915 Trace: 0.2943\n",
      "Rep: 64 Cost: 0.4927 Fidelity: 0.3159 Trace: 0.3166\n",
      "Rep: 65 Cost: 0.5518 Fidelity: 0.3359 Trace: 0.3442\n",
      "Rep: 66 Cost: 0.4833 Fidelity: 0.3718 Trace: 0.3785\n",
      "Rep: 67 Cost: 0.4642 Fidelity: 0.4050 Trace: 0.4083\n",
      "Rep: 68 Cost: 0.4467 Fidelity: 0.4208 Trace: 0.4254\n",
      "Rep: 69 Cost: 0.3757 Fidelity: 0.4718 Trace: 0.4741\n",
      "Rep: 70 Cost: 0.3962 Fidelity: 0.5326 Trace: 0.5356\n",
      "Rep: 71 Cost: 0.3165 Fidelity: 0.5616 Trace: 0.5624\n",
      "Rep: 72 Cost: 0.3669 Fidelity: 0.5883 Trace: 0.5997\n",
      "Rep: 73 Cost: 0.2545 Fidelity: 0.6589 Trace: 0.6602\n",
      "Rep: 74 Cost: 0.5007 Fidelity: 0.6187 Trace: 0.6495\n",
      "Rep: 75 Cost: 0.5052 Fidelity: 0.6289 Trace: 0.6759\n",
      "Rep: 76 Cost: 0.3187 Fidelity: 0.7188 Trace: 0.7316\n",
      "Rep: 77 Cost: 0.4069 Fidelity: 0.7008 Trace: 0.7162\n",
      "Rep: 78 Cost: 0.4423 Fidelity: 0.6595 Trace: 0.6926\n",
      "Rep: 79 Cost: 0.2954 Fidelity: 0.7322 Trace: 0.7392\n",
      "Rep: 80 Cost: 0.3596 Fidelity: 0.7438 Trace: 0.7671\n",
      "Rep: 81 Cost: 0.4370 Fidelity: 0.7152 Trace: 0.7662\n",
      "Rep: 82 Cost: 0.3622 Fidelity: 0.7528 Trace: 0.7829\n",
      "Rep: 83 Cost: 0.2151 Fidelity: 0.7946 Trace: 0.7974\n",
      "Rep: 84 Cost: 0.3382 Fidelity: 0.7675 Trace: 0.7888\n",
      "Rep: 85 Cost: 0.3537 Fidelity: 0.7640 Trace: 0.7930\n",
      "Rep: 86 Cost: 0.2410 Fidelity: 0.8012 Trace: 0.8096\n",
      "Rep: 87 Cost: 0.2470 Fidelity: 0.7966 Trace: 0.8040\n",
      "Rep: 88 Cost: 0.2931 Fidelity: 0.7917 Trace: 0.8095\n",
      "Rep: 89 Cost: 0.2290 Fidelity: 0.8301 Trace: 0.8403\n",
      "Rep: 90 Cost: 0.2053 Fidelity: 0.8440 Trace: 0.8488\n",
      "Rep: 91 Cost: 0.2314 Fidelity: 0.8012 Trace: 0.8066\n",
      "Rep: 92 Cost: 0.2336 Fidelity: 0.7899 Trace: 0.7965\n",
      "Rep: 93 Cost: 0.1618 Fidelity: 0.8432 Trace: 0.8441\n",
      "Rep: 94 Cost: 0.2180 Fidelity: 0.8518 Trace: 0.8622\n",
      "Rep: 95 Cost: 0.2558 Fidelity: 0.8372 Trace: 0.8500\n",
      "Rep: 96 Cost: 0.1992 Fidelity: 0.8442 Trace: 0.8479\n",
      "Rep: 97 Cost: 0.2295 Fidelity: 0.8497 Trace: 0.8605\n",
      "Rep: 98 Cost: 0.2057 Fidelity: 0.8611 Trace: 0.8708\n",
      "Rep: 99 Cost: 0.1462 Fidelity: 0.8693 Trace: 0.8703\n",
      "Rep: 100 Cost: 0.2192 Fidelity: 0.8474 Trace: 0.8553\n",
      "Rep: 101 Cost: 0.1869 Fidelity: 0.8584 Trace: 0.8666\n",
      "Rep: 102 Cost: 0.1773 Fidelity: 0.8863 Trace: 0.8893\n",
      "Rep: 103 Cost: 0.1391 Fidelity: 0.8832 Trace: 0.8862\n",
      "Rep: 104 Cost: 0.1854 Fidelity: 0.8836 Trace: 0.8876\n",
      "Rep: 105 Cost: 0.1813 Fidelity: 0.8932 Trace: 0.9008\n",
      "Rep: 106 Cost: 0.0991 Fidelity: 0.9005 Trace: 0.9020\n",
      "Rep: 107 Cost: 0.1929 Fidelity: 0.8858 Trace: 0.8996\n",
      "Rep: 108 Cost: 0.2517 Fidelity: 0.8775 Trace: 0.8991\n",
      "Rep: 109 Cost: 0.2063 Fidelity: 0.8783 Trace: 0.8896\n",
      "Rep: 110 Cost: 0.1534 Fidelity: 0.8844 Trace: 0.8891\n",
      "Rep: 111 Cost: 0.1752 Fidelity: 0.9092 Trace: 0.9179\n",
      "Rep: 112 Cost: 0.1342 Fidelity: 0.9132 Trace: 0.9153\n",
      "Rep: 113 Cost: 0.1953 Fidelity: 0.8737 Trace: 0.8828\n",
      "Rep: 114 Cost: 0.1711 Fidelity: 0.8950 Trace: 0.9025\n",
      "Rep: 115 Cost: 0.1260 Fidelity: 0.9304 Trace: 0.9322\n",
      "Rep: 116 Cost: 0.1065 Fidelity: 0.9258 Trace: 0.9273\n",
      "Rep: 117 Cost: 0.1623 Fidelity: 0.9175 Trace: 0.9252\n",
      "Rep: 118 Cost: 0.1691 Fidelity: 0.9221 Trace: 0.9313\n",
      "Rep: 119 Cost: 0.0980 Fidelity: 0.9296 Trace: 0.9328\n",
      "Rep: 120 Cost: 0.1295 Fidelity: 0.9352 Trace: 0.9404\n",
      "Rep: 121 Cost: 0.1170 Fidelity: 0.9380 Trace: 0.9405\n",
      "Rep: 122 Cost: 0.1471 Fidelity: 0.9325 Trace: 0.9363\n",
      "Rep: 123 Cost: 0.1021 Fidelity: 0.9381 Trace: 0.9409\n",
      "Rep: 124 Cost: 0.0950 Fidelity: 0.9442 Trace: 0.9467\n",
      "Rep: 125 Cost: 0.1209 Fidelity: 0.9339 Trace: 0.9369\n",
      "Rep: 126 Cost: 0.1149 Fidelity: 0.9490 Trace: 0.9511\n",
      "Rep: 127 Cost: 0.0788 Fidelity: 0.9443 Trace: 0.9457\n",
      "Rep: 128 Cost: 0.0838 Fidelity: 0.9371 Trace: 0.9383\n",
      "Rep: 129 Cost: 0.0768 Fidelity: 0.9487 Trace: 0.9502\n",
      "Rep: 130 Cost: 0.0738 Fidelity: 0.9507 Trace: 0.9511\n",
      "Rep: 131 Cost: 0.1156 Fidelity: 0.9488 Trace: 0.9524\n",
      "Rep: 132 Cost: 0.0663 Fidelity: 0.9513 Trace: 0.9520\n",
      "Rep: 133 Cost: 0.1840 Fidelity: 0.9324 Trace: 0.9427\n",
      "Rep: 134 Cost: 0.1240 Fidelity: 0.9455 Trace: 0.9506\n",
      "Rep: 135 Cost: 0.1650 Fidelity: 0.9265 Trace: 0.9337\n",
      "Rep: 136 Cost: 0.1710 Fidelity: 0.9203 Trace: 0.9288\n",
      "Rep: 137 Cost: 0.1049 Fidelity: 0.9463 Trace: 0.9499\n",
      "Rep: 138 Cost: 0.1824 Fidelity: 0.9301 Trace: 0.9394\n",
      "Rep: 139 Cost: 0.1155 Fidelity: 0.9369 Trace: 0.9392\n",
      "Rep: 140 Cost: 0.1424 Fidelity: 0.9449 Trace: 0.9533\n",
      "Rep: 141 Cost: 0.1789 Fidelity: 0.9419 Trace: 0.9534\n",
      "Rep: 142 Cost: 0.0960 Fidelity: 0.9421 Trace: 0.9440\n",
      "Rep: 143 Cost: 0.1345 Fidelity: 0.9231 Trace: 0.9279\n",
      "Rep: 144 Cost: 0.1111 Fidelity: 0.9364 Trace: 0.9397\n",
      "Rep: 145 Cost: 0.1119 Fidelity: 0.9486 Trace: 0.9510\n",
      "Rep: 146 Cost: 0.1271 Fidelity: 0.9370 Trace: 0.9397\n",
      "Rep: 147 Cost: 0.1132 Fidelity: 0.9311 Trace: 0.9342\n",
      "Rep: 148 Cost: 0.1371 Fidelity: 0.9375 Trace: 0.9436\n",
      "Rep: 149 Cost: 0.1320 Fidelity: 0.9435 Trace: 0.9473\n",
      "Rep: 150 Cost: 0.0679 Fidelity: 0.9488 Trace: 0.9497\n",
      "Rep: 151 Cost: 0.0952 Fidelity: 0.9483 Trace: 0.9505\n",
      "Rep: 152 Cost: 0.0879 Fidelity: 0.9362 Trace: 0.9371\n",
      "Rep: 153 Cost: 0.1050 Fidelity: 0.9421 Trace: 0.9438\n",
      "Rep: 154 Cost: 0.0362 Fidelity: 0.9534 Trace: 0.9535\n",
      "Rep: 155 Cost: 0.1057 Fidelity: 0.9536 Trace: 0.9555\n",
      "Rep: 156 Cost: 0.0896 Fidelity: 0.9492 Trace: 0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 157 Cost: 0.0887 Fidelity: 0.9487 Trace: 0.9507\n",
      "Rep: 158 Cost: 0.0738 Fidelity: 0.9511 Trace: 0.9522\n",
      "Rep: 159 Cost: 0.1616 Fidelity: 0.9384 Trace: 0.9457\n",
      "Rep: 160 Cost: 0.1060 Fidelity: 0.9550 Trace: 0.9581\n",
      "Rep: 161 Cost: 0.1298 Fidelity: 0.9363 Trace: 0.9407\n",
      "Rep: 162 Cost: 0.1407 Fidelity: 0.9219 Trace: 0.9265\n",
      "Rep: 163 Cost: 0.0958 Fidelity: 0.9454 Trace: 0.9484\n",
      "Rep: 164 Cost: 0.1473 Fidelity: 0.9520 Trace: 0.9589\n",
      "Rep: 165 Cost: 0.1217 Fidelity: 0.9480 Trace: 0.9510\n",
      "Rep: 166 Cost: 0.1098 Fidelity: 0.9429 Trace: 0.9484\n",
      "Rep: 167 Cost: 0.1779 Fidelity: 0.9337 Trace: 0.9436\n",
      "Rep: 168 Cost: 0.1454 Fidelity: 0.9399 Trace: 0.9455\n",
      "Rep: 169 Cost: 0.0891 Fidelity: 0.9529 Trace: 0.9555\n",
      "Rep: 170 Cost: 0.1493 Fidelity: 0.9458 Trace: 0.9507\n",
      "Rep: 171 Cost: 0.1449 Fidelity: 0.9321 Trace: 0.9377\n",
      "Rep: 172 Cost: 0.0888 Fidelity: 0.9452 Trace: 0.9463\n",
      "Rep: 173 Cost: 0.1388 Fidelity: 0.9384 Trace: 0.9430\n",
      "Rep: 174 Cost: 0.1654 Fidelity: 0.9278 Trace: 0.9363\n",
      "Rep: 175 Cost: 0.1069 Fidelity: 0.9443 Trace: 0.9466\n",
      "Rep: 176 Cost: 0.1423 Fidelity: 0.9377 Trace: 0.9424\n",
      "Rep: 177 Cost: 0.1494 Fidelity: 0.9280 Trace: 0.9346\n",
      "Rep: 178 Cost: 0.1342 Fidelity: 0.9385 Trace: 0.9414\n",
      "Rep: 179 Cost: 0.0812 Fidelity: 0.9403 Trace: 0.9419\n",
      "Rep: 180 Cost: 0.1362 Fidelity: 0.9310 Trace: 0.9332\n",
      "Rep: 181 Cost: 0.0976 Fidelity: 0.9348 Trace: 0.9370\n",
      "Rep: 182 Cost: 0.1097 Fidelity: 0.9431 Trace: 0.9438\n",
      "Rep: 183 Cost: 0.1064 Fidelity: 0.9393 Trace: 0.9422\n",
      "Rep: 184 Cost: 0.1402 Fidelity: 0.9411 Trace: 0.9442\n",
      "Rep: 185 Cost: 0.0624 Fidelity: 0.9418 Trace: 0.9426\n",
      "Rep: 186 Cost: 0.1070 Fidelity: 0.9388 Trace: 0.9406\n",
      "Rep: 187 Cost: 0.1150 Fidelity: 0.9345 Trace: 0.9365\n",
      "Rep: 188 Cost: 0.0729 Fidelity: 0.9431 Trace: 0.9437\n",
      "Rep: 189 Cost: 0.1292 Fidelity: 0.9464 Trace: 0.9485\n",
      "Rep: 190 Cost: 0.1213 Fidelity: 0.9389 Trace: 0.9412\n",
      "Rep: 191 Cost: 0.1446 Fidelity: 0.9392 Trace: 0.9421\n",
      "Rep: 192 Cost: 0.0759 Fidelity: 0.9462 Trace: 0.9465\n",
      "Rep: 193 Cost: 0.1718 Fidelity: 0.9428 Trace: 0.9470\n",
      "Rep: 194 Cost: 0.1730 Fidelity: 0.9420 Trace: 0.9467\n",
      "Rep: 195 Cost: 0.1224 Fidelity: 0.9449 Trace: 0.9473\n",
      "Rep: 196 Cost: 0.1212 Fidelity: 0.9523 Trace: 0.9537\n",
      "Rep: 197 Cost: 0.0812 Fidelity: 0.9496 Trace: 0.9511\n",
      "Rep: 198 Cost: 0.1042 Fidelity: 0.9453 Trace: 0.9473\n",
      "Rep: 199 Cost: 0.0678 Fidelity: 0.9533 Trace: 0.9538\n",
      "Rep: 200 Cost: 0.0514 Fidelity: 0.9526 Trace: 0.9532\n",
      "Rep: 201 Cost: 0.1188 Fidelity: 0.9499 Trace: 0.9540\n",
      "Rep: 202 Cost: 0.1104 Fidelity: 0.9519 Trace: 0.9560\n",
      "Rep: 203 Cost: 0.0993 Fidelity: 0.9500 Trace: 0.9524\n",
      "Rep: 204 Cost: 0.1507 Fidelity: 0.9501 Trace: 0.9560\n",
      "Rep: 205 Cost: 0.1431 Fidelity: 0.9536 Trace: 0.9605\n",
      "Rep: 206 Cost: 0.1014 Fidelity: 0.9558 Trace: 0.9575\n",
      "Rep: 207 Cost: 0.1825 Fidelity: 0.9276 Trace: 0.9388\n",
      "Rep: 208 Cost: 0.1726 Fidelity: 0.9329 Trace: 0.9426\n",
      "Rep: 209 Cost: 0.1080 Fidelity: 0.9319 Trace: 0.9353\n",
      "Rep: 210 Cost: 0.2096 Fidelity: 0.9099 Trace: 0.9231\n",
      "Rep: 211 Cost: 0.1941 Fidelity: 0.9361 Trace: 0.9467\n",
      "Rep: 212 Cost: 0.0984 Fidelity: 0.9453 Trace: 0.9473\n",
      "Rep: 213 Cost: 0.1306 Fidelity: 0.8988 Trace: 0.9013\n",
      "Rep: 214 Cost: 0.1826 Fidelity: 0.8653 Trace: 0.8699\n",
      "Rep: 215 Cost: 0.1352 Fidelity: 0.8818 Trace: 0.8839\n",
      "Rep: 216 Cost: 0.0980 Fidelity: 0.9218 Trace: 0.9236\n",
      "Rep: 217 Cost: 0.1212 Fidelity: 0.9430 Trace: 0.9458\n",
      "Rep: 218 Cost: 0.1175 Fidelity: 0.9317 Trace: 0.9362\n",
      "Rep: 219 Cost: 0.1421 Fidelity: 0.9140 Trace: 0.9183\n",
      "Rep: 220 Cost: 0.0963 Fidelity: 0.9318 Trace: 0.9337\n",
      "Rep: 221 Cost: 0.0936 Fidelity: 0.9371 Trace: 0.9384\n",
      "Rep: 222 Cost: 0.1106 Fidelity: 0.9360 Trace: 0.9392\n",
      "Rep: 223 Cost: 0.1290 Fidelity: 0.9352 Trace: 0.9387\n",
      "Rep: 224 Cost: 0.1030 Fidelity: 0.9270 Trace: 0.9290\n",
      "Rep: 225 Cost: 0.1077 Fidelity: 0.9237 Trace: 0.9258\n",
      "Rep: 226 Cost: 0.0975 Fidelity: 0.9461 Trace: 0.9488\n",
      "Rep: 227 Cost: 0.0932 Fidelity: 0.9493 Trace: 0.9516\n",
      "Rep: 228 Cost: 0.0916 Fidelity: 0.9400 Trace: 0.9419\n",
      "Rep: 229 Cost: 0.0914 Fidelity: 0.9447 Trace: 0.9474\n",
      "Rep: 230 Cost: 0.0580 Fidelity: 0.9494 Trace: 0.9499\n",
      "Rep: 231 Cost: 0.1020 Fidelity: 0.9419 Trace: 0.9461\n",
      "Rep: 232 Cost: 0.1234 Fidelity: 0.9383 Trace: 0.9429\n",
      "Rep: 233 Cost: 0.1004 Fidelity: 0.9345 Trace: 0.9370\n",
      "Rep: 234 Cost: 0.1253 Fidelity: 0.9407 Trace: 0.9452\n",
      "Rep: 235 Cost: 0.0689 Fidelity: 0.9562 Trace: 0.9574\n",
      "Rep: 236 Cost: 0.1224 Fidelity: 0.9341 Trace: 0.9382\n",
      "Rep: 237 Cost: 0.1438 Fidelity: 0.9182 Trace: 0.9245\n",
      "Rep: 238 Cost: 0.1129 Fidelity: 0.9377 Trace: 0.9411\n",
      "Rep: 239 Cost: 0.0936 Fidelity: 0.9544 Trace: 0.9572\n",
      "Rep: 240 Cost: 0.0880 Fidelity: 0.9475 Trace: 0.9493\n",
      "Rep: 241 Cost: 0.0858 Fidelity: 0.9332 Trace: 0.9346\n",
      "Rep: 242 Cost: 0.1046 Fidelity: 0.9395 Trace: 0.9429\n",
      "Rep: 243 Cost: 0.0787 Fidelity: 0.9566 Trace: 0.9587\n",
      "Rep: 244 Cost: 0.0591 Fidelity: 0.9612 Trace: 0.9618\n",
      "Rep: 245 Cost: 0.0765 Fidelity: 0.9635 Trace: 0.9660\n",
      "Rep: 246 Cost: 0.1112 Fidelity: 0.9574 Trace: 0.9615\n",
      "Rep: 247 Cost: 0.0589 Fidelity: 0.9641 Trace: 0.9649\n",
      "Rep: 248 Cost: 0.0789 Fidelity: 0.9703 Trace: 0.9725\n",
      "Rep: 249 Cost: 0.0354 Fidelity: 0.9690 Trace: 0.9692\n",
      "Rep: 250 Cost: 0.0656 Fidelity: 0.9654 Trace: 0.9662\n",
      "Rep: 251 Cost: 0.0853 Fidelity: 0.9650 Trace: 0.9664\n",
      "Rep: 252 Cost: 0.0604 Fidelity: 0.9702 Trace: 0.9709\n",
      "Rep: 253 Cost: 0.0458 Fidelity: 0.9738 Trace: 0.9743\n",
      "Rep: 254 Cost: 0.0803 Fidelity: 0.9664 Trace: 0.9680\n",
      "Rep: 255 Cost: 0.0511 Fidelity: 0.9744 Trace: 0.9746\n",
      "Rep: 256 Cost: 0.0407 Fidelity: 0.9732 Trace: 0.9736\n",
      "Rep: 257 Cost: 0.0937 Fidelity: 0.9687 Trace: 0.9727\n",
      "Rep: 258 Cost: 0.0806 Fidelity: 0.9685 Trace: 0.9708\n",
      "Rep: 259 Cost: 0.0638 Fidelity: 0.9657 Trace: 0.9668\n",
      "Rep: 260 Cost: 0.0372 Fidelity: 0.9706 Trace: 0.9709\n",
      "Rep: 261 Cost: 0.1088 Fidelity: 0.9657 Trace: 0.9699\n",
      "Rep: 262 Cost: 0.0649 Fidelity: 0.9711 Trace: 0.9721\n",
      "Rep: 263 Cost: 0.1726 Fidelity: 0.9623 Trace: 0.9735\n",
      "Rep: 264 Cost: 0.1549 Fidelity: 0.9637 Trace: 0.9744\n",
      "Rep: 265 Cost: 0.0830 Fidelity: 0.9653 Trace: 0.9674\n",
      "Rep: 266 Cost: 0.1493 Fidelity: 0.9558 Trace: 0.9625\n",
      "Rep: 267 Cost: 0.1067 Fidelity: 0.9682 Trace: 0.9719\n",
      "Rep: 268 Cost: 0.1015 Fidelity: 0.9745 Trace: 0.9782\n",
      "Rep: 269 Cost: 0.1172 Fidelity: 0.9609 Trace: 0.9630\n",
      "Rep: 270 Cost: 0.1340 Fidelity: 0.9559 Trace: 0.9602\n",
      "Rep: 271 Cost: 0.1264 Fidelity: 0.9646 Trace: 0.9683\n",
      "Rep: 272 Cost: 0.0850 Fidelity: 0.9656 Trace: 0.9663\n",
      "Rep: 273 Cost: 0.0732 Fidelity: 0.9578 Trace: 0.9590\n",
      "Rep: 274 Cost: 0.0746 Fidelity: 0.9579 Trace: 0.9595\n",
      "Rep: 275 Cost: 0.1240 Fidelity: 0.9594 Trace: 0.9637\n",
      "Rep: 276 Cost: 0.0722 Fidelity: 0.9623 Trace: 0.9637\n",
      "Rep: 277 Cost: 0.0902 Fidelity: 0.9607 Trace: 0.9629\n",
      "Rep: 278 Cost: 0.1092 Fidelity: 0.9640 Trace: 0.9676\n",
      "Rep: 279 Cost: 0.0785 Fidelity: 0.9639 Trace: 0.9658\n",
      "Rep: 280 Cost: 0.1015 Fidelity: 0.9624 Trace: 0.9644\n",
      "Rep: 281 Cost: 0.0703 Fidelity: 0.9624 Trace: 0.9644\n",
      "Rep: 282 Cost: 0.1263 Fidelity: 0.9540 Trace: 0.9602\n",
      "Rep: 283 Cost: 0.0862 Fidelity: 0.9627 Trace: 0.9639\n",
      "Rep: 284 Cost: 0.1107 Fidelity: 0.9640 Trace: 0.9671\n",
      "Rep: 285 Cost: 0.0994 Fidelity: 0.9626 Trace: 0.9638\n",
      "Rep: 286 Cost: 0.1654 Fidelity: 0.9491 Trace: 0.9532\n",
      "Rep: 287 Cost: 0.1234 Fidelity: 0.9412 Trace: 0.9456\n",
      "Rep: 288 Cost: 0.1127 Fidelity: 0.9541 Trace: 0.9549\n",
      "Rep: 289 Cost: 0.0974 Fidelity: 0.9670 Trace: 0.9684\n",
      "Rep: 290 Cost: 0.1230 Fidelity: 0.9631 Trace: 0.9661\n",
      "Rep: 291 Cost: 0.1302 Fidelity: 0.9534 Trace: 0.9566\n",
      "Rep: 292 Cost: 0.0863 Fidelity: 0.9564 Trace: 0.9580\n",
      "Rep: 293 Cost: 0.0613 Fidelity: 0.9645 Trace: 0.9649\n",
      "Rep: 294 Cost: 0.1044 Fidelity: 0.9540 Trace: 0.9561\n",
      "Rep: 295 Cost: 0.0812 Fidelity: 0.9546 Trace: 0.9562\n",
      "Rep: 296 Cost: 0.1251 Fidelity: 0.9665 Trace: 0.9698\n",
      "Rep: 297 Cost: 0.1309 Fidelity: 0.9676 Trace: 0.9734\n",
      "Rep: 298 Cost: 0.1149 Fidelity: 0.9657 Trace: 0.9672\n",
      "Rep: 299 Cost: 0.1391 Fidelity: 0.9669 Trace: 0.9679\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0010\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9313e-04\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3469e-04\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3705e-04\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2414e-04\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.8999e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5980e-05\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1251e-04\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2672e-04\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9170e-04\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9277e-04\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4874e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7408e-04\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1738e-04\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2444e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.1118e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2037e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1549e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8654e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8384e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.2989e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.3560e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2845e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2327e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.7877e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2410e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8046e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4717e-06\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7526e-07\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8978e-06\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4638e-05\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0072e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2996e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2221e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0737e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8880e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3584e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0780e-06\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7053e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7125e-07\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3681e-06\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5020e-06\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6487e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4513e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6065e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9973e-06\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4521e-06\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0200e-06\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8053e-06\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1922e-06\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9468e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.6045e-07\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.8211e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0108e-06\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3472e-06\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8883e-06\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3056e-06\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1153e-06\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6028e-06\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0568e-06\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8475e-07\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6085e-07\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3593e-07\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0912e-07\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7394e-07\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8357e-07\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.7829e-07\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0210e-07\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5535e-07\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9662e-07\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7674e-07\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6573e-07\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4587e-07\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2594e-08\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4423e-08\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7985e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2460e-07\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8305e-07\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9907e-07\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9303e-07\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6456e-07\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3593e-07\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.9634e-08\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.5845e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0732e-08\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5810e-09\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7755e-08\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0068e-08\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.3005e-08\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5608e-08\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.0307e-08\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8287e-08\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6159e-08\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9671e-08\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1805e-08\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2171e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.8635e-09\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4342e-09\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0809e-08\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4799e-08\n",
      "[[-0.34310877 -0.49723393]]\n",
      "[1. 0. 0.]\n",
      "Rep: 0 Cost: 1.2452 Fidelity: 0.0133 Trace: 0.0715\n",
      "Rep: 1 Cost: 1.4064 Fidelity: 0.0490 Trace: 0.0725\n",
      "Rep: 2 Cost: 1.0942 Fidelity: 0.0184 Trace: 0.0305\n",
      "Rep: 3 Cost: 1.1081 Fidelity: 0.0127 Trace: 0.0163\n",
      "Rep: 4 Cost: 1.1422 Fidelity: 0.0007 Trace: 0.0074\n",
      "Rep: 5 Cost: 1.0536 Fidelity: 0.0012 Trace: 0.0027\n",
      "Rep: 6 Cost: 1.0014 Fidelity: 0.0033 Trace: 0.0053\n",
      "Rep: 7 Cost: 0.9682 Fidelity: 0.0104 Trace: 0.0128\n",
      "Rep: 8 Cost: 0.9457 Fidelity: 0.0160 Trace: 0.0197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 9 Cost: 0.9261 Fidelity: 0.0238 Trace: 0.0278\n",
      "Rep: 10 Cost: 0.8633 Fidelity: 0.0393 Trace: 0.0412\n",
      "Rep: 11 Cost: 0.8127 Fidelity: 0.0581 Trace: 0.0595\n",
      "Rep: 12 Cost: 0.7355 Fidelity: 0.0854 Trace: 0.0858\n",
      "Rep: 13 Cost: 0.7581 Fidelity: 0.1163 Trace: 0.1221\n",
      "Rep: 14 Cost: 0.7411 Fidelity: 0.1533 Trace: 0.1634\n",
      "Rep: 15 Cost: 0.6403 Fidelity: 0.2000 Trace: 0.2040\n",
      "Rep: 16 Cost: 0.6258 Fidelity: 0.2251 Trace: 0.2302\n",
      "Rep: 17 Cost: 0.5353 Fidelity: 0.2816 Trace: 0.2836\n",
      "Rep: 18 Cost: 0.5431 Fidelity: 0.3331 Trace: 0.3405\n",
      "Rep: 19 Cost: 0.5309 Fidelity: 0.3594 Trace: 0.3690\n",
      "Rep: 20 Cost: 0.4341 Fidelity: 0.4118 Trace: 0.4139\n",
      "Rep: 21 Cost: 0.4741 Fidelity: 0.4665 Trace: 0.4842\n",
      "Rep: 22 Cost: 0.5055 Fidelity: 0.5724 Trace: 0.5999\n",
      "Rep: 23 Cost: 0.3130 Fidelity: 0.6549 Trace: 0.6599\n",
      "Rep: 24 Cost: 0.3476 Fidelity: 0.6467 Trace: 0.6631\n",
      "Rep: 25 Cost: 0.4151 Fidelity: 0.6846 Trace: 0.7083\n",
      "Rep: 26 Cost: 0.3363 Fidelity: 0.7405 Trace: 0.7647\n",
      "Rep: 27 Cost: 0.2456 Fidelity: 0.8021 Trace: 0.8127\n",
      "Rep: 28 Cost: 0.3758 Fidelity: 0.7456 Trace: 0.7795\n",
      "Rep: 29 Cost: 0.3584 Fidelity: 0.7585 Trace: 0.7876\n",
      "Rep: 30 Cost: 0.2332 Fidelity: 0.8177 Trace: 0.8182\n",
      "Rep: 31 Cost: 0.3800 Fidelity: 0.8077 Trace: 0.8355\n",
      "Rep: 32 Cost: 0.3133 Fidelity: 0.8033 Trace: 0.8176\n",
      "Rep: 33 Cost: 0.3491 Fidelity: 0.7880 Trace: 0.7896\n",
      "Rep: 34 Cost: 0.3547 Fidelity: 0.8062 Trace: 0.8106\n",
      "Rep: 35 Cost: 0.1913 Fidelity: 0.8218 Trace: 0.8268\n",
      "Rep: 36 Cost: 0.2269 Fidelity: 0.8441 Trace: 0.8482\n",
      "Rep: 37 Cost: 0.3004 Fidelity: 0.8229 Trace: 0.8373\n",
      "Rep: 38 Cost: 0.3177 Fidelity: 0.8081 Trace: 0.8333\n",
      "Rep: 39 Cost: 0.1556 Fidelity: 0.8589 Trace: 0.8639\n",
      "Rep: 40 Cost: 0.2534 Fidelity: 0.8671 Trace: 0.8880\n",
      "Rep: 41 Cost: 0.3400 Fidelity: 0.8515 Trace: 0.8852\n",
      "Rep: 42 Cost: 0.2871 Fidelity: 0.8739 Trace: 0.8872\n",
      "Rep: 43 Cost: 0.2116 Fidelity: 0.8724 Trace: 0.8748\n",
      "Rep: 44 Cost: 0.1204 Fidelity: 0.8872 Trace: 0.8888\n",
      "Rep: 45 Cost: 0.3144 Fidelity: 0.8828 Trace: 0.9021\n",
      "Rep: 46 Cost: 0.3301 Fidelity: 0.8836 Trace: 0.8989\n",
      "Rep: 47 Cost: 0.2002 Fidelity: 0.8983 Trace: 0.9014\n",
      "Rep: 48 Cost: 0.2233 Fidelity: 0.8794 Trace: 0.8935\n",
      "Rep: 49 Cost: 0.1280 Fidelity: 0.8979 Trace: 0.9010\n",
      "Rep: 50 Cost: 0.2318 Fidelity: 0.8812 Trace: 0.9021\n",
      "Rep: 51 Cost: 0.2554 Fidelity: 0.8788 Trace: 0.9073\n",
      "Rep: 52 Cost: 0.2027 Fidelity: 0.9107 Trace: 0.9172\n",
      "Rep: 53 Cost: 0.2552 Fidelity: 0.8929 Trace: 0.8996\n",
      "Rep: 54 Cost: 0.1684 Fidelity: 0.9054 Trace: 0.9098\n",
      "Rep: 55 Cost: 0.1879 Fidelity: 0.9179 Trace: 0.9247\n",
      "Rep: 56 Cost: 0.2235 Fidelity: 0.9113 Trace: 0.9163\n",
      "Rep: 57 Cost: 0.1967 Fidelity: 0.9015 Trace: 0.9055\n",
      "Rep: 58 Cost: 0.1589 Fidelity: 0.9071 Trace: 0.9132\n",
      "Rep: 59 Cost: 0.0945 Fidelity: 0.9296 Trace: 0.9306\n",
      "Rep: 60 Cost: 0.1819 Fidelity: 0.9182 Trace: 0.9282\n",
      "Rep: 61 Cost: 0.1440 Fidelity: 0.9293 Trace: 0.9347\n",
      "Rep: 62 Cost: 0.1447 Fidelity: 0.9228 Trace: 0.9279\n",
      "Rep: 63 Cost: 0.0730 Fidelity: 0.9331 Trace: 0.9338\n",
      "Rep: 64 Cost: 0.2395 Fidelity: 0.9284 Trace: 0.9421\n",
      "Rep: 65 Cost: 0.1567 Fidelity: 0.9372 Trace: 0.9403\n",
      "Rep: 66 Cost: 0.2483 Fidelity: 0.8691 Trace: 0.8948\n",
      "Rep: 67 Cost: 0.2842 Fidelity: 0.8519 Trace: 0.8842\n",
      "Rep: 68 Cost: 0.1075 Fidelity: 0.9207 Trace: 0.9237\n",
      "Rep: 69 Cost: 0.2701 Fidelity: 0.9021 Trace: 0.9417\n",
      "Rep: 70 Cost: 0.3496 Fidelity: 0.8637 Trace: 0.9327\n",
      "Rep: 71 Cost: 0.2377 Fidelity: 0.9120 Trace: 0.9405\n",
      "Rep: 72 Cost: 0.1515 Fidelity: 0.9225 Trace: 0.9289\n",
      "Rep: 73 Cost: 0.2243 Fidelity: 0.8775 Trace: 0.8945\n",
      "Rep: 74 Cost: 0.1949 Fidelity: 0.8722 Trace: 0.8871\n",
      "Rep: 75 Cost: 0.1212 Fidelity: 0.9034 Trace: 0.9058\n",
      "Rep: 76 Cost: 0.2346 Fidelity: 0.9179 Trace: 0.9352\n",
      "Rep: 77 Cost: 0.2567 Fidelity: 0.9051 Trace: 0.9336\n",
      "Rep: 78 Cost: 0.1758 Fidelity: 0.9226 Trace: 0.9312\n",
      "Rep: 79 Cost: 0.1582 Fidelity: 0.9195 Trace: 0.9240\n",
      "Rep: 80 Cost: 0.1691 Fidelity: 0.9109 Trace: 0.9206\n",
      "Rep: 81 Cost: 0.1393 Fidelity: 0.9283 Trace: 0.9300\n",
      "Rep: 82 Cost: 0.2428 Fidelity: 0.9295 Trace: 0.9399\n",
      "Rep: 83 Cost: 0.2128 Fidelity: 0.9261 Trace: 0.9395\n",
      "Rep: 84 Cost: 0.1081 Fidelity: 0.9281 Trace: 0.9307\n",
      "Rep: 85 Cost: 0.2051 Fidelity: 0.9073 Trace: 0.9174\n",
      "Rep: 86 Cost: 0.2348 Fidelity: 0.8936 Trace: 0.9112\n",
      "Rep: 87 Cost: 0.1342 Fidelity: 0.9167 Trace: 0.9222\n",
      "Rep: 88 Cost: 0.1400 Fidelity: 0.9236 Trace: 0.9305\n",
      "Rep: 89 Cost: 0.2038 Fidelity: 0.9128 Trace: 0.9297\n",
      "Rep: 90 Cost: 0.1383 Fidelity: 0.9279 Trace: 0.9336\n",
      "Rep: 91 Cost: 0.1166 Fidelity: 0.9224 Trace: 0.9265\n",
      "Rep: 92 Cost: 0.1566 Fidelity: 0.9165 Trace: 0.9242\n",
      "Rep: 93 Cost: 0.0995 Fidelity: 0.9330 Trace: 0.9338\n",
      "Rep: 94 Cost: 0.1603 Fidelity: 0.9278 Trace: 0.9395\n",
      "Rep: 95 Cost: 0.2010 Fidelity: 0.9279 Trace: 0.9444\n",
      "Rep: 96 Cost: 0.1522 Fidelity: 0.9393 Trace: 0.9430\n",
      "Rep: 97 Cost: 0.1534 Fidelity: 0.9166 Trace: 0.9227\n",
      "Rep: 98 Cost: 0.1562 Fidelity: 0.9156 Trace: 0.9264\n",
      "Rep: 99 Cost: 0.1255 Fidelity: 0.9435 Trace: 0.9460\n",
      "Rep: 100 Cost: 0.1528 Fidelity: 0.9402 Trace: 0.9473\n",
      "Rep: 101 Cost: 0.1404 Fidelity: 0.9376 Trace: 0.9478\n",
      "Rep: 102 Cost: 0.0993 Fidelity: 0.9457 Trace: 0.9471\n",
      "Rep: 103 Cost: 0.1537 Fidelity: 0.9225 Trace: 0.9312\n",
      "Rep: 104 Cost: 0.1779 Fidelity: 0.9121 Trace: 0.9269\n",
      "Rep: 105 Cost: 0.1156 Fidelity: 0.9393 Trace: 0.9430\n",
      "Rep: 106 Cost: 0.1179 Fidelity: 0.9438 Trace: 0.9506\n",
      "Rep: 107 Cost: 0.1780 Fidelity: 0.9367 Trace: 0.9525\n",
      "Rep: 108 Cost: 0.1368 Fidelity: 0.9479 Trace: 0.9553\n",
      "Rep: 109 Cost: 0.1044 Fidelity: 0.9442 Trace: 0.9466\n",
      "Rep: 110 Cost: 0.1288 Fidelity: 0.9336 Trace: 0.9412\n",
      "Rep: 111 Cost: 0.1107 Fidelity: 0.9418 Trace: 0.9460\n",
      "Rep: 112 Cost: 0.1385 Fidelity: 0.9377 Trace: 0.9435\n",
      "Rep: 113 Cost: 0.1263 Fidelity: 0.9445 Trace: 0.9502\n",
      "Rep: 114 Cost: 0.0910 Fidelity: 0.9525 Trace: 0.9541\n",
      "Rep: 115 Cost: 0.1344 Fidelity: 0.9441 Trace: 0.9495\n",
      "Rep: 116 Cost: 0.1054 Fidelity: 0.9463 Trace: 0.9493\n",
      "Rep: 117 Cost: 0.1344 Fidelity: 0.9481 Trace: 0.9530\n",
      "Rep: 118 Cost: 0.1295 Fidelity: 0.9493 Trace: 0.9559\n",
      "Rep: 119 Cost: 0.1264 Fidelity: 0.9478 Trace: 0.9512\n",
      "Rep: 120 Cost: 0.1156 Fidelity: 0.9467 Trace: 0.9500\n",
      "Rep: 121 Cost: 0.1141 Fidelity: 0.9455 Trace: 0.9511\n",
      "Rep: 122 Cost: 0.0507 Fidelity: 0.9541 Trace: 0.9544\n",
      "Rep: 123 Cost: 0.1584 Fidelity: 0.9374 Trace: 0.9499\n",
      "Rep: 124 Cost: 0.1701 Fidelity: 0.9330 Trace: 0.9496\n",
      "Rep: 125 Cost: 0.1080 Fidelity: 0.9501 Trace: 0.9546\n",
      "Rep: 126 Cost: 0.1162 Fidelity: 0.9347 Trace: 0.9389\n",
      "Rep: 127 Cost: 0.1538 Fidelity: 0.9220 Trace: 0.9320\n",
      "Rep: 128 Cost: 0.0924 Fidelity: 0.9430 Trace: 0.9464\n",
      "Rep: 129 Cost: 0.0912 Fidelity: 0.9546 Trace: 0.9575\n",
      "Rep: 130 Cost: 0.1196 Fidelity: 0.9481 Trace: 0.9550\n",
      "Rep: 131 Cost: 0.0852 Fidelity: 0.9545 Trace: 0.9567\n",
      "Rep: 132 Cost: 0.1055 Fidelity: 0.9464 Trace: 0.9506\n",
      "Rep: 133 Cost: 0.1211 Fidelity: 0.9405 Trace: 0.9477\n",
      "Rep: 134 Cost: 0.0881 Fidelity: 0.9513 Trace: 0.9547\n",
      "Rep: 135 Cost: 0.1008 Fidelity: 0.9519 Trace: 0.9546\n",
      "Rep: 136 Cost: 0.0984 Fidelity: 0.9508 Trace: 0.9537\n",
      "Rep: 137 Cost: 0.0659 Fidelity: 0.9576 Trace: 0.9579\n",
      "Rep: 138 Cost: 0.0687 Fidelity: 0.9592 Trace: 0.9603\n",
      "Rep: 139 Cost: 0.0956 Fidelity: 0.9567 Trace: 0.9588\n",
      "Rep: 140 Cost: 0.0551 Fidelity: 0.9593 Trace: 0.9597\n",
      "Rep: 141 Cost: 0.0693 Fidelity: 0.9591 Trace: 0.9608\n",
      "Rep: 142 Cost: 0.0510 Fidelity: 0.9612 Trace: 0.9617\n",
      "Rep: 143 Cost: 0.1161 Fidelity: 0.9526 Trace: 0.9575\n",
      "Rep: 144 Cost: 0.1040 Fidelity: 0.9525 Trace: 0.9567\n",
      "Rep: 145 Cost: 0.0562 Fidelity: 0.9613 Trace: 0.9615\n",
      "Rep: 146 Cost: 0.2077 Fidelity: 0.9379 Trace: 0.9567\n",
      "Rep: 147 Cost: 0.1977 Fidelity: 0.9431 Trace: 0.9606\n",
      "Rep: 148 Cost: 0.1167 Fidelity: 0.9542 Trace: 0.9618\n",
      "Rep: 149 Cost: 0.1392 Fidelity: 0.9445 Trace: 0.9538\n",
      "Rep: 150 Cost: 0.1497 Fidelity: 0.9462 Trace: 0.9547\n",
      "Rep: 151 Cost: 0.1407 Fidelity: 0.9507 Trace: 0.9589\n",
      "Rep: 152 Cost: 0.1049 Fidelity: 0.9602 Trace: 0.9628\n",
      "Rep: 153 Cost: 0.1202 Fidelity: 0.9586 Trace: 0.9643\n",
      "Rep: 154 Cost: 0.1559 Fidelity: 0.9575 Trace: 0.9624\n",
      "Rep: 155 Cost: 0.1449 Fidelity: 0.9581 Trace: 0.9590\n",
      "Rep: 156 Cost: 0.1158 Fidelity: 0.9541 Trace: 0.9559\n",
      "Rep: 157 Cost: 0.0722 Fidelity: 0.9586 Trace: 0.9590\n",
      "Rep: 158 Cost: 0.1955 Fidelity: 0.9352 Trace: 0.9490\n",
      "Rep: 159 Cost: 0.1936 Fidelity: 0.9275 Trace: 0.9444\n",
      "Rep: 160 Cost: 0.0632 Fidelity: 0.9524 Trace: 0.9533\n",
      "Rep: 161 Cost: 0.2294 Fidelity: 0.9151 Trace: 0.9432\n",
      "Rep: 162 Cost: 0.3007 Fidelity: 0.8820 Trace: 0.9268\n",
      "Rep: 163 Cost: 0.2199 Fidelity: 0.9159 Trace: 0.9365\n",
      "Rep: 164 Cost: 0.0888 Fidelity: 0.9522 Trace: 0.9526\n",
      "Rep: 165 Cost: 0.1218 Fidelity: 0.9442 Trace: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 166 Cost: 0.1251 Fidelity: 0.9466 Trace: 0.9476\n",
      "Rep: 167 Cost: 0.2249 Fidelity: 0.9304 Trace: 0.9365\n",
      "Rep: 168 Cost: 0.1785 Fidelity: 0.9335 Trace: 0.9374\n",
      "Rep: 169 Cost: 0.1298 Fidelity: 0.9396 Trace: 0.9463\n",
      "Rep: 170 Cost: 0.1309 Fidelity: 0.9470 Trace: 0.9526\n",
      "Rep: 171 Cost: 0.1309 Fidelity: 0.9368 Trace: 0.9425\n",
      "Rep: 172 Cost: 0.1108 Fidelity: 0.9417 Trace: 0.9461\n",
      "Rep: 173 Cost: 0.1134 Fidelity: 0.9511 Trace: 0.9527\n",
      "Rep: 174 Cost: 0.0497 Fidelity: 0.9529 Trace: 0.9533\n",
      "Rep: 175 Cost: 0.1908 Fidelity: 0.9441 Trace: 0.9530\n",
      "Rep: 176 Cost: 0.1649 Fidelity: 0.9428 Trace: 0.9502\n",
      "Rep: 177 Cost: 0.1042 Fidelity: 0.9383 Trace: 0.9412\n",
      "Rep: 178 Cost: 0.1206 Fidelity: 0.9351 Trace: 0.9380\n",
      "Rep: 179 Cost: 0.1253 Fidelity: 0.9358 Trace: 0.9416\n",
      "Rep: 180 Cost: 0.0672 Fidelity: 0.9429 Trace: 0.9439\n",
      "Rep: 181 Cost: 0.1801 Fidelity: 0.9202 Trace: 0.9326\n",
      "Rep: 182 Cost: 0.1617 Fidelity: 0.9343 Trace: 0.9460\n",
      "Rep: 183 Cost: 0.0801 Fidelity: 0.9546 Trace: 0.9554\n",
      "Rep: 184 Cost: 0.1159 Fidelity: 0.9343 Trace: 0.9380\n",
      "Rep: 185 Cost: 0.1301 Fidelity: 0.9327 Trace: 0.9365\n",
      "Rep: 186 Cost: 0.0971 Fidelity: 0.9371 Trace: 0.9383\n",
      "Rep: 187 Cost: 0.1230 Fidelity: 0.9265 Trace: 0.9322\n",
      "Rep: 188 Cost: 0.1064 Fidelity: 0.9451 Trace: 0.9477\n",
      "Rep: 189 Cost: 0.1368 Fidelity: 0.9445 Trace: 0.9512\n",
      "Rep: 190 Cost: 0.1419 Fidelity: 0.9435 Trace: 0.9510\n",
      "Rep: 191 Cost: 0.0518 Fidelity: 0.9567 Trace: 0.9571\n",
      "Rep: 192 Cost: 0.0867 Fidelity: 0.9473 Trace: 0.9497\n",
      "Rep: 193 Cost: 0.0572 Fidelity: 0.9486 Trace: 0.9491\n",
      "Rep: 194 Cost: 0.1443 Fidelity: 0.9382 Trace: 0.9483\n",
      "Rep: 195 Cost: 0.1613 Fidelity: 0.9308 Trace: 0.9464\n",
      "Rep: 196 Cost: 0.1043 Fidelity: 0.9514 Trace: 0.9564\n",
      "Rep: 197 Cost: 0.0824 Fidelity: 0.9595 Trace: 0.9622\n",
      "Rep: 198 Cost: 0.1068 Fidelity: 0.9546 Trace: 0.9613\n",
      "Rep: 199 Cost: 0.0865 Fidelity: 0.9579 Trace: 0.9608\n",
      "Rep: 200 Cost: 0.0882 Fidelity: 0.9502 Trace: 0.9525\n",
      "Rep: 201 Cost: 0.0876 Fidelity: 0.9522 Trace: 0.9545\n",
      "Rep: 202 Cost: 0.0703 Fidelity: 0.9619 Trace: 0.9628\n",
      "Rep: 203 Cost: 0.0708 Fidelity: 0.9643 Trace: 0.9654\n",
      "Rep: 204 Cost: 0.0730 Fidelity: 0.9630 Trace: 0.9644\n",
      "Rep: 205 Cost: 0.0451 Fidelity: 0.9694 Trace: 0.9696\n",
      "Rep: 206 Cost: 0.1260 Fidelity: 0.9553 Trace: 0.9646\n",
      "Rep: 207 Cost: 0.1311 Fidelity: 0.9544 Trace: 0.9638\n",
      "Rep: 208 Cost: 0.0493 Fidelity: 0.9701 Trace: 0.9711\n",
      "Rep: 209 Cost: 0.0966 Fidelity: 0.9613 Trace: 0.9663\n",
      "Rep: 210 Cost: 0.1032 Fidelity: 0.9615 Trace: 0.9670\n",
      "Rep: 211 Cost: 0.0635 Fidelity: 0.9712 Trace: 0.9722\n",
      "Rep: 212 Cost: 0.1031 Fidelity: 0.9639 Trace: 0.9695\n",
      "Rep: 213 Cost: 0.1185 Fidelity: 0.9656 Trace: 0.9709\n",
      "Rep: 214 Cost: 0.0633 Fidelity: 0.9707 Trace: 0.9711\n",
      "Rep: 215 Cost: 0.1280 Fidelity: 0.9490 Trace: 0.9563\n",
      "Rep: 216 Cost: 0.1544 Fidelity: 0.9460 Trace: 0.9557\n",
      "Rep: 217 Cost: 0.1199 Fidelity: 0.9623 Trace: 0.9665\n",
      "Rep: 218 Cost: 0.0656 Fidelity: 0.9678 Trace: 0.9690\n",
      "Rep: 219 Cost: 0.1179 Fidelity: 0.9610 Trace: 0.9642\n",
      "Rep: 220 Cost: 0.0986 Fidelity: 0.9641 Trace: 0.9653\n",
      "Rep: 221 Cost: 0.1068 Fidelity: 0.9632 Trace: 0.9663\n",
      "Rep: 222 Cost: 0.0826 Fidelity: 0.9668 Trace: 0.9671\n",
      "Rep: 223 Cost: 0.2222 Fidelity: 0.9379 Trace: 0.9490\n",
      "Rep: 224 Cost: 0.1925 Fidelity: 0.9335 Trace: 0.9442\n",
      "Rep: 225 Cost: 0.1035 Fidelity: 0.9539 Trace: 0.9583\n",
      "Rep: 226 Cost: 0.1145 Fidelity: 0.9588 Trace: 0.9647\n",
      "Rep: 227 Cost: 0.1631 Fidelity: 0.9400 Trace: 0.9488\n",
      "Rep: 228 Cost: 0.0894 Fidelity: 0.9499 Trace: 0.9524\n",
      "Rep: 229 Cost: 0.1682 Fidelity: 0.9524 Trace: 0.9587\n",
      "Rep: 230 Cost: 0.1682 Fidelity: 0.9477 Trace: 0.9549\n",
      "Rep: 231 Cost: 0.0733 Fidelity: 0.9619 Trace: 0.9628\n",
      "Rep: 232 Cost: 0.1609 Fidelity: 0.9564 Trace: 0.9649\n",
      "Rep: 233 Cost: 0.1418 Fidelity: 0.9477 Trace: 0.9579\n",
      "Rep: 234 Cost: 0.0656 Fidelity: 0.9566 Trace: 0.9570\n",
      "Rep: 235 Cost: 0.2012 Fidelity: 0.9361 Trace: 0.9548\n",
      "Rep: 236 Cost: 0.2409 Fidelity: 0.9205 Trace: 0.9518\n",
      "Rep: 237 Cost: 0.1627 Fidelity: 0.9435 Trace: 0.9576\n",
      "Rep: 238 Cost: 0.0773 Fidelity: 0.9602 Trace: 0.9616\n",
      "Rep: 239 Cost: 0.1364 Fidelity: 0.9477 Trace: 0.9549\n",
      "Rep: 240 Cost: 0.1345 Fidelity: 0.9444 Trace: 0.9510\n",
      "Rep: 241 Cost: 0.0603 Fidelity: 0.9505 Trace: 0.9512\n",
      "Rep: 242 Cost: 0.1630 Fidelity: 0.9305 Trace: 0.9398\n",
      "Rep: 243 Cost: 0.1795 Fidelity: 0.9272 Trace: 0.9396\n",
      "Rep: 244 Cost: 0.1046 Fidelity: 0.9474 Trace: 0.9504\n",
      "Rep: 245 Cost: 0.1317 Fidelity: 0.9429 Trace: 0.9487\n",
      "Rep: 246 Cost: 0.1766 Fidelity: 0.9347 Trace: 0.9474\n",
      "Rep: 247 Cost: 0.1227 Fidelity: 0.9536 Trace: 0.9593\n",
      "Rep: 248 Cost: 0.0602 Fidelity: 0.9597 Trace: 0.9605\n",
      "Rep: 249 Cost: 0.1036 Fidelity: 0.9510 Trace: 0.9545\n",
      "Rep: 250 Cost: 0.0618 Fidelity: 0.9604 Trace: 0.9611\n",
      "Rep: 251 Cost: 0.0975 Fidelity: 0.9605 Trace: 0.9658\n",
      "Rep: 252 Cost: 0.1262 Fidelity: 0.9543 Trace: 0.9628\n",
      "Rep: 253 Cost: 0.0785 Fidelity: 0.9638 Trace: 0.9660\n",
      "Rep: 254 Cost: 0.0909 Fidelity: 0.9595 Trace: 0.9622\n",
      "Rep: 255 Cost: 0.0995 Fidelity: 0.9545 Trace: 0.9593\n",
      "Rep: 256 Cost: 0.0687 Fidelity: 0.9629 Trace: 0.9639\n",
      "Rep: 257 Cost: 0.1165 Fidelity: 0.9579 Trace: 0.9628\n",
      "Rep: 258 Cost: 0.1330 Fidelity: 0.9565 Trace: 0.9635\n",
      "Rep: 259 Cost: 0.0708 Fidelity: 0.9649 Trace: 0.9667\n",
      "Rep: 260 Cost: 0.0949 Fidelity: 0.9509 Trace: 0.9537\n",
      "Rep: 261 Cost: 0.1288 Fidelity: 0.9383 Trace: 0.9433\n",
      "Rep: 262 Cost: 0.0878 Fidelity: 0.9487 Trace: 0.9502\n",
      "Rep: 263 Cost: 0.0808 Fidelity: 0.9586 Trace: 0.9611\n",
      "Rep: 264 Cost: 0.1157 Fidelity: 0.9595 Trace: 0.9640\n",
      "Rep: 265 Cost: 0.0901 Fidelity: 0.9622 Trace: 0.9644\n",
      "Rep: 266 Cost: 0.0753 Fidelity: 0.9627 Trace: 0.9642\n",
      "Rep: 267 Cost: 0.0939 Fidelity: 0.9617 Trace: 0.9647\n",
      "Rep: 268 Cost: 0.0564 Fidelity: 0.9656 Trace: 0.9665\n",
      "Rep: 269 Cost: 0.0945 Fidelity: 0.9600 Trace: 0.9635\n",
      "Rep: 270 Cost: 0.1154 Fidelity: 0.9623 Trace: 0.9647\n",
      "Rep: 271 Cost: 0.0896 Fidelity: 0.9667 Trace: 0.9671\n",
      "Rep: 272 Cost: 0.0944 Fidelity: 0.9549 Trace: 0.9580\n",
      "Rep: 273 Cost: 0.1014 Fidelity: 0.9574 Trace: 0.9597\n",
      "Rep: 274 Cost: 0.1118 Fidelity: 0.9628 Trace: 0.9683\n",
      "Rep: 275 Cost: 0.0834 Fidelity: 0.9676 Trace: 0.9706\n",
      "Rep: 276 Cost: 0.0780 Fidelity: 0.9648 Trace: 0.9672\n",
      "Rep: 277 Cost: 0.1121 Fidelity: 0.9641 Trace: 0.9658\n",
      "Rep: 278 Cost: 0.1105 Fidelity: 0.9655 Trace: 0.9667\n",
      "Rep: 279 Cost: 0.0766 Fidelity: 0.9688 Trace: 0.9701\n",
      "Rep: 280 Cost: 0.0475 Fidelity: 0.9722 Trace: 0.9725\n",
      "Rep: 281 Cost: 0.0717 Fidelity: 0.9675 Trace: 0.9689\n",
      "Rep: 282 Cost: 0.0469 Fidelity: 0.9722 Trace: 0.9728\n",
      "Rep: 283 Cost: 0.0989 Fidelity: 0.9665 Trace: 0.9718\n",
      "Rep: 284 Cost: 0.1016 Fidelity: 0.9677 Trace: 0.9710\n",
      "Rep: 285 Cost: 0.0698 Fidelity: 0.9730 Trace: 0.9739\n",
      "Rep: 286 Cost: 0.1116 Fidelity: 0.9675 Trace: 0.9703\n",
      "Rep: 287 Cost: 0.0622 Fidelity: 0.9725 Trace: 0.9728\n",
      "Rep: 288 Cost: 0.1520 Fidelity: 0.9601 Trace: 0.9694\n",
      "Rep: 289 Cost: 0.1544 Fidelity: 0.9598 Trace: 0.9711\n",
      "Rep: 290 Cost: 0.1071 Fidelity: 0.9658 Trace: 0.9708\n",
      "Rep: 291 Cost: 0.0768 Fidelity: 0.9596 Trace: 0.9612\n",
      "Rep: 292 Cost: 0.1451 Fidelity: 0.9552 Trace: 0.9627\n",
      "Rep: 293 Cost: 0.1219 Fidelity: 0.9641 Trace: 0.9694\n",
      "Rep: 294 Cost: 0.1084 Fidelity: 0.9691 Trace: 0.9710\n",
      "Rep: 295 Cost: 0.0936 Fidelity: 0.9718 Trace: 0.9730\n",
      "Rep: 296 Cost: 0.1110 Fidelity: 0.9624 Trace: 0.9670\n",
      "Rep: 297 Cost: 0.0979 Fidelity: 0.9592 Trace: 0.9623\n",
      "Rep: 298 Cost: 0.1099 Fidelity: 0.9610 Trace: 0.9651\n",
      "Rep: 299 Cost: 0.0936 Fidelity: 0.9658 Trace: 0.9693\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1575e-04\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7132e-04\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6976e-04\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2611e-04\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1911e-04\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0957e-04\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4297e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8824e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9322e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6718e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2573e-04\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1845e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.8576e-05\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 5.2224e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0415e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4951e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1370e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5162e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5617e-06\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1587e-05\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0017e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7395e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3675e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5571e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8367e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7625e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7287e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0849e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8696e-06\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1011e-06\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9083e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1959e-06\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1318e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7405e-06\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5415e-06\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5963e-06\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2537e-06\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.9105e-06\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9186e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0850e-06\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7763e-07\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0780e-06\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3804e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2372e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8910e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1737e-06\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1928e-06\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3148e-06\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9701e-06\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2556e-06\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.6852e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2880e-08\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0136e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8225e-07\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8512e-07\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0801e-07\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6751e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.2176e-07\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2318e-07\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5535e-07\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5196e-07\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1217e-07\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3012e-08\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5120e-07\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6026e-07\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4053e-07\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1142e-07\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1839e-07\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5228e-07\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7700e-07\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0631e-07\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.0142e-08\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.1841e-08\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9500e-08\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8706e-08\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.9159e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2906e-08\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6475e-08\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6638e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.1418e-08\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.7776e-08\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8342e-08\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1835e-08\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1371e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7280e-08\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1173e-08\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0819e-08\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5874e-08\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8036e-08\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8741e-08\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.1979e-08\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5360e-08\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4673e-08\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9324e-09\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5090e-09\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6514e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2232e-09\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5459e-09\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6128e-09\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2648e-09\n",
      "[[-0.34595335 -0.500751  ]]\n",
      "[1. 0. 0.]\n",
      "Rep: 0 Cost: 1.2359 Fidelity: 0.0076 Trace: 0.0574\n",
      "Rep: 1 Cost: 1.3961 Fidelity: 0.0178 Trace: 0.0539\n",
      "Rep: 2 Cost: 1.1855 Fidelity: 0.0005 Trace: 0.0271\n",
      "Rep: 3 Cost: 1.1105 Fidelity: 0.0191 Trace: 0.0314\n",
      "Rep: 4 Cost: 1.0380 Fidelity: 0.0099 Trace: 0.0183\n",
      "Rep: 5 Cost: 1.0069 Fidelity: 0.0107 Trace: 0.0127\n",
      "Rep: 6 Cost: 0.9754 Fidelity: 0.0081 Trace: 0.0098\n",
      "Rep: 7 Cost: 0.9588 Fidelity: 0.0061 Trace: 0.0067\n",
      "Rep: 8 Cost: 0.9924 Fidelity: 0.0044 Trace: 0.0056\n",
      "Rep: 9 Cost: 1.0092 Fidelity: 0.0030 Trace: 0.0048\n",
      "Rep: 10 Cost: 1.0085 Fidelity: 0.0023 Trace: 0.0042\n",
      "Rep: 11 Cost: 1.0012 Fidelity: 0.0021 Trace: 0.0036\n",
      "Rep: 12 Cost: 0.9910 Fidelity: 0.0019 Trace: 0.0029\n",
      "Rep: 13 Cost: 0.9897 Fidelity: 0.0017 Trace: 0.0023\n",
      "Rep: 14 Cost: 0.9865 Fidelity: 0.0017 Trace: 0.0020\n",
      "Rep: 15 Cost: 0.9813 Fidelity: 0.0017 Trace: 0.0020\n",
      "Rep: 16 Cost: 0.9766 Fidelity: 0.0019 Trace: 0.0021\n",
      "Rep: 17 Cost: 0.9743 Fidelity: 0.0020 Trace: 0.0022\n",
      "Rep: 18 Cost: 0.9716 Fidelity: 0.0022 Trace: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 19 Cost: 0.9658 Fidelity: 0.0026 Trace: 0.0028\n",
      "Rep: 20 Cost: 0.9601 Fidelity: 0.0033 Trace: 0.0034\n",
      "Rep: 21 Cost: 0.9571 Fidelity: 0.0041 Trace: 0.0043\n",
      "Rep: 22 Cost: 0.9505 Fidelity: 0.0051 Trace: 0.0053\n",
      "Rep: 23 Cost: 0.9390 Fidelity: 0.0065 Trace: 0.0068\n",
      "Rep: 24 Cost: 0.9231 Fidelity: 0.0089 Trace: 0.0091\n",
      "Rep: 25 Cost: 0.9124 Fidelity: 0.0124 Trace: 0.0127\n",
      "Rep: 26 Cost: 0.8993 Fidelity: 0.0165 Trace: 0.0169\n",
      "Rep: 27 Cost: 0.8768 Fidelity: 0.0217 Trace: 0.0221\n",
      "Rep: 28 Cost: 0.8459 Fidelity: 0.0283 Trace: 0.0284\n",
      "Rep: 29 Cost: 0.8191 Fidelity: 0.0374 Trace: 0.0374\n",
      "Rep: 30 Cost: 0.8238 Fidelity: 0.0520 Trace: 0.0533\n",
      "Rep: 31 Cost: 0.7595 Fidelity: 0.0676 Trace: 0.0679\n",
      "Rep: 32 Cost: 0.7576 Fidelity: 0.0766 Trace: 0.0772\n",
      "Rep: 33 Cost: 0.7663 Fidelity: 0.0930 Trace: 0.0953\n",
      "Rep: 34 Cost: 0.6953 Fidelity: 0.1073 Trace: 0.1076\n",
      "Rep: 35 Cost: 0.8046 Fidelity: 0.0976 Trace: 0.1037\n",
      "Rep: 36 Cost: 0.7661 Fidelity: 0.1073 Trace: 0.1114\n",
      "Rep: 37 Cost: 0.7112 Fidelity: 0.1223 Trace: 0.1231\n",
      "Rep: 38 Cost: 0.7268 Fidelity: 0.1216 Trace: 0.1246\n",
      "Rep: 39 Cost: 0.7388 Fidelity: 0.1208 Trace: 0.1247\n",
      "Rep: 40 Cost: 0.6873 Fidelity: 0.1335 Trace: 0.1349\n",
      "Rep: 41 Cost: 0.6536 Fidelity: 0.1492 Trace: 0.1499\n",
      "Rep: 42 Cost: 0.6963 Fidelity: 0.1508 Trace: 0.1542\n",
      "Rep: 43 Cost: 0.6614 Fidelity: 0.1531 Trace: 0.1554\n",
      "Rep: 44 Cost: 0.7063 Fidelity: 0.1652 Trace: 0.1736\n",
      "Rep: 45 Cost: 0.6758 Fidelity: 0.1889 Trace: 0.1959\n",
      "Rep: 46 Cost: 0.6483 Fidelity: 0.1931 Trace: 0.1960\n",
      "Rep: 47 Cost: 0.6373 Fidelity: 0.2046 Trace: 0.2101\n",
      "Rep: 48 Cost: 0.6109 Fidelity: 0.2309 Trace: 0.2354\n",
      "Rep: 49 Cost: 0.5382 Fidelity: 0.2617 Trace: 0.2627\n",
      "Rep: 50 Cost: 0.5227 Fidelity: 0.2721 Trace: 0.2731\n",
      "Rep: 51 Cost: 0.5387 Fidelity: 0.2859 Trace: 0.2870\n",
      "Rep: 52 Cost: 0.5353 Fidelity: 0.3024 Trace: 0.3066\n",
      "Rep: 53 Cost: 0.5012 Fidelity: 0.3085 Trace: 0.3112\n",
      "Rep: 54 Cost: 0.4979 Fidelity: 0.3234 Trace: 0.3256\n",
      "Rep: 55 Cost: 0.4485 Fidelity: 0.3534 Trace: 0.3545\n",
      "Rep: 56 Cost: 0.4724 Fidelity: 0.3612 Trace: 0.3637\n",
      "Rep: 57 Cost: 0.4399 Fidelity: 0.3735 Trace: 0.3748\n",
      "Rep: 58 Cost: 0.4421 Fidelity: 0.3838 Trace: 0.3867\n",
      "Rep: 59 Cost: 0.4134 Fidelity: 0.4060 Trace: 0.4073\n",
      "Rep: 60 Cost: 0.4378 Fidelity: 0.4084 Trace: 0.4115\n",
      "Rep: 61 Cost: 0.3837 Fidelity: 0.4297 Trace: 0.4305\n",
      "Rep: 62 Cost: 0.4666 Fidelity: 0.4121 Trace: 0.4180\n",
      "Rep: 63 Cost: 0.4500 Fidelity: 0.4226 Trace: 0.4274\n",
      "Rep: 64 Cost: 0.3979 Fidelity: 0.4618 Trace: 0.4643\n",
      "Rep: 65 Cost: 0.4042 Fidelity: 0.4643 Trace: 0.4688\n",
      "Rep: 66 Cost: 0.3716 Fidelity: 0.4702 Trace: 0.4723\n",
      "Rep: 67 Cost: 0.3876 Fidelity: 0.5005 Trace: 0.5050\n",
      "Rep: 68 Cost: 0.3753 Fidelity: 0.5088 Trace: 0.5131\n",
      "Rep: 69 Cost: 0.3329 Fidelity: 0.5232 Trace: 0.5242\n",
      "Rep: 70 Cost: 0.3347 Fidelity: 0.5231 Trace: 0.5246\n",
      "Rep: 71 Cost: 0.3222 Fidelity: 0.5507 Trace: 0.5529\n",
      "Rep: 72 Cost: 0.2902 Fidelity: 0.5494 Trace: 0.5498\n",
      "Rep: 73 Cost: 0.3797 Fidelity: 0.5615 Trace: 0.5663\n",
      "Rep: 74 Cost: 0.3385 Fidelity: 0.5593 Trace: 0.5614\n",
      "Rep: 75 Cost: 0.3840 Fidelity: 0.5802 Trace: 0.5865\n",
      "Rep: 76 Cost: 0.3555 Fidelity: 0.5689 Trace: 0.5738\n",
      "Rep: 77 Cost: 0.2935 Fidelity: 0.5898 Trace: 0.5916\n",
      "Rep: 78 Cost: 0.3163 Fidelity: 0.6032 Trace: 0.6073\n",
      "Rep: 79 Cost: 0.3123 Fidelity: 0.6101 Trace: 0.6124\n",
      "Rep: 80 Cost: 0.3605 Fidelity: 0.6436 Trace: 0.6476\n",
      "Rep: 81 Cost: 0.3297 Fidelity: 0.6167 Trace: 0.6215\n",
      "Rep: 82 Cost: 0.2299 Fidelity: 0.6340 Trace: 0.6344\n",
      "Rep: 83 Cost: 0.3593 Fidelity: 0.6237 Trace: 0.6336\n",
      "Rep: 84 Cost: 0.3697 Fidelity: 0.6188 Trace: 0.6286\n",
      "Rep: 85 Cost: 0.2739 Fidelity: 0.6429 Trace: 0.6440\n",
      "Rep: 86 Cost: 0.3177 Fidelity: 0.6338 Trace: 0.6394\n",
      "Rep: 87 Cost: 0.3158 Fidelity: 0.6468 Trace: 0.6558\n",
      "Rep: 88 Cost: 0.2497 Fidelity: 0.6743 Trace: 0.6779\n",
      "Rep: 89 Cost: 0.2555 Fidelity: 0.6708 Trace: 0.6731\n",
      "Rep: 90 Cost: 0.2911 Fidelity: 0.6880 Trace: 0.6911\n",
      "Rep: 91 Cost: 0.2437 Fidelity: 0.6866 Trace: 0.6869\n",
      "Rep: 92 Cost: 0.2211 Fidelity: 0.7043 Trace: 0.7050\n",
      "Rep: 93 Cost: 0.2456 Fidelity: 0.6989 Trace: 0.7020\n",
      "Rep: 94 Cost: 0.2210 Fidelity: 0.7141 Trace: 0.7157\n",
      "Rep: 95 Cost: 0.2115 Fidelity: 0.7121 Trace: 0.7127\n",
      "Rep: 96 Cost: 0.2313 Fidelity: 0.7250 Trace: 0.7269\n",
      "Rep: 97 Cost: 0.2078 Fidelity: 0.7291 Trace: 0.7294\n",
      "Rep: 98 Cost: 0.2415 Fidelity: 0.7413 Trace: 0.7427\n",
      "Rep: 99 Cost: 0.2166 Fidelity: 0.7375 Trace: 0.7388\n",
      "Rep: 100 Cost: 0.1629 Fidelity: 0.7515 Trace: 0.7517\n",
      "Rep: 101 Cost: 0.2627 Fidelity: 0.7327 Trace: 0.7397\n",
      "Rep: 102 Cost: 0.2554 Fidelity: 0.7629 Trace: 0.7661\n",
      "Rep: 103 Cost: 0.2571 Fidelity: 0.7448 Trace: 0.7473\n",
      "Rep: 104 Cost: 0.1784 Fidelity: 0.7588 Trace: 0.7598\n",
      "Rep: 105 Cost: 0.2102 Fidelity: 0.7704 Trace: 0.7737\n",
      "Rep: 106 Cost: 0.2221 Fidelity: 0.7731 Trace: 0.7738\n",
      "Rep: 107 Cost: 0.2162 Fidelity: 0.7810 Trace: 0.7840\n",
      "Rep: 108 Cost: 0.1761 Fidelity: 0.7799 Trace: 0.7816\n",
      "Rep: 109 Cost: 0.2027 Fidelity: 0.7723 Trace: 0.7768\n",
      "Rep: 110 Cost: 0.2018 Fidelity: 0.7839 Trace: 0.7864\n",
      "Rep: 111 Cost: 0.2146 Fidelity: 0.7986 Trace: 0.8009\n",
      "Rep: 112 Cost: 0.2008 Fidelity: 0.7876 Trace: 0.7920\n",
      "Rep: 113 Cost: 0.1707 Fidelity: 0.7992 Trace: 0.8004\n",
      "Rep: 114 Cost: 0.3061 Fidelity: 0.7870 Trace: 0.7997\n",
      "Rep: 115 Cost: 0.2750 Fidelity: 0.7976 Trace: 0.8088\n",
      "Rep: 116 Cost: 0.1552 Fidelity: 0.8092 Trace: 0.8103\n",
      "Rep: 117 Cost: 0.1572 Fidelity: 0.8127 Trace: 0.8142\n",
      "Rep: 118 Cost: 0.1675 Fidelity: 0.8168 Trace: 0.8198\n",
      "Rep: 119 Cost: 0.1513 Fidelity: 0.8221 Trace: 0.8247\n",
      "Rep: 120 Cost: 0.1316 Fidelity: 0.8240 Trace: 0.8246\n",
      "Rep: 121 Cost: 0.2757 Fidelity: 0.8194 Trace: 0.8274\n",
      "Rep: 122 Cost: 0.2143 Fidelity: 0.8186 Trace: 0.8254\n",
      "Rep: 123 Cost: 0.1795 Fidelity: 0.8251 Trace: 0.8294\n",
      "Rep: 124 Cost: 0.1918 Fidelity: 0.8304 Trace: 0.8336\n",
      "Rep: 125 Cost: 0.2119 Fidelity: 0.8320 Trace: 0.8367\n",
      "Rep: 126 Cost: 0.1911 Fidelity: 0.8309 Trace: 0.8354\n",
      "Rep: 127 Cost: 0.1897 Fidelity: 0.8334 Trace: 0.8373\n",
      "Rep: 128 Cost: 0.1874 Fidelity: 0.8392 Trace: 0.8411\n",
      "Rep: 129 Cost: 0.1825 Fidelity: 0.8433 Trace: 0.8481\n",
      "Rep: 130 Cost: 0.1592 Fidelity: 0.8465 Trace: 0.8501\n",
      "Rep: 131 Cost: 0.1553 Fidelity: 0.8429 Trace: 0.8465\n",
      "Rep: 132 Cost: 0.1556 Fidelity: 0.8459 Trace: 0.8483\n",
      "Rep: 133 Cost: 0.2046 Fidelity: 0.8545 Trace: 0.8580\n",
      "Rep: 134 Cost: 0.1729 Fidelity: 0.8533 Trace: 0.8568\n",
      "Rep: 135 Cost: 0.1494 Fidelity: 0.8583 Trace: 0.8612\n",
      "Rep: 136 Cost: 0.1662 Fidelity: 0.8565 Trace: 0.8588\n",
      "Rep: 137 Cost: 0.1576 Fidelity: 0.8627 Trace: 0.8642\n",
      "Rep: 138 Cost: 0.1048 Fidelity: 0.8669 Trace: 0.8675\n",
      "Rep: 139 Cost: 0.1638 Fidelity: 0.8599 Trace: 0.8654\n",
      "Rep: 140 Cost: 0.1423 Fidelity: 0.8643 Trace: 0.8677\n",
      "Rep: 141 Cost: 0.1365 Fidelity: 0.8694 Trace: 0.8721\n",
      "Rep: 142 Cost: 0.1477 Fidelity: 0.8700 Trace: 0.8728\n",
      "Rep: 143 Cost: 0.1816 Fidelity: 0.8758 Trace: 0.8796\n",
      "Rep: 144 Cost: 0.1639 Fidelity: 0.8782 Trace: 0.8803\n",
      "Rep: 145 Cost: 0.1222 Fidelity: 0.8824 Trace: 0.8848\n",
      "Rep: 146 Cost: 0.1138 Fidelity: 0.8853 Trace: 0.8865\n",
      "Rep: 147 Cost: 0.1872 Fidelity: 0.8875 Trace: 0.8892\n",
      "Rep: 148 Cost: 0.0896 Fidelity: 0.8898 Trace: 0.8906\n",
      "Rep: 149 Cost: 0.1850 Fidelity: 0.8915 Trace: 0.8940\n",
      "Rep: 150 Cost: 0.1228 Fidelity: 0.8950 Trace: 0.8965\n",
      "Rep: 151 Cost: 0.1491 Fidelity: 0.8898 Trace: 0.8951\n",
      "Rep: 152 Cost: 0.1481 Fidelity: 0.8925 Trace: 0.8959\n",
      "Rep: 153 Cost: 0.1329 Fidelity: 0.8986 Trace: 0.9006\n",
      "Rep: 154 Cost: 0.1254 Fidelity: 0.8986 Trace: 0.9014\n",
      "Rep: 155 Cost: 0.1418 Fidelity: 0.9032 Trace: 0.9071\n",
      "Rep: 156 Cost: 0.1363 Fidelity: 0.9075 Trace: 0.9080\n",
      "Rep: 157 Cost: 0.1233 Fidelity: 0.9017 Trace: 0.9056\n",
      "Rep: 158 Cost: 0.1435 Fidelity: 0.9115 Trace: 0.9125\n",
      "Rep: 159 Cost: 0.1485 Fidelity: 0.9086 Trace: 0.9134\n",
      "Rep: 160 Cost: 0.1234 Fidelity: 0.9100 Trace: 0.9142\n",
      "Rep: 161 Cost: 0.1155 Fidelity: 0.9125 Trace: 0.9141\n",
      "Rep: 162 Cost: 0.1387 Fidelity: 0.9127 Trace: 0.9156\n",
      "Rep: 163 Cost: 0.1381 Fidelity: 0.9164 Trace: 0.9187\n",
      "Rep: 164 Cost: 0.1018 Fidelity: 0.9198 Trace: 0.9202\n",
      "Rep: 165 Cost: 0.1334 Fidelity: 0.9040 Trace: 0.9097\n",
      "Rep: 166 Cost: 0.1734 Fidelity: 0.9161 Trace: 0.9198\n",
      "Rep: 167 Cost: 0.0586 Fidelity: 0.9245 Trace: 0.9247\n",
      "Rep: 168 Cost: 0.1172 Fidelity: 0.9203 Trace: 0.9213\n",
      "Rep: 169 Cost: 0.0989 Fidelity: 0.9261 Trace: 0.9262\n",
      "Rep: 170 Cost: 0.0742 Fidelity: 0.9263 Trace: 0.9269\n",
      "Rep: 171 Cost: 0.1538 Fidelity: 0.9296 Trace: 0.9304\n",
      "Rep: 172 Cost: 0.0694 Fidelity: 0.9315 Trace: 0.9317\n",
      "Rep: 173 Cost: 0.2203 Fidelity: 0.9231 Trace: 0.9255\n",
      "Rep: 174 Cost: 0.2185 Fidelity: 0.9264 Trace: 0.9289\n",
      "Rep: 175 Cost: 0.0501 Fidelity: 0.9344 Trace: 0.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 176 Cost: 0.2166 Fidelity: 0.9208 Trace: 0.9259\n",
      "Rep: 177 Cost: 0.1505 Fidelity: 0.9224 Trace: 0.9275\n",
      "Rep: 178 Cost: 0.1918 Fidelity: 0.9321 Trace: 0.9336\n",
      "Rep: 179 Cost: 0.2055 Fidelity: 0.9206 Trace: 0.9300\n",
      "Rep: 180 Cost: 0.2731 Fidelity: 0.8827 Trace: 0.9016\n",
      "Rep: 181 Cost: 0.1476 Fidelity: 0.9109 Trace: 0.9170\n",
      "Rep: 182 Cost: 0.2861 Fidelity: 0.9122 Trace: 0.9239\n",
      "Rep: 183 Cost: 0.2496 Fidelity: 0.9051 Trace: 0.9224\n",
      "Rep: 184 Cost: 0.2559 Fidelity: 0.9172 Trace: 0.9224\n",
      "Rep: 185 Cost: 0.2956 Fidelity: 0.9054 Trace: 0.9095\n",
      "Rep: 186 Cost: 0.1447 Fidelity: 0.9151 Trace: 0.9195\n",
      "Rep: 187 Cost: 0.3072 Fidelity: 0.9150 Trace: 0.9197\n",
      "Rep: 188 Cost: 0.3495 Fidelity: 0.8951 Trace: 0.8986\n",
      "Rep: 189 Cost: 0.2539 Fidelity: 0.8863 Trace: 0.8912\n",
      "Rep: 190 Cost: 0.1695 Fidelity: 0.8935 Trace: 0.8963\n",
      "Rep: 191 Cost: 0.2558 Fidelity: 0.8965 Trace: 0.8999\n",
      "Rep: 192 Cost: 0.2045 Fidelity: 0.8900 Trace: 0.8942\n",
      "Rep: 193 Cost: 0.1243 Fidelity: 0.8877 Trace: 0.8880\n",
      "Rep: 194 Cost: 0.1982 Fidelity: 0.8984 Trace: 0.9002\n",
      "Rep: 195 Cost: 0.0926 Fidelity: 0.9091 Trace: 0.9096\n",
      "Rep: 196 Cost: 0.1667 Fidelity: 0.9115 Trace: 0.9120\n",
      "Rep: 197 Cost: 0.2001 Fidelity: 0.9184 Trace: 0.9193\n",
      "Rep: 198 Cost: 0.0896 Fidelity: 0.9221 Trace: 0.9226\n",
      "Rep: 199 Cost: 0.1973 Fidelity: 0.9182 Trace: 0.9198\n",
      "Rep: 200 Cost: 0.2379 Fidelity: 0.9165 Trace: 0.9175\n",
      "Rep: 201 Cost: 0.1891 Fidelity: 0.9085 Trace: 0.9106\n",
      "Rep: 202 Cost: 0.1045 Fidelity: 0.9160 Trace: 0.9162\n",
      "Rep: 203 Cost: 0.2516 Fidelity: 0.9094 Trace: 0.9155\n",
      "Rep: 204 Cost: 0.1890 Fidelity: 0.9139 Trace: 0.9194\n",
      "Rep: 205 Cost: 0.1280 Fidelity: 0.9157 Trace: 0.9172\n",
      "Rep: 206 Cost: 0.1845 Fidelity: 0.9016 Trace: 0.9061\n",
      "Rep: 207 Cost: 0.1244 Fidelity: 0.9141 Trace: 0.9186\n",
      "Rep: 208 Cost: 0.1300 Fidelity: 0.9289 Trace: 0.9297\n",
      "Rep: 209 Cost: 0.1636 Fidelity: 0.9072 Trace: 0.9125\n",
      "Rep: 210 Cost: 0.1220 Fidelity: 0.8952 Trace: 0.8981\n",
      "Rep: 211 Cost: 0.1418 Fidelity: 0.9213 Trace: 0.9259\n",
      "Rep: 212 Cost: 0.1419 Fidelity: 0.9280 Trace: 0.9366\n",
      "Rep: 213 Cost: 0.0800 Fidelity: 0.9399 Trace: 0.9412\n",
      "Rep: 214 Cost: 0.1868 Fidelity: 0.9147 Trace: 0.9247\n",
      "Rep: 215 Cost: 0.1731 Fidelity: 0.9152 Trace: 0.9267\n",
      "Rep: 216 Cost: 0.1134 Fidelity: 0.9348 Trace: 0.9368\n",
      "Rep: 217 Cost: 0.1244 Fidelity: 0.9325 Trace: 0.9383\n",
      "Rep: 218 Cost: 0.1718 Fidelity: 0.9353 Trace: 0.9424\n",
      "Rep: 219 Cost: 0.1104 Fidelity: 0.9437 Trace: 0.9476\n",
      "Rep: 220 Cost: 0.1385 Fidelity: 0.9459 Trace: 0.9484\n",
      "Rep: 221 Cost: 0.1476 Fidelity: 0.9345 Trace: 0.9392\n",
      "Rep: 222 Cost: 0.1738 Fidelity: 0.9287 Trace: 0.9358\n",
      "Rep: 223 Cost: 0.0733 Fidelity: 0.9491 Trace: 0.9502\n",
      "Rep: 224 Cost: 0.1973 Fidelity: 0.8971 Trace: 0.9022\n",
      "Rep: 225 Cost: 0.1821 Fidelity: 0.8875 Trace: 0.8948\n",
      "Rep: 226 Cost: 0.1575 Fidelity: 0.9349 Trace: 0.9370\n",
      "Rep: 227 Cost: 0.1993 Fidelity: 0.9381 Trace: 0.9449\n",
      "Rep: 228 Cost: 0.1833 Fidelity: 0.9275 Trace: 0.9357\n",
      "Rep: 229 Cost: 0.1338 Fidelity: 0.9348 Trace: 0.9362\n",
      "Rep: 230 Cost: 0.1459 Fidelity: 0.9304 Trace: 0.9377\n",
      "Rep: 231 Cost: 0.1898 Fidelity: 0.9267 Trace: 0.9370\n",
      "Rep: 232 Cost: 0.0923 Fidelity: 0.9415 Trace: 0.9443\n",
      "Rep: 233 Cost: 0.1295 Fidelity: 0.9432 Trace: 0.9481\n",
      "Rep: 234 Cost: 0.1892 Fidelity: 0.9321 Trace: 0.9421\n",
      "Rep: 235 Cost: 0.1330 Fidelity: 0.9390 Trace: 0.9442\n",
      "Rep: 236 Cost: 0.1127 Fidelity: 0.9434 Trace: 0.9452\n",
      "Rep: 237 Cost: 0.1072 Fidelity: 0.9367 Trace: 0.9398\n",
      "Rep: 238 Cost: 0.1075 Fidelity: 0.9349 Trace: 0.9362\n",
      "Rep: 239 Cost: 0.1144 Fidelity: 0.9317 Trace: 0.9350\n",
      "Rep: 240 Cost: 0.1270 Fidelity: 0.9376 Trace: 0.9411\n",
      "Rep: 241 Cost: 0.0972 Fidelity: 0.9423 Trace: 0.9451\n",
      "Rep: 242 Cost: 0.0925 Fidelity: 0.9408 Trace: 0.9431\n",
      "Rep: 243 Cost: 0.1010 Fidelity: 0.9374 Trace: 0.9394\n",
      "Rep: 244 Cost: 0.0921 Fidelity: 0.9377 Trace: 0.9403\n",
      "Rep: 245 Cost: 0.0899 Fidelity: 0.9425 Trace: 0.9437\n",
      "Rep: 246 Cost: 0.0724 Fidelity: 0.9459 Trace: 0.9466\n",
      "Rep: 247 Cost: 0.0741 Fidelity: 0.9472 Trace: 0.9485\n",
      "Rep: 248 Cost: 0.0638 Fidelity: 0.9492 Trace: 0.9494\n",
      "Rep: 249 Cost: 0.1126 Fidelity: 0.9423 Trace: 0.9469\n",
      "Rep: 250 Cost: 0.1132 Fidelity: 0.9392 Trace: 0.9445\n",
      "Rep: 251 Cost: 0.0904 Fidelity: 0.9395 Trace: 0.9413\n",
      "Rep: 252 Cost: 0.0803 Fidelity: 0.9449 Trace: 0.9461\n",
      "Rep: 253 Cost: 0.0771 Fidelity: 0.9474 Trace: 0.9493\n",
      "Rep: 254 Cost: 0.0955 Fidelity: 0.9444 Trace: 0.9467\n",
      "Rep: 255 Cost: 0.0865 Fidelity: 0.9472 Trace: 0.9487\n",
      "Rep: 256 Cost: 0.0908 Fidelity: 0.9515 Trace: 0.9534\n",
      "Rep: 257 Cost: 0.0911 Fidelity: 0.9473 Trace: 0.9502\n",
      "Rep: 258 Cost: 0.0551 Fidelity: 0.9486 Trace: 0.9490\n",
      "Rep: 259 Cost: 0.0998 Fidelity: 0.9501 Trace: 0.9535\n",
      "Rep: 260 Cost: 0.1238 Fidelity: 0.9456 Trace: 0.9514\n",
      "Rep: 261 Cost: 0.0823 Fidelity: 0.9537 Trace: 0.9558\n",
      "Rep: 262 Cost: 0.0741 Fidelity: 0.9555 Trace: 0.9579\n",
      "Rep: 263 Cost: 0.1010 Fidelity: 0.9501 Trace: 0.9531\n",
      "Rep: 264 Cost: 0.1003 Fidelity: 0.9496 Trace: 0.9520\n",
      "Rep: 265 Cost: 0.0946 Fidelity: 0.9531 Trace: 0.9556\n",
      "Rep: 266 Cost: 0.0575 Fidelity: 0.9564 Trace: 0.9570\n",
      "Rep: 267 Cost: 0.0979 Fidelity: 0.9489 Trace: 0.9509\n",
      "Rep: 268 Cost: 0.0925 Fidelity: 0.9470 Trace: 0.9492\n",
      "Rep: 269 Cost: 0.0865 Fidelity: 0.9510 Trace: 0.9529\n",
      "Rep: 270 Cost: 0.0722 Fidelity: 0.9582 Trace: 0.9594\n",
      "Rep: 271 Cost: 0.0805 Fidelity: 0.9580 Trace: 0.9595\n",
      "Rep: 272 Cost: 0.0795 Fidelity: 0.9578 Trace: 0.9596\n",
      "Rep: 273 Cost: 0.0549 Fidelity: 0.9614 Trace: 0.9624\n",
      "Rep: 274 Cost: 0.0859 Fidelity: 0.9600 Trace: 0.9615\n",
      "Rep: 275 Cost: 0.0422 Fidelity: 0.9626 Trace: 0.9630\n",
      "Rep: 276 Cost: 0.0834 Fidelity: 0.9618 Trace: 0.9635\n",
      "Rep: 277 Cost: 0.0562 Fidelity: 0.9638 Trace: 0.9644\n",
      "Rep: 278 Cost: 0.1060 Fidelity: 0.9583 Trace: 0.9623\n",
      "Rep: 279 Cost: 0.1027 Fidelity: 0.9596 Trace: 0.9626\n",
      "Rep: 280 Cost: 0.0526 Fidelity: 0.9618 Trace: 0.9622\n",
      "Rep: 281 Cost: 0.1264 Fidelity: 0.9596 Trace: 0.9608\n",
      "Rep: 282 Cost: 0.1054 Fidelity: 0.9644 Trace: 0.9650\n",
      "Rep: 283 Cost: 0.0634 Fidelity: 0.9651 Trace: 0.9656\n",
      "Rep: 284 Cost: 0.0749 Fidelity: 0.9624 Trace: 0.9628\n",
      "Rep: 285 Cost: 0.1045 Fidelity: 0.9556 Trace: 0.9573\n",
      "Rep: 286 Cost: 0.0621 Fidelity: 0.9618 Trace: 0.9625\n",
      "Rep: 287 Cost: 0.1293 Fidelity: 0.9569 Trace: 0.9590\n",
      "Rep: 288 Cost: 0.1252 Fidelity: 0.9529 Trace: 0.9559\n",
      "Rep: 289 Cost: 0.1034 Fidelity: 0.9618 Trace: 0.9629\n",
      "Rep: 290 Cost: 0.0935 Fidelity: 0.9623 Trace: 0.9651\n",
      "Rep: 291 Cost: 0.1435 Fidelity: 0.9556 Trace: 0.9592\n",
      "Rep: 292 Cost: 0.0758 Fidelity: 0.9567 Trace: 0.9572\n",
      "Rep: 293 Cost: 0.1375 Fidelity: 0.9520 Trace: 0.9550\n",
      "Rep: 294 Cost: 0.0998 Fidelity: 0.9581 Trace: 0.9590\n",
      "Rep: 295 Cost: 0.1363 Fidelity: 0.9525 Trace: 0.9567\n",
      "Rep: 296 Cost: 0.1400 Fidelity: 0.9526 Trace: 0.9576\n",
      "Rep: 297 Cost: 0.0710 Fidelity: 0.9606 Trace: 0.9607\n",
      "Rep: 298 Cost: 0.1344 Fidelity: 0.9475 Trace: 0.9515\n",
      "Rep: 299 Cost: 0.1159 Fidelity: 0.9457 Trace: 0.9498\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.8773e-04\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1892e-04\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8542e-04\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3871e-04\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8469e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5501e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2072e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0638e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0201e-04\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3441e-04\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3347e-04\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7226e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5863e-04\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0809e-04\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.2444e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.1599e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7017e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3616e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6433e-05\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 8.4359e-06\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5029e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.2707e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0186e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.4014e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.6518e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.1807e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1555e-05\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7476e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3520e-06\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0964e-06\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8747e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6858e-06\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9542e-06\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2149e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3206e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1668e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2574e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0033e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5009e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.1865e-06\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4744e-06\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4147e-07\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8756e-07\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9457e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8659e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1571e-06\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1486e-06\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4573e-06\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2202e-06\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9273e-06\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5805e-06\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6341e-07\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0977e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3986e-07\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3238e-07\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4110e-07\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0526e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.7017e-07\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2205e-06\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0680e-06\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.3716e-07\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7904e-07\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5670e-07\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7161e-07\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6312e-07\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5366e-08\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5217e-08\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9595e-07\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5710e-07\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6305e-07\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.4887e-07\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2071e-07\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2207e-07\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9428e-07\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2165e-07\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2379e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3078e-08\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9797e-08\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7873e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3111e-08\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8846e-08\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3139e-08\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.9480e-08\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.1817e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.7755e-08\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6114e-08\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9355e-08\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1050e-09\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1267e-09\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1011e-08\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.0412e-09\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1774e-08\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9452e-08\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5947e-08\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8253e-08\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4846e-08\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4023e-08\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0371e-08\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3313e-09\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5484e-10\n",
      "[[-0.35760087 -0.5067742 ]]\n",
      "[1. 0. 0.]\n",
      "Rep: 0 Cost: 1.2086 Fidelity: 0.0052 Trace: 0.0494\n",
      "Rep: 1 Cost: 1.1880 Fidelity: 0.0356 Trace: 0.0390\n",
      "Rep: 2 Cost: 1.2484 Fidelity: 0.0162 Trace: 0.0413\n",
      "Rep: 3 Cost: 1.2475 Fidelity: 0.0227 Trace: 0.0428\n",
      "Rep: 4 Cost: 1.0488 Fidelity: 0.0118 Trace: 0.0299\n",
      "Rep: 5 Cost: 1.0032 Fidelity: 0.0100 Trace: 0.0127\n",
      "Rep: 6 Cost: 1.0565 Fidelity: 0.0044 Trace: 0.0122\n",
      "Rep: 7 Cost: 1.0284 Fidelity: 0.0042 Trace: 0.0102\n",
      "Rep: 8 Cost: 0.9950 Fidelity: 0.0061 Trace: 0.0090\n",
      "Rep: 9 Cost: 0.9569 Fidelity: 0.0097 Trace: 0.0109\n",
      "Rep: 10 Cost: 0.9641 Fidelity: 0.0117 Trace: 0.0148\n",
      "Rep: 11 Cost: 0.9896 Fidelity: 0.0126 Trace: 0.0181\n",
      "Rep: 12 Cost: 0.9626 Fidelity: 0.0158 Trace: 0.0200\n",
      "Rep: 13 Cost: 0.8979 Fidelity: 0.0196 Trace: 0.0204\n",
      "Rep: 14 Cost: 0.9258 Fidelity: 0.0175 Trace: 0.0194\n",
      "Rep: 15 Cost: 0.9339 Fidelity: 0.0176 Trace: 0.0207\n",
      "Rep: 16 Cost: 0.8873 Fidelity: 0.0246 Trace: 0.0256\n",
      "Rep: 17 Cost: 0.8922 Fidelity: 0.0305 Trace: 0.0320\n",
      "Rep: 18 Cost: 0.8740 Fidelity: 0.0339 Trace: 0.0360\n",
      "Rep: 19 Cost: 0.8593 Fidelity: 0.0388 Trace: 0.0403\n",
      "Rep: 20 Cost: 0.8547 Fidelity: 0.0479 Trace: 0.0488\n",
      "Rep: 21 Cost: 0.8140 Fidelity: 0.0545 Trace: 0.0557\n",
      "Rep: 22 Cost: 0.8415 Fidelity: 0.0562 Trace: 0.0577\n",
      "Rep: 23 Cost: 0.7719 Fidelity: 0.0630 Trace: 0.0631\n",
      "Rep: 24 Cost: 0.8510 Fidelity: 0.0626 Trace: 0.0672\n",
      "Rep: 25 Cost: 0.8497 Fidelity: 0.0738 Trace: 0.0770\n",
      "Rep: 26 Cost: 0.7513 Fidelity: 0.0794 Trace: 0.0799\n",
      "Rep: 27 Cost: 0.8401 Fidelity: 0.0693 Trace: 0.0728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 28 Cost: 0.8145 Fidelity: 0.0735 Trace: 0.0768\n",
      "Rep: 29 Cost: 0.7573 Fidelity: 0.0881 Trace: 0.0891\n",
      "Rep: 30 Cost: 0.8035 Fidelity: 0.0919 Trace: 0.0955\n",
      "Rep: 31 Cost: 0.7969 Fidelity: 0.0873 Trace: 0.0932\n",
      "Rep: 32 Cost: 0.7388 Fidelity: 0.0918 Trace: 0.0928\n",
      "Rep: 33 Cost: 0.7958 Fidelity: 0.0831 Trace: 0.0881\n",
      "Rep: 34 Cost: 0.8139 Fidelity: 0.0834 Trace: 0.0912\n",
      "Rep: 35 Cost: 0.7583 Fidelity: 0.0957 Trace: 0.0990\n",
      "Rep: 36 Cost: 0.7469 Fidelity: 0.0978 Trace: 0.1004\n",
      "Rep: 37 Cost: 0.7558 Fidelity: 0.0969 Trace: 0.1002\n",
      "Rep: 38 Cost: 0.7380 Fidelity: 0.1032 Trace: 0.1054\n",
      "Rep: 39 Cost: 0.7172 Fidelity: 0.1107 Trace: 0.1116\n",
      "Rep: 40 Cost: 0.7401 Fidelity: 0.1124 Trace: 0.1156\n",
      "Rep: 41 Cost: 0.7249 Fidelity: 0.1134 Trace: 0.1158\n",
      "Rep: 42 Cost: 0.6746 Fidelity: 0.1155 Trace: 0.1156\n",
      "Rep: 43 Cost: 0.6767 Fidelity: 0.1225 Trace: 0.1227\n",
      "Rep: 44 Cost: 0.6986 Fidelity: 0.1278 Trace: 0.1296\n",
      "Rep: 45 Cost: 0.6863 Fidelity: 0.1294 Trace: 0.1308\n",
      "Rep: 46 Cost: 0.6733 Fidelity: 0.1326 Trace: 0.1333\n",
      "Rep: 47 Cost: 0.6665 Fidelity: 0.1384 Trace: 0.1396\n",
      "Rep: 48 Cost: 0.6492 Fidelity: 0.1472 Trace: 0.1477\n",
      "Rep: 49 Cost: 0.6605 Fidelity: 0.1519 Trace: 0.1533\n",
      "Rep: 50 Cost: 0.6525 Fidelity: 0.1552 Trace: 0.1568\n",
      "Rep: 51 Cost: 0.6097 Fidelity: 0.1640 Trace: 0.1641\n",
      "Rep: 52 Cost: 0.6022 Fidelity: 0.1739 Trace: 0.1740\n",
      "Rep: 53 Cost: 0.6239 Fidelity: 0.1808 Trace: 0.1821\n",
      "Rep: 54 Cost: 0.5923 Fidelity: 0.1882 Trace: 0.1886\n",
      "Rep: 55 Cost: 0.6157 Fidelity: 0.1936 Trace: 0.1953\n",
      "Rep: 56 Cost: 0.5857 Fidelity: 0.2106 Trace: 0.2118\n",
      "Rep: 57 Cost: 0.5827 Fidelity: 0.2216 Trace: 0.2230\n",
      "Rep: 58 Cost: 0.5595 Fidelity: 0.2365 Trace: 0.2376\n",
      "Rep: 59 Cost: 0.5546 Fidelity: 0.2532 Trace: 0.2554\n",
      "Rep: 60 Cost: 0.5372 Fidelity: 0.2719 Trace: 0.2744\n",
      "Rep: 61 Cost: 0.5009 Fidelity: 0.2899 Trace: 0.2906\n",
      "Rep: 62 Cost: 0.4678 Fidelity: 0.3155 Trace: 0.3160\n",
      "Rep: 63 Cost: 0.4990 Fidelity: 0.3376 Trace: 0.3416\n",
      "Rep: 64 Cost: 0.4486 Fidelity: 0.3705 Trace: 0.3728\n",
      "Rep: 65 Cost: 0.4700 Fidelity: 0.3853 Trace: 0.3907\n",
      "Rep: 66 Cost: 0.4142 Fidelity: 0.4297 Trace: 0.4326\n",
      "Rep: 67 Cost: 0.4399 Fidelity: 0.4619 Trace: 0.4711\n",
      "Rep: 68 Cost: 0.4183 Fidelity: 0.5005 Trace: 0.5100\n",
      "Rep: 69 Cost: 0.3105 Fidelity: 0.5589 Trace: 0.5612\n",
      "Rep: 70 Cost: 0.3160 Fidelity: 0.5883 Trace: 0.5927\n",
      "Rep: 71 Cost: 0.2896 Fidelity: 0.6476 Trace: 0.6542\n",
      "Rep: 72 Cost: 0.2518 Fidelity: 0.7026 Trace: 0.7073\n",
      "Rep: 73 Cost: 0.3363 Fidelity: 0.7002 Trace: 0.7087\n",
      "Rep: 74 Cost: 0.3260 Fidelity: 0.7595 Trace: 0.7671\n",
      "Rep: 75 Cost: 0.2598 Fidelity: 0.7766 Trace: 0.7858\n",
      "Rep: 76 Cost: 0.2086 Fidelity: 0.8126 Trace: 0.8179\n",
      "Rep: 77 Cost: 0.2732 Fidelity: 0.7951 Trace: 0.8031\n",
      "Rep: 78 Cost: 0.2162 Fidelity: 0.8171 Trace: 0.8230\n",
      "Rep: 79 Cost: 0.2409 Fidelity: 0.8331 Trace: 0.8501\n",
      "Rep: 80 Cost: 0.2839 Fidelity: 0.8363 Trace: 0.8570\n",
      "Rep: 81 Cost: 0.1444 Fidelity: 0.8617 Trace: 0.8626\n",
      "Rep: 82 Cost: 0.3757 Fidelity: 0.7956 Trace: 0.8409\n",
      "Rep: 83 Cost: 0.3915 Fidelity: 0.7817 Trace: 0.8392\n",
      "Rep: 84 Cost: 0.2100 Fidelity: 0.8654 Trace: 0.8749\n",
      "Rep: 85 Cost: 0.2843 Fidelity: 0.8302 Trace: 0.8485\n",
      "Rep: 86 Cost: 0.3275 Fidelity: 0.7942 Trace: 0.8369\n",
      "Rep: 87 Cost: 0.2909 Fidelity: 0.8389 Trace: 0.8655\n",
      "Rep: 88 Cost: 0.1752 Fidelity: 0.8646 Trace: 0.8681\n",
      "Rep: 89 Cost: 0.2956 Fidelity: 0.8176 Trace: 0.8396\n",
      "Rep: 90 Cost: 0.2437 Fidelity: 0.8648 Trace: 0.8769\n",
      "Rep: 91 Cost: 0.1355 Fidelity: 0.8976 Trace: 0.9006\n",
      "Rep: 92 Cost: 0.1628 Fidelity: 0.8875 Trace: 0.8929\n",
      "Rep: 93 Cost: 0.1640 Fidelity: 0.8875 Trace: 0.8929\n",
      "Rep: 94 Cost: 0.1676 Fidelity: 0.8941 Trace: 0.8989\n",
      "Rep: 95 Cost: 0.1801 Fidelity: 0.8872 Trace: 0.8928\n",
      "Rep: 96 Cost: 0.0876 Fidelity: 0.8987 Trace: 0.8991\n",
      "Rep: 97 Cost: 0.1993 Fidelity: 0.8772 Trace: 0.8919\n",
      "Rep: 98 Cost: 0.2089 Fidelity: 0.8932 Trace: 0.9084\n",
      "Rep: 99 Cost: 0.1460 Fidelity: 0.9166 Trace: 0.9182\n",
      "Rep: 100 Cost: 0.2014 Fidelity: 0.8967 Trace: 0.9133\n",
      "Rep: 101 Cost: 0.2741 Fidelity: 0.8992 Trace: 0.9209\n",
      "Rep: 102 Cost: 0.1829 Fidelity: 0.9029 Trace: 0.9131\n",
      "Rep: 103 Cost: 0.2051 Fidelity: 0.9013 Trace: 0.9060\n",
      "Rep: 104 Cost: 0.1698 Fidelity: 0.9145 Trace: 0.9243\n",
      "Rep: 105 Cost: 0.1838 Fidelity: 0.9269 Trace: 0.9343\n",
      "Rep: 106 Cost: 0.1759 Fidelity: 0.9205 Trace: 0.9247\n",
      "Rep: 107 Cost: 0.1735 Fidelity: 0.9102 Trace: 0.9186\n",
      "Rep: 108 Cost: 0.1638 Fidelity: 0.9331 Trace: 0.9369\n",
      "Rep: 109 Cost: 0.1392 Fidelity: 0.9320 Trace: 0.9370\n",
      "Rep: 110 Cost: 0.1236 Fidelity: 0.9313 Trace: 0.9358\n",
      "Rep: 111 Cost: 0.1354 Fidelity: 0.9403 Trace: 0.9436\n",
      "Rep: 112 Cost: 0.1531 Fidelity: 0.9380 Trace: 0.9416\n",
      "Rep: 113 Cost: 0.0649 Fidelity: 0.9469 Trace: 0.9480\n",
      "Rep: 114 Cost: 0.1211 Fidelity: 0.9410 Trace: 0.9426\n",
      "Rep: 115 Cost: 0.1224 Fidelity: 0.9264 Trace: 0.9285\n",
      "Rep: 116 Cost: 0.0929 Fidelity: 0.9337 Trace: 0.9351\n",
      "Rep: 117 Cost: 0.1419 Fidelity: 0.9505 Trace: 0.9531\n",
      "Rep: 118 Cost: 0.0880 Fidelity: 0.9480 Trace: 0.9504\n",
      "Rep: 119 Cost: 0.1255 Fidelity: 0.9408 Trace: 0.9426\n",
      "Rep: 120 Cost: 0.0891 Fidelity: 0.9480 Trace: 0.9492\n",
      "Rep: 121 Cost: 0.1087 Fidelity: 0.9543 Trace: 0.9588\n",
      "Rep: 122 Cost: 0.1394 Fidelity: 0.9562 Trace: 0.9603\n",
      "Rep: 123 Cost: 0.0585 Fidelity: 0.9525 Trace: 0.9530\n",
      "Rep: 124 Cost: 0.1055 Fidelity: 0.9511 Trace: 0.9549\n",
      "Rep: 125 Cost: 0.1101 Fidelity: 0.9576 Trace: 0.9584\n",
      "Rep: 126 Cost: 0.1236 Fidelity: 0.9444 Trace: 0.9510\n",
      "Rep: 127 Cost: 0.2237 Fidelity: 0.9432 Trace: 0.9510\n",
      "Rep: 128 Cost: 0.1201 Fidelity: 0.9609 Trace: 0.9612\n",
      "Rep: 129 Cost: 0.2094 Fidelity: 0.9400 Trace: 0.9541\n",
      "Rep: 130 Cost: 0.2557 Fidelity: 0.9184 Trace: 0.9420\n",
      "Rep: 131 Cost: 0.1855 Fidelity: 0.9328 Trace: 0.9469\n",
      "Rep: 132 Cost: 0.0695 Fidelity: 0.9541 Trace: 0.9547\n",
      "Rep: 133 Cost: 0.2644 Fidelity: 0.9242 Trace: 0.9444\n",
      "Rep: 134 Cost: 0.2985 Fidelity: 0.9057 Trace: 0.9366\n",
      "Rep: 135 Cost: 0.2390 Fidelity: 0.9166 Trace: 0.9321\n",
      "Rep: 136 Cost: 0.1892 Fidelity: 0.9049 Trace: 0.9092\n",
      "Rep: 137 Cost: 0.2178 Fidelity: 0.8843 Trace: 0.8969\n",
      "Rep: 138 Cost: 0.2225 Fidelity: 0.9199 Trace: 0.9310\n",
      "Rep: 139 Cost: 0.0703 Fidelity: 0.9569 Trace: 0.9587\n",
      "Rep: 140 Cost: 0.2453 Fidelity: 0.9302 Trace: 0.9400\n",
      "Rep: 141 Cost: 0.2508 Fidelity: 0.8964 Trace: 0.9096\n",
      "Rep: 142 Cost: 0.1665 Fidelity: 0.8912 Trace: 0.8949\n",
      "Rep: 143 Cost: 0.3025 Fidelity: 0.8652 Trace: 0.8826\n",
      "Rep: 144 Cost: 0.2750 Fidelity: 0.8733 Trace: 0.9008\n",
      "Rep: 145 Cost: 0.2310 Fidelity: 0.9276 Trace: 0.9423\n",
      "Rep: 146 Cost: 0.1746 Fidelity: 0.9392 Trace: 0.9464\n",
      "Rep: 147 Cost: 0.2084 Fidelity: 0.9140 Trace: 0.9239\n",
      "Rep: 148 Cost: 0.2264 Fidelity: 0.9220 Trace: 0.9374\n",
      "Rep: 149 Cost: 0.2253 Fidelity: 0.9304 Trace: 0.9476\n",
      "Rep: 150 Cost: 0.1750 Fidelity: 0.9349 Trace: 0.9412\n",
      "Rep: 151 Cost: 0.1779 Fidelity: 0.9164 Trace: 0.9238\n",
      "Rep: 152 Cost: 0.1715 Fidelity: 0.9364 Trace: 0.9454\n",
      "Rep: 153 Cost: 0.1029 Fidelity: 0.9600 Trace: 0.9608\n",
      "Rep: 154 Cost: 0.1954 Fidelity: 0.9083 Trace: 0.9222\n",
      "Rep: 155 Cost: 0.2289 Fidelity: 0.8955 Trace: 0.9110\n",
      "Rep: 156 Cost: 0.0950 Fidelity: 0.9339 Trace: 0.9356\n",
      "Rep: 157 Cost: 0.2173 Fidelity: 0.9338 Trace: 0.9489\n",
      "Rep: 158 Cost: 0.1879 Fidelity: 0.9328 Trace: 0.9500\n",
      "Rep: 159 Cost: 0.1902 Fidelity: 0.9495 Trace: 0.9549\n",
      "Rep: 160 Cost: 0.2031 Fidelity: 0.9490 Trace: 0.9555\n",
      "Rep: 161 Cost: 0.1525 Fidelity: 0.9418 Trace: 0.9510\n",
      "Rep: 162 Cost: 0.1440 Fidelity: 0.9454 Trace: 0.9496\n",
      "Rep: 163 Cost: 0.0870 Fidelity: 0.9529 Trace: 0.9551\n",
      "Rep: 164 Cost: 0.1524 Fidelity: 0.9553 Trace: 0.9591\n",
      "Rep: 165 Cost: 0.1149 Fidelity: 0.9539 Trace: 0.9573\n",
      "Rep: 166 Cost: 0.1373 Fidelity: 0.9534 Trace: 0.9545\n",
      "Rep: 167 Cost: 0.1535 Fidelity: 0.9532 Trace: 0.9565\n",
      "Rep: 168 Cost: 0.1105 Fidelity: 0.9511 Trace: 0.9553\n",
      "Rep: 169 Cost: 0.0839 Fidelity: 0.9564 Trace: 0.9571\n",
      "Rep: 170 Cost: 0.1098 Fidelity: 0.9508 Trace: 0.9542\n",
      "Rep: 171 Cost: 0.0768 Fidelity: 0.9499 Trace: 0.9515\n",
      "Rep: 172 Cost: 0.1074 Fidelity: 0.9522 Trace: 0.9546\n",
      "Rep: 173 Cost: 0.0982 Fidelity: 0.9495 Trace: 0.9529\n",
      "Rep: 174 Cost: 0.0667 Fidelity: 0.9549 Trace: 0.9555\n",
      "Rep: 175 Cost: 0.0987 Fidelity: 0.9563 Trace: 0.9589\n",
      "Rep: 176 Cost: 0.0787 Fidelity: 0.9655 Trace: 0.9670\n",
      "Rep: 177 Cost: 0.0624 Fidelity: 0.9679 Trace: 0.9691\n",
      "Rep: 178 Cost: 0.0568 Fidelity: 0.9635 Trace: 0.9648\n",
      "Rep: 179 Cost: 0.0641 Fidelity: 0.9573 Trace: 0.9583\n",
      "Rep: 180 Cost: 0.0814 Fidelity: 0.9597 Trace: 0.9618\n",
      "Rep: 181 Cost: 0.0680 Fidelity: 0.9662 Trace: 0.9673\n",
      "Rep: 182 Cost: 0.0873 Fidelity: 0.9665 Trace: 0.9680\n",
      "Rep: 183 Cost: 0.0780 Fidelity: 0.9612 Trace: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 184 Cost: 0.0791 Fidelity: 0.9627 Trace: 0.9642\n",
      "Rep: 185 Cost: 0.0740 Fidelity: 0.9671 Trace: 0.9678\n",
      "Rep: 186 Cost: 0.0780 Fidelity: 0.9658 Trace: 0.9681\n",
      "Rep: 187 Cost: 0.0784 Fidelity: 0.9668 Trace: 0.9692\n",
      "Rep: 188 Cost: 0.0472 Fidelity: 0.9703 Trace: 0.9708\n",
      "Rep: 189 Cost: 0.0707 Fidelity: 0.9728 Trace: 0.9737\n",
      "Rep: 190 Cost: 0.0757 Fidelity: 0.9704 Trace: 0.9715\n",
      "Rep: 191 Cost: 0.0637 Fidelity: 0.9703 Trace: 0.9713\n",
      "Rep: 192 Cost: 0.0458 Fidelity: 0.9734 Trace: 0.9736\n",
      "Rep: 193 Cost: 0.0611 Fidelity: 0.9736 Trace: 0.9746\n",
      "Rep: 194 Cost: 0.0466 Fidelity: 0.9758 Trace: 0.9761\n",
      "Rep: 195 Cost: 0.1361 Fidelity: 0.9694 Trace: 0.9750\n",
      "Rep: 196 Cost: 0.0919 Fidelity: 0.9702 Trace: 0.9748\n",
      "Rep: 197 Cost: 0.0788 Fidelity: 0.9734 Trace: 0.9738\n",
      "Rep: 198 Cost: 0.0517 Fidelity: 0.9767 Trace: 0.9773\n",
      "Rep: 199 Cost: 0.0355 Fidelity: 0.9770 Trace: 0.9773\n",
      "Rep: 200 Cost: 0.0319 Fidelity: 0.9761 Trace: 0.9763\n",
      "Rep: 201 Cost: 0.0778 Fidelity: 0.9769 Trace: 0.9781\n",
      "Rep: 202 Cost: 0.0382 Fidelity: 0.9777 Trace: 0.9779\n",
      "Rep: 203 Cost: 0.1158 Fidelity: 0.9674 Trace: 0.9733\n",
      "Rep: 204 Cost: 0.1226 Fidelity: 0.9716 Trace: 0.9770\n",
      "Rep: 205 Cost: 0.0497 Fidelity: 0.9773 Trace: 0.9780\n",
      "Rep: 206 Cost: 0.1667 Fidelity: 0.9625 Trace: 0.9734\n",
      "Rep: 207 Cost: 0.1722 Fidelity: 0.9540 Trace: 0.9750\n",
      "Rep: 208 Cost: 0.1355 Fidelity: 0.9680 Trace: 0.9773\n",
      "Rep: 209 Cost: 0.0929 Fidelity: 0.9734 Trace: 0.9744\n",
      "Rep: 210 Cost: 0.0888 Fidelity: 0.9714 Trace: 0.9754\n",
      "Rep: 211 Cost: 0.1421 Fidelity: 0.9781 Trace: 0.9793\n",
      "Rep: 212 Cost: 0.1447 Fidelity: 0.9779 Trace: 0.9801\n",
      "Rep: 213 Cost: 0.0871 Fidelity: 0.9732 Trace: 0.9761\n",
      "Rep: 214 Cost: 0.0575 Fidelity: 0.9754 Trace: 0.9755\n",
      "Rep: 215 Cost: 0.1512 Fidelity: 0.9604 Trace: 0.9699\n",
      "Rep: 216 Cost: 0.1515 Fidelity: 0.9507 Trace: 0.9628\n",
      "Rep: 217 Cost: 0.1318 Fidelity: 0.9692 Trace: 0.9722\n",
      "Rep: 218 Cost: 0.1182 Fidelity: 0.9704 Trace: 0.9733\n",
      "Rep: 219 Cost: 0.1445 Fidelity: 0.9616 Trace: 0.9682\n",
      "Rep: 220 Cost: 0.1252 Fidelity: 0.9665 Trace: 0.9693\n",
      "Rep: 221 Cost: 0.0897 Fidelity: 0.9716 Trace: 0.9733\n",
      "Rep: 222 Cost: 0.0784 Fidelity: 0.9717 Trace: 0.9741\n",
      "Rep: 223 Cost: 0.1045 Fidelity: 0.9643 Trace: 0.9648\n",
      "Rep: 224 Cost: 0.1360 Fidelity: 0.9616 Trace: 0.9644\n",
      "Rep: 225 Cost: 0.0990 Fidelity: 0.9661 Trace: 0.9673\n",
      "Rep: 226 Cost: 0.1277 Fidelity: 0.9621 Trace: 0.9649\n",
      "Rep: 227 Cost: 0.0807 Fidelity: 0.9670 Trace: 0.9698\n",
      "Rep: 228 Cost: 0.0454 Fidelity: 0.9699 Trace: 0.9701\n",
      "Rep: 229 Cost: 0.0782 Fidelity: 0.9653 Trace: 0.9657\n",
      "Rep: 230 Cost: 0.0626 Fidelity: 0.9724 Trace: 0.9734\n",
      "Rep: 231 Cost: 0.0985 Fidelity: 0.9704 Trace: 0.9707\n",
      "Rep: 232 Cost: 0.1593 Fidelity: 0.9544 Trace: 0.9597\n",
      "Rep: 233 Cost: 0.1324 Fidelity: 0.9552 Trace: 0.9617\n",
      "Rep: 234 Cost: 0.0530 Fidelity: 0.9724 Trace: 0.9726\n",
      "Rep: 235 Cost: 0.1892 Fidelity: 0.9619 Trace: 0.9753\n",
      "Rep: 236 Cost: 0.2257 Fidelity: 0.9482 Trace: 0.9711\n",
      "Rep: 237 Cost: 0.1916 Fidelity: 0.9574 Trace: 0.9686\n",
      "Rep: 238 Cost: 0.0858 Fidelity: 0.9654 Trace: 0.9660\n",
      "Rep: 239 Cost: 0.1399 Fidelity: 0.9557 Trace: 0.9640\n",
      "Rep: 240 Cost: 0.1832 Fidelity: 0.9498 Trace: 0.9632\n",
      "Rep: 241 Cost: 0.1256 Fidelity: 0.9592 Trace: 0.9654\n",
      "Rep: 242 Cost: 0.0597 Fidelity: 0.9673 Trace: 0.9679\n",
      "Rep: 243 Cost: 0.0996 Fidelity: 0.9697 Trace: 0.9719\n",
      "Rep: 244 Cost: 0.0502 Fidelity: 0.9665 Trace: 0.9669\n",
      "Rep: 245 Cost: 0.1587 Fidelity: 0.9546 Trace: 0.9615\n",
      "Rep: 246 Cost: 0.1635 Fidelity: 0.9569 Trace: 0.9676\n",
      "Rep: 247 Cost: 0.1159 Fidelity: 0.9701 Trace: 0.9754\n",
      "Rep: 248 Cost: 0.0612 Fidelity: 0.9702 Trace: 0.9710\n",
      "Rep: 249 Cost: 0.1203 Fidelity: 0.9656 Trace: 0.9687\n",
      "Rep: 250 Cost: 0.0511 Fidelity: 0.9726 Trace: 0.9731\n",
      "Rep: 251 Cost: 0.1587 Fidelity: 0.9675 Trace: 0.9725\n",
      "Rep: 252 Cost: 0.1860 Fidelity: 0.9608 Trace: 0.9693\n",
      "Rep: 253 Cost: 0.0829 Fidelity: 0.9667 Trace: 0.9697\n",
      "Rep: 254 Cost: 0.1055 Fidelity: 0.9695 Trace: 0.9705\n",
      "Rep: 255 Cost: 0.1053 Fidelity: 0.9637 Trace: 0.9660\n",
      "Rep: 256 Cost: 0.1221 Fidelity: 0.9602 Trace: 0.9620\n",
      "Rep: 257 Cost: 0.1469 Fidelity: 0.9601 Trace: 0.9640\n",
      "Rep: 258 Cost: 0.1119 Fidelity: 0.9580 Trace: 0.9640\n",
      "Rep: 259 Cost: 0.0806 Fidelity: 0.9609 Trace: 0.9622\n",
      "Rep: 260 Cost: 0.1364 Fidelity: 0.9504 Trace: 0.9570\n",
      "Rep: 261 Cost: 0.1208 Fidelity: 0.9593 Trace: 0.9675\n",
      "Rep: 262 Cost: 0.1175 Fidelity: 0.9656 Trace: 0.9679\n",
      "Rep: 263 Cost: 0.1407 Fidelity: 0.9443 Trace: 0.9503\n",
      "Rep: 264 Cost: 0.1523 Fidelity: 0.9552 Trace: 0.9591\n",
      "Rep: 265 Cost: 0.1169 Fidelity: 0.9707 Trace: 0.9727\n",
      "Rep: 266 Cost: 0.0996 Fidelity: 0.9661 Trace: 0.9686\n",
      "Rep: 267 Cost: 0.0865 Fidelity: 0.9645 Trace: 0.9654\n",
      "Rep: 268 Cost: 0.1017 Fidelity: 0.9580 Trace: 0.9601\n",
      "Rep: 269 Cost: 0.0688 Fidelity: 0.9652 Trace: 0.9658\n",
      "Rep: 270 Cost: 0.1148 Fidelity: 0.9637 Trace: 0.9670\n",
      "Rep: 271 Cost: 0.1098 Fidelity: 0.9598 Trace: 0.9630\n",
      "Rep: 272 Cost: 0.0764 Fidelity: 0.9671 Trace: 0.9675\n",
      "Rep: 273 Cost: 0.1149 Fidelity: 0.9626 Trace: 0.9660\n",
      "Rep: 274 Cost: 0.1037 Fidelity: 0.9617 Trace: 0.9645\n",
      "Rep: 275 Cost: 0.0613 Fidelity: 0.9588 Trace: 0.9593\n",
      "Rep: 276 Cost: 0.0959 Fidelity: 0.9549 Trace: 0.9562\n",
      "Rep: 277 Cost: 0.0624 Fidelity: 0.9627 Trace: 0.9633\n",
      "Rep: 278 Cost: 0.0923 Fidelity: 0.9682 Trace: 0.9692\n",
      "Rep: 279 Cost: 0.0800 Fidelity: 0.9692 Trace: 0.9697\n",
      "Rep: 280 Cost: 0.0918 Fidelity: 0.9677 Trace: 0.9694\n",
      "Rep: 281 Cost: 0.0683 Fidelity: 0.9717 Trace: 0.9722\n",
      "Rep: 282 Cost: 0.1000 Fidelity: 0.9669 Trace: 0.9699\n",
      "Rep: 283 Cost: 0.1015 Fidelity: 0.9650 Trace: 0.9683\n",
      "Rep: 284 Cost: 0.0562 Fidelity: 0.9714 Trace: 0.9716\n",
      "Rep: 285 Cost: 0.0900 Fidelity: 0.9621 Trace: 0.9653\n",
      "Rep: 286 Cost: 0.0972 Fidelity: 0.9632 Trace: 0.9652\n",
      "Rep: 287 Cost: 0.0694 Fidelity: 0.9688 Trace: 0.9711\n",
      "Rep: 288 Cost: 0.0978 Fidelity: 0.9671 Trace: 0.9708\n",
      "Rep: 289 Cost: 0.0600 Fidelity: 0.9710 Trace: 0.9718\n",
      "Rep: 290 Cost: 0.0683 Fidelity: 0.9703 Trace: 0.9723\n",
      "Rep: 291 Cost: 0.0819 Fidelity: 0.9729 Trace: 0.9743\n",
      "Rep: 292 Cost: 0.0462 Fidelity: 0.9747 Trace: 0.9753\n",
      "Rep: 293 Cost: 0.0634 Fidelity: 0.9675 Trace: 0.9688\n",
      "Rep: 294 Cost: 0.0595 Fidelity: 0.9683 Trace: 0.9688\n",
      "Rep: 295 Cost: 0.0606 Fidelity: 0.9726 Trace: 0.9744\n",
      "Rep: 296 Cost: 0.0682 Fidelity: 0.9712 Trace: 0.9728\n",
      "Rep: 297 Cost: 0.0719 Fidelity: 0.9704 Trace: 0.9717\n",
      "Rep: 298 Cost: 0.0727 Fidelity: 0.9719 Trace: 0.9735\n",
      "Rep: 299 Cost: 0.0388 Fidelity: 0.9762 Trace: 0.9763\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2244e-04\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1874e-04\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.6351e-05\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5825e-05\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.1348e-05\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.9805e-05\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8854e-05\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2046e-05\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2918e-05\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.2085e-05\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.7453e-05\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.1572e-05\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8028e-05\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1523e-05\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2742e-05\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2050e-05\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2160e-05\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3394e-05\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.2845e-06\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0369e-06\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1230e-05\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1744e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7951e-05\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1688e-05\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0523e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.1330e-06\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1448e-06\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.5888e-06\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0213e-06\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9288e-07\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1654e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8922e-06\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8953e-06\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.0516e-06\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.3224e-06\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2590e-06\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0854e-06\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.4881e-06\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4131e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7357e-06\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6455e-07\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1644e-07\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1482e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9028e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4572e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2129e-06\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0601e-06\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.2391e-07\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0831e-06\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0636e-06\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.2135e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3961e-07\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3075e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8814e-07\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9322e-07\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5517e-07\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0694e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5982e-07\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8169e-07\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4516e-07\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8360e-07\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9460e-07\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2325e-08\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5204e-08\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4655e-07\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4763e-07\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2421e-07\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1797e-07\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.3369e-08\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0378e-07\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3913e-07\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9763e-08\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7047e-08\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6219e-08\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8278e-08\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5486e-08\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5646e-08\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5177e-08\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2949e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5941e-08\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8241e-08\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7471e-08\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2056e-08\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0911e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2904e-09\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2256e-08\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3373e-08\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2236e-08\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1912e-08\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2495e-09\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6367e-09\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5534e-08\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2113e-08\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9448e-09\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4321e-09\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8426e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3423e-09\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5845e-09\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3016e-09\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0953e-09\n",
      "[[-0.35060096 -0.5034406 ]]\n",
      "[1. 0. 0.]\n",
      "Rep: 0 Cost: 1.2339 Fidelity: 0.0044 Trace: 0.0514\n",
      "Rep: 1 Cost: 1.2697 Fidelity: 0.0049 Trace: 0.0382\n",
      "Rep: 2 Cost: 1.1835 Fidelity: 0.0056 Trace: 0.0392\n",
      "Rep: 3 Cost: 1.0822 Fidelity: 0.0140 Trace: 0.0319\n",
      "Rep: 4 Cost: 1.0044 Fidelity: 0.0192 Trace: 0.0295\n",
      "Rep: 5 Cost: 0.9641 Fidelity: 0.0207 Trace: 0.0240\n",
      "Rep: 6 Cost: 0.9492 Fidelity: 0.0161 Trace: 0.0190\n",
      "Rep: 7 Cost: 0.9555 Fidelity: 0.0184 Trace: 0.0207\n",
      "Rep: 8 Cost: 0.9137 Fidelity: 0.0257 Trace: 0.0278\n",
      "Rep: 9 Cost: 0.8876 Fidelity: 0.0394 Trace: 0.0420\n",
      "Rep: 10 Cost: 0.8631 Fidelity: 0.0499 Trace: 0.0517\n",
      "Rep: 11 Cost: 0.8223 Fidelity: 0.0527 Trace: 0.0541\n",
      "Rep: 12 Cost: 0.8074 Fidelity: 0.0578 Trace: 0.0583\n",
      "Rep: 13 Cost: 0.7940 Fidelity: 0.0631 Trace: 0.0643\n",
      "Rep: 14 Cost: 0.7905 Fidelity: 0.0710 Trace: 0.0722\n",
      "Rep: 15 Cost: 0.7734 Fidelity: 0.0797 Trace: 0.0808\n",
      "Rep: 16 Cost: 0.7493 Fidelity: 0.0888 Trace: 0.0898\n",
      "Rep: 17 Cost: 0.7387 Fidelity: 0.0977 Trace: 0.0988\n",
      "Rep: 18 Cost: 0.6974 Fidelity: 0.1181 Trace: 0.1190\n",
      "Rep: 19 Cost: 0.6668 Fidelity: 0.1416 Trace: 0.1422\n",
      "Rep: 20 Cost: 0.6454 Fidelity: 0.1646 Trace: 0.1663\n",
      "Rep: 21 Cost: 0.6354 Fidelity: 0.1863 Trace: 0.1882\n",
      "Rep: 22 Cost: 0.6346 Fidelity: 0.2200 Trace: 0.2244\n",
      "Rep: 23 Cost: 0.5649 Fidelity: 0.2545 Trace: 0.2565\n",
      "Rep: 24 Cost: 0.6033 Fidelity: 0.2654 Trace: 0.2725\n",
      "Rep: 25 Cost: 0.5374 Fidelity: 0.3024 Trace: 0.3066\n",
      "Rep: 26 Cost: 0.5555 Fidelity: 0.3149 Trace: 0.3207\n",
      "Rep: 27 Cost: 0.5275 Fidelity: 0.3248 Trace: 0.3296\n",
      "Rep: 28 Cost: 0.4928 Fidelity: 0.3604 Trace: 0.3667\n",
      "Rep: 29 Cost: 0.4923 Fidelity: 0.3795 Trace: 0.3897\n",
      "Rep: 30 Cost: 0.4471 Fidelity: 0.3924 Trace: 0.3955\n",
      "Rep: 31 Cost: 0.4899 Fidelity: 0.4064 Trace: 0.4124\n",
      "Rep: 32 Cost: 0.3759 Fidelity: 0.4240 Trace: 0.4244\n",
      "Rep: 33 Cost: 0.4585 Fidelity: 0.4270 Trace: 0.4355\n",
      "Rep: 34 Cost: 0.3859 Fidelity: 0.4773 Trace: 0.4779\n",
      "Rep: 35 Cost: 0.3650 Fidelity: 0.4688 Trace: 0.4701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 36 Cost: 0.4128 Fidelity: 0.4801 Trace: 0.4857\n",
      "Rep: 37 Cost: 0.3777 Fidelity: 0.5265 Trace: 0.5276\n",
      "Rep: 38 Cost: 0.3573 Fidelity: 0.5120 Trace: 0.5143\n",
      "Rep: 39 Cost: 0.3779 Fidelity: 0.5129 Trace: 0.5175\n",
      "Rep: 40 Cost: 0.3569 Fidelity: 0.5613 Trace: 0.5628\n",
      "Rep: 41 Cost: 0.3663 Fidelity: 0.5366 Trace: 0.5447\n",
      "Rep: 42 Cost: 0.3294 Fidelity: 0.5444 Trace: 0.5458\n",
      "Rep: 43 Cost: 0.4487 Fidelity: 0.5564 Trace: 0.5737\n",
      "Rep: 44 Cost: 0.3759 Fidelity: 0.5694 Trace: 0.5811\n",
      "Rep: 45 Cost: 0.3652 Fidelity: 0.5662 Trace: 0.5725\n",
      "Rep: 46 Cost: 0.3665 Fidelity: 0.5826 Trace: 0.5917\n",
      "Rep: 47 Cost: 0.3108 Fidelity: 0.6075 Trace: 0.6121\n",
      "Rep: 48 Cost: 0.3235 Fidelity: 0.6041 Trace: 0.6113\n",
      "Rep: 49 Cost: 0.2725 Fidelity: 0.6264 Trace: 0.6283\n",
      "Rep: 50 Cost: 0.3167 Fidelity: 0.6141 Trace: 0.6225\n",
      "Rep: 51 Cost: 0.2571 Fidelity: 0.6319 Trace: 0.6334\n",
      "Rep: 52 Cost: 0.3980 Fidelity: 0.6196 Trace: 0.6447\n",
      "Rep: 53 Cost: 0.3780 Fidelity: 0.6198 Trace: 0.6442\n",
      "Rep: 54 Cost: 0.2344 Fidelity: 0.6526 Trace: 0.6536\n",
      "Rep: 55 Cost: 0.3401 Fidelity: 0.6373 Trace: 0.6524\n",
      "Rep: 56 Cost: 0.2877 Fidelity: 0.6540 Trace: 0.6606\n",
      "Rep: 57 Cost: 0.3206 Fidelity: 0.6555 Trace: 0.6678\n",
      "Rep: 58 Cost: 0.3145 Fidelity: 0.6588 Trace: 0.6726\n",
      "Rep: 59 Cost: 0.2347 Fidelity: 0.6764 Trace: 0.6786\n",
      "Rep: 60 Cost: 0.3008 Fidelity: 0.6669 Trace: 0.6775\n",
      "Rep: 61 Cost: 0.2507 Fidelity: 0.6947 Trace: 0.6982\n",
      "Rep: 62 Cost: 0.3048 Fidelity: 0.6749 Trace: 0.6872\n",
      "Rep: 63 Cost: 0.2757 Fidelity: 0.6831 Trace: 0.6912\n",
      "Rep: 64 Cost: 0.2731 Fidelity: 0.6981 Trace: 0.7032\n",
      "Rep: 65 Cost: 0.2508 Fidelity: 0.6962 Trace: 0.7002\n",
      "Rep: 66 Cost: 0.2836 Fidelity: 0.6940 Trace: 0.7049\n",
      "Rep: 67 Cost: 0.2741 Fidelity: 0.6954 Trace: 0.7039\n",
      "Rep: 68 Cost: 0.2361 Fidelity: 0.7102 Trace: 0.7141\n",
      "Rep: 69 Cost: 0.2227 Fidelity: 0.7145 Trace: 0.7174\n",
      "Rep: 70 Cost: 0.2801 Fidelity: 0.7037 Trace: 0.7124\n",
      "Rep: 71 Cost: 0.2598 Fidelity: 0.7117 Trace: 0.7182\n",
      "Rep: 72 Cost: 0.2395 Fidelity: 0.7134 Trace: 0.7199\n",
      "Rep: 73 Cost: 0.2769 Fidelity: 0.7128 Trace: 0.7231\n",
      "Rep: 74 Cost: 0.1778 Fidelity: 0.7291 Trace: 0.7296\n",
      "Rep: 75 Cost: 0.2146 Fidelity: 0.7272 Trace: 0.7307\n",
      "Rep: 76 Cost: 0.2192 Fidelity: 0.7379 Trace: 0.7404\n",
      "Rep: 77 Cost: 0.2186 Fidelity: 0.7281 Trace: 0.7293\n",
      "Rep: 78 Cost: 0.2675 Fidelity: 0.7351 Trace: 0.7413\n",
      "Rep: 79 Cost: 0.1659 Fidelity: 0.7365 Trace: 0.7366\n",
      "Rep: 80 Cost: 0.3493 Fidelity: 0.6957 Trace: 0.7270\n",
      "Rep: 81 Cost: 0.3664 Fidelity: 0.7090 Trace: 0.7395\n",
      "Rep: 82 Cost: 0.1786 Fidelity: 0.7445 Trace: 0.7455\n",
      "Rep: 83 Cost: 0.4306 Fidelity: 0.6580 Trace: 0.7064\n",
      "Rep: 84 Cost: 0.4866 Fidelity: 0.6226 Trace: 0.6906\n",
      "Rep: 85 Cost: 0.3060 Fidelity: 0.7122 Trace: 0.7302\n",
      "Rep: 86 Cost: 0.3103 Fidelity: 0.7279 Trace: 0.7444\n",
      "Rep: 87 Cost: 0.4040 Fidelity: 0.6859 Trace: 0.7259\n",
      "Rep: 88 Cost: 0.2554 Fidelity: 0.7314 Trace: 0.7414\n",
      "Rep: 89 Cost: 0.3279 Fidelity: 0.7257 Trace: 0.7440\n",
      "Rep: 90 Cost: 0.4052 Fidelity: 0.6908 Trace: 0.7242\n",
      "Rep: 91 Cost: 0.2720 Fidelity: 0.7348 Trace: 0.7441\n",
      "Rep: 92 Cost: 0.3014 Fidelity: 0.7286 Trace: 0.7428\n",
      "Rep: 93 Cost: 0.3690 Fidelity: 0.6894 Trace: 0.7202\n",
      "Rep: 94 Cost: 0.2401 Fidelity: 0.7343 Trace: 0.7412\n",
      "Rep: 95 Cost: 0.2993 Fidelity: 0.7333 Trace: 0.7507\n",
      "Rep: 96 Cost: 0.3813 Fidelity: 0.6941 Trace: 0.7290\n",
      "Rep: 97 Cost: 0.2952 Fidelity: 0.7280 Trace: 0.7467\n",
      "Rep: 98 Cost: 0.2048 Fidelity: 0.7651 Trace: 0.7657\n",
      "Rep: 99 Cost: 0.2429 Fidelity: 0.7370 Trace: 0.7455\n",
      "Rep: 100 Cost: 0.2025 Fidelity: 0.7459 Trace: 0.7487\n",
      "Rep: 101 Cost: 0.2685 Fidelity: 0.7545 Trace: 0.7630\n",
      "Rep: 102 Cost: 0.2581 Fidelity: 0.7415 Trace: 0.7528\n",
      "Rep: 103 Cost: 0.1535 Fidelity: 0.7572 Trace: 0.7574\n",
      "Rep: 104 Cost: 0.3194 Fidelity: 0.7271 Trace: 0.7461\n",
      "Rep: 105 Cost: 0.3282 Fidelity: 0.7085 Trace: 0.7350\n",
      "Rep: 106 Cost: 0.2241 Fidelity: 0.7476 Trace: 0.7537\n",
      "Rep: 107 Cost: 0.2783 Fidelity: 0.7526 Trace: 0.7634\n",
      "Rep: 108 Cost: 0.3226 Fidelity: 0.7199 Trace: 0.7436\n",
      "Rep: 109 Cost: 0.2500 Fidelity: 0.7448 Trace: 0.7548\n",
      "Rep: 110 Cost: 0.2127 Fidelity: 0.7703 Trace: 0.7727\n",
      "Rep: 111 Cost: 0.2475 Fidelity: 0.7475 Trace: 0.7580\n",
      "Rep: 112 Cost: 0.1783 Fidelity: 0.7641 Trace: 0.7663\n",
      "Rep: 113 Cost: 0.2804 Fidelity: 0.7579 Trace: 0.7695\n",
      "Rep: 114 Cost: 0.2910 Fidelity: 0.7361 Trace: 0.7529\n",
      "Rep: 115 Cost: 0.2024 Fidelity: 0.7630 Trace: 0.7669\n",
      "Rep: 116 Cost: 0.2732 Fidelity: 0.7687 Trace: 0.7764\n",
      "Rep: 117 Cost: 0.2653 Fidelity: 0.7472 Trace: 0.7623\n",
      "Rep: 118 Cost: 0.2251 Fidelity: 0.7624 Trace: 0.7653\n",
      "Rep: 119 Cost: 0.2615 Fidelity: 0.7658 Trace: 0.7768\n",
      "Rep: 120 Cost: 0.2865 Fidelity: 0.7440 Trace: 0.7635\n",
      "Rep: 121 Cost: 0.2396 Fidelity: 0.7600 Trace: 0.7659\n",
      "Rep: 122 Cost: 0.2180 Fidelity: 0.7734 Trace: 0.7790\n",
      "Rep: 123 Cost: 0.2449 Fidelity: 0.7603 Trace: 0.7721\n",
      "Rep: 124 Cost: 0.1978 Fidelity: 0.7724 Trace: 0.7745\n",
      "Rep: 125 Cost: 0.2615 Fidelity: 0.7719 Trace: 0.7819\n",
      "Rep: 126 Cost: 0.2742 Fidelity: 0.7542 Trace: 0.7704\n",
      "Rep: 127 Cost: 0.1965 Fidelity: 0.7721 Trace: 0.7750\n",
      "Rep: 128 Cost: 0.2685 Fidelity: 0.7646 Trace: 0.7753\n",
      "Rep: 129 Cost: 0.3006 Fidelity: 0.7378 Trace: 0.7580\n",
      "Rep: 130 Cost: 0.2377 Fidelity: 0.7636 Trace: 0.7701\n",
      "Rep: 131 Cost: 0.2203 Fidelity: 0.7855 Trace: 0.7903\n",
      "Rep: 132 Cost: 0.2390 Fidelity: 0.7613 Trace: 0.7726\n",
      "Rep: 133 Cost: 0.2024 Fidelity: 0.7716 Trace: 0.7746\n",
      "Rep: 134 Cost: 0.2209 Fidelity: 0.7875 Trace: 0.7936\n",
      "Rep: 135 Cost: 0.2301 Fidelity: 0.7766 Trace: 0.7870\n",
      "Rep: 136 Cost: 0.1693 Fidelity: 0.7865 Trace: 0.7881\n",
      "Rep: 137 Cost: 0.2626 Fidelity: 0.7808 Trace: 0.7913\n",
      "Rep: 138 Cost: 0.2765 Fidelity: 0.7600 Trace: 0.7763\n",
      "Rep: 139 Cost: 0.1983 Fidelity: 0.7819 Trace: 0.7866\n",
      "Rep: 140 Cost: 0.2052 Fidelity: 0.7974 Trace: 0.8010\n",
      "Rep: 141 Cost: 0.2362 Fidelity: 0.7801 Trace: 0.7901\n",
      "Rep: 142 Cost: 0.1793 Fidelity: 0.7921 Trace: 0.7946\n",
      "Rep: 143 Cost: 0.2274 Fidelity: 0.7956 Trace: 0.8013\n",
      "Rep: 144 Cost: 0.2352 Fidelity: 0.7805 Trace: 0.7897\n",
      "Rep: 145 Cost: 0.1776 Fidelity: 0.7925 Trace: 0.7954\n",
      "Rep: 146 Cost: 0.2126 Fidelity: 0.7902 Trace: 0.7932\n",
      "Rep: 147 Cost: 0.2161 Fidelity: 0.7801 Trace: 0.7855\n",
      "Rep: 148 Cost: 0.1342 Fidelity: 0.7991 Trace: 0.7995\n",
      "Rep: 149 Cost: 0.2473 Fidelity: 0.7831 Trace: 0.7920\n",
      "Rep: 150 Cost: 0.2819 Fidelity: 0.7604 Trace: 0.7733\n",
      "Rep: 151 Cost: 0.1982 Fidelity: 0.7869 Trace: 0.7910\n",
      "Rep: 152 Cost: 0.1892 Fidelity: 0.8072 Trace: 0.8100\n",
      "Rep: 153 Cost: 0.2342 Fidelity: 0.7840 Trace: 0.7930\n",
      "Rep: 154 Cost: 0.1918 Fidelity: 0.7935 Trace: 0.7979\n",
      "Rep: 155 Cost: 0.1464 Fidelity: 0.8070 Trace: 0.8077\n",
      "Rep: 156 Cost: 0.1936 Fidelity: 0.7964 Trace: 0.7988\n",
      "Rep: 157 Cost: 0.1369 Fidelity: 0.8085 Trace: 0.8091\n",
      "Rep: 158 Cost: 0.1869 Fidelity: 0.8040 Trace: 0.8091\n",
      "Rep: 159 Cost: 0.2127 Fidelity: 0.7934 Trace: 0.8014\n",
      "Rep: 160 Cost: 0.1510 Fidelity: 0.8125 Trace: 0.8137\n",
      "Rep: 161 Cost: 0.1815 Fidelity: 0.8021 Trace: 0.8069\n",
      "Rep: 162 Cost: 0.2125 Fidelity: 0.7961 Trace: 0.8055\n",
      "Rep: 163 Cost: 0.1823 Fidelity: 0.8120 Trace: 0.8165\n",
      "Rep: 164 Cost: 0.1551 Fidelity: 0.8087 Trace: 0.8098\n",
      "Rep: 165 Cost: 0.1802 Fidelity: 0.8140 Trace: 0.8175\n",
      "Rep: 166 Cost: 0.1650 Fidelity: 0.8069 Trace: 0.8095\n",
      "Rep: 167 Cost: 0.1357 Fidelity: 0.8103 Trace: 0.8110\n",
      "Rep: 168 Cost: 0.1354 Fidelity: 0.8183 Trace: 0.8193\n",
      "Rep: 169 Cost: 0.1583 Fidelity: 0.8122 Trace: 0.8133\n",
      "Rep: 170 Cost: 0.1548 Fidelity: 0.8194 Trace: 0.8205\n",
      "Rep: 171 Cost: 0.1819 Fidelity: 0.8120 Trace: 0.8158\n",
      "Rep: 172 Cost: 0.1597 Fidelity: 0.8194 Trace: 0.8226\n",
      "Rep: 173 Cost: 0.1426 Fidelity: 0.8173 Trace: 0.8183\n",
      "Rep: 174 Cost: 0.1891 Fidelity: 0.8044 Trace: 0.8073\n",
      "Rep: 175 Cost: 0.1813 Fidelity: 0.8167 Trace: 0.8190\n",
      "Rep: 176 Cost: 0.1119 Fidelity: 0.8226 Trace: 0.8228\n",
      "Rep: 177 Cost: 0.1567 Fidelity: 0.8238 Trace: 0.8263\n",
      "Rep: 178 Cost: 0.1437 Fidelity: 0.8311 Trace: 0.8317\n",
      "Rep: 179 Cost: 0.2237 Fidelity: 0.8098 Trace: 0.8175\n",
      "Rep: 180 Cost: 0.2124 Fidelity: 0.8181 Trace: 0.8268\n",
      "Rep: 181 Cost: 0.1254 Fidelity: 0.8260 Trace: 0.8266\n",
      "Rep: 182 Cost: 0.2274 Fidelity: 0.8080 Trace: 0.8206\n",
      "Rep: 183 Cost: 0.2786 Fidelity: 0.8043 Trace: 0.8273\n",
      "Rep: 184 Cost: 0.2087 Fidelity: 0.8180 Trace: 0.8275\n",
      "Rep: 185 Cost: 0.1308 Fidelity: 0.8306 Trace: 0.8318\n",
      "Rep: 186 Cost: 0.1849 Fidelity: 0.8274 Trace: 0.8326\n",
      "Rep: 187 Cost: 0.1701 Fidelity: 0.8226 Trace: 0.8251\n",
      "Rep: 188 Cost: 0.1647 Fidelity: 0.8318 Trace: 0.8346\n",
      "Rep: 189 Cost: 0.1670 Fidelity: 0.8273 Trace: 0.8301\n",
      "Rep: 190 Cost: 0.1481 Fidelity: 0.8318 Trace: 0.8334\n",
      "Rep: 191 Cost: 0.1426 Fidelity: 0.8313 Trace: 0.8328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 192 Cost: 0.1368 Fidelity: 0.8291 Trace: 0.8307\n",
      "Rep: 193 Cost: 0.1293 Fidelity: 0.8277 Trace: 0.8285\n",
      "Rep: 194 Cost: 0.1668 Fidelity: 0.8248 Trace: 0.8283\n",
      "Rep: 195 Cost: 0.1580 Fidelity: 0.8368 Trace: 0.8388\n",
      "Rep: 196 Cost: 0.1604 Fidelity: 0.8292 Trace: 0.8317\n",
      "Rep: 197 Cost: 0.1592 Fidelity: 0.8354 Trace: 0.8388\n",
      "Rep: 198 Cost: 0.1289 Fidelity: 0.8348 Trace: 0.8358\n",
      "Rep: 199 Cost: 0.1299 Fidelity: 0.8378 Trace: 0.8390\n",
      "Rep: 200 Cost: 0.1485 Fidelity: 0.8316 Trace: 0.8335\n",
      "Rep: 201 Cost: 0.1363 Fidelity: 0.8349 Trace: 0.8359\n",
      "Rep: 202 Cost: 0.1724 Fidelity: 0.8281 Trace: 0.8326\n",
      "Rep: 203 Cost: 0.1817 Fidelity: 0.8385 Trace: 0.8428\n",
      "Rep: 204 Cost: 0.1196 Fidelity: 0.8383 Trace: 0.8387\n",
      "Rep: 205 Cost: 0.1141 Fidelity: 0.8426 Trace: 0.8431\n",
      "Rep: 206 Cost: 0.1425 Fidelity: 0.8387 Trace: 0.8415\n",
      "Rep: 207 Cost: 0.1303 Fidelity: 0.8357 Trace: 0.8370\n",
      "Rep: 208 Cost: 0.1549 Fidelity: 0.8305 Trace: 0.8332\n",
      "Rep: 209 Cost: 0.1377 Fidelity: 0.8407 Trace: 0.8429\n",
      "Rep: 210 Cost: 0.1624 Fidelity: 0.8350 Trace: 0.8374\n",
      "Rep: 211 Cost: 0.1878 Fidelity: 0.8446 Trace: 0.8471\n",
      "Rep: 212 Cost: 0.1540 Fidelity: 0.8402 Trace: 0.8418\n",
      "Rep: 213 Cost: 0.1290 Fidelity: 0.8450 Trace: 0.8464\n",
      "Rep: 214 Cost: 0.1239 Fidelity: 0.8407 Trace: 0.8418\n",
      "Rep: 215 Cost: 0.1333 Fidelity: 0.8369 Trace: 0.8381\n",
      "Rep: 216 Cost: 0.1315 Fidelity: 0.8426 Trace: 0.8438\n",
      "Rep: 217 Cost: 0.1526 Fidelity: 0.8395 Trace: 0.8409\n",
      "Rep: 218 Cost: 0.1475 Fidelity: 0.8535 Trace: 0.8537\n",
      "Rep: 219 Cost: 0.1698 Fidelity: 0.8391 Trace: 0.8454\n",
      "Rep: 220 Cost: 0.1960 Fidelity: 0.8334 Trace: 0.8401\n",
      "Rep: 221 Cost: 0.1455 Fidelity: 0.8531 Trace: 0.8534\n",
      "Rep: 222 Cost: 0.2485 Fidelity: 0.8151 Trace: 0.8317\n",
      "Rep: 223 Cost: 0.2650 Fidelity: 0.8093 Trace: 0.8330\n",
      "Rep: 224 Cost: 0.2005 Fidelity: 0.8325 Trace: 0.8400\n",
      "Rep: 225 Cost: 0.1864 Fidelity: 0.8260 Trace: 0.8285\n",
      "Rep: 226 Cost: 0.1979 Fidelity: 0.8250 Trace: 0.8337\n",
      "Rep: 227 Cost: 0.1633 Fidelity: 0.8436 Trace: 0.8488\n",
      "Rep: 228 Cost: 0.1697 Fidelity: 0.8407 Trace: 0.8436\n",
      "Rep: 229 Cost: 0.2106 Fidelity: 0.8464 Trace: 0.8497\n",
      "Rep: 230 Cost: 0.1464 Fidelity: 0.8403 Trace: 0.8429\n",
      "Rep: 231 Cost: 0.1577 Fidelity: 0.8395 Trace: 0.8410\n",
      "Rep: 232 Cost: 0.1899 Fidelity: 0.8404 Trace: 0.8434\n",
      "Rep: 233 Cost: 0.1549 Fidelity: 0.8351 Trace: 0.8376\n",
      "Rep: 234 Cost: 0.1300 Fidelity: 0.8414 Trace: 0.8427\n",
      "Rep: 235 Cost: 0.1571 Fidelity: 0.8496 Trace: 0.8519\n",
      "Rep: 236 Cost: 0.1517 Fidelity: 0.8389 Trace: 0.8394\n",
      "Rep: 237 Cost: 0.1521 Fidelity: 0.8385 Trace: 0.8415\n",
      "Rep: 238 Cost: 0.1484 Fidelity: 0.8444 Trace: 0.8465\n",
      "Rep: 239 Cost: 0.1559 Fidelity: 0.8401 Trace: 0.8409\n",
      "Rep: 240 Cost: 0.1408 Fidelity: 0.8473 Trace: 0.8489\n",
      "Rep: 241 Cost: 0.1199 Fidelity: 0.8445 Trace: 0.8454\n",
      "Rep: 242 Cost: 0.1002 Fidelity: 0.8483 Trace: 0.8486\n",
      "Rep: 243 Cost: 0.1357 Fidelity: 0.8474 Trace: 0.8496\n",
      "Rep: 244 Cost: 0.1228 Fidelity: 0.8493 Trace: 0.8503\n",
      "Rep: 245 Cost: 0.1500 Fidelity: 0.8494 Trace: 0.8516\n",
      "Rep: 246 Cost: 0.1498 Fidelity: 0.8415 Trace: 0.8440\n",
      "Rep: 247 Cost: 0.1445 Fidelity: 0.8473 Trace: 0.8484\n",
      "Rep: 248 Cost: 0.1428 Fidelity: 0.8448 Trace: 0.8457\n",
      "Rep: 249 Cost: 0.1161 Fidelity: 0.8527 Trace: 0.8537\n",
      "Rep: 250 Cost: 0.1022 Fidelity: 0.8521 Trace: 0.8526\n",
      "Rep: 251 Cost: 0.1259 Fidelity: 0.8490 Trace: 0.8509\n",
      "Rep: 252 Cost: 0.1248 Fidelity: 0.8530 Trace: 0.8543\n",
      "Rep: 253 Cost: 0.1571 Fidelity: 0.8431 Trace: 0.8453\n",
      "Rep: 254 Cost: 0.1607 Fidelity: 0.8556 Trace: 0.8568\n",
      "Rep: 255 Cost: 0.1179 Fidelity: 0.8525 Trace: 0.8536\n",
      "Rep: 256 Cost: 0.1306 Fidelity: 0.8512 Trace: 0.8520\n",
      "Rep: 257 Cost: 0.1708 Fidelity: 0.8587 Trace: 0.8595\n",
      "Rep: 258 Cost: 0.1076 Fidelity: 0.8523 Trace: 0.8529\n",
      "Rep: 259 Cost: 0.1594 Fidelity: 0.8487 Trace: 0.8500\n",
      "Rep: 260 Cost: 0.1511 Fidelity: 0.8606 Trace: 0.8618\n",
      "Rep: 261 Cost: 0.1304 Fidelity: 0.8501 Trace: 0.8522\n",
      "Rep: 262 Cost: 0.1237 Fidelity: 0.8519 Trace: 0.8536\n",
      "Rep: 263 Cost: 0.1133 Fidelity: 0.8561 Trace: 0.8573\n",
      "Rep: 264 Cost: 0.1531 Fidelity: 0.8512 Trace: 0.8525\n",
      "Rep: 265 Cost: 0.1366 Fidelity: 0.8620 Trace: 0.8624\n",
      "Rep: 266 Cost: 0.1223 Fidelity: 0.8551 Trace: 0.8567\n",
      "Rep: 267 Cost: 0.1142 Fidelity: 0.8536 Trace: 0.8540\n",
      "Rep: 268 Cost: 0.1926 Fidelity: 0.8541 Trace: 0.8583\n",
      "Rep: 269 Cost: 0.1601 Fidelity: 0.8474 Trace: 0.8521\n",
      "Rep: 270 Cost: 0.0957 Fidelity: 0.8597 Trace: 0.8600\n",
      "Rep: 271 Cost: 0.1061 Fidelity: 0.8598 Trace: 0.8605\n",
      "Rep: 272 Cost: 0.1263 Fidelity: 0.8547 Trace: 0.8563\n",
      "Rep: 273 Cost: 0.1488 Fidelity: 0.8622 Trace: 0.8625\n",
      "Rep: 274 Cost: 0.1587 Fidelity: 0.8475 Trace: 0.8518\n",
      "Rep: 275 Cost: 0.1511 Fidelity: 0.8517 Trace: 0.8560\n",
      "Rep: 276 Cost: 0.1316 Fidelity: 0.8665 Trace: 0.8666\n",
      "Rep: 277 Cost: 0.1650 Fidelity: 0.8457 Trace: 0.8488\n",
      "Rep: 278 Cost: 0.1091 Fidelity: 0.8616 Trace: 0.8620\n",
      "Rep: 279 Cost: 0.1548 Fidelity: 0.8502 Trace: 0.8555\n",
      "Rep: 280 Cost: 0.1698 Fidelity: 0.8443 Trace: 0.8510\n",
      "Rep: 281 Cost: 0.1095 Fidelity: 0.8601 Trace: 0.8608\n",
      "Rep: 282 Cost: 0.2177 Fidelity: 0.8338 Trace: 0.8428\n",
      "Rep: 283 Cost: 0.2449 Fidelity: 0.8343 Trace: 0.8478\n",
      "Rep: 284 Cost: 0.1589 Fidelity: 0.8481 Trace: 0.8535\n",
      "Rep: 285 Cost: 0.1613 Fidelity: 0.8501 Trace: 0.8512\n",
      "Rep: 286 Cost: 0.1832 Fidelity: 0.8407 Trace: 0.8463\n",
      "Rep: 287 Cost: 0.1353 Fidelity: 0.8460 Trace: 0.8483\n",
      "Rep: 288 Cost: 0.1203 Fidelity: 0.8520 Trace: 0.8530\n",
      "Rep: 289 Cost: 0.1780 Fidelity: 0.8591 Trace: 0.8605\n",
      "Rep: 290 Cost: 0.1041 Fidelity: 0.8543 Trace: 0.8547\n",
      "Rep: 291 Cost: 0.1321 Fidelity: 0.8517 Trace: 0.8532\n",
      "Rep: 292 Cost: 0.1482 Fidelity: 0.8652 Trace: 0.8652\n",
      "Rep: 293 Cost: 0.1390 Fidelity: 0.8500 Trace: 0.8532\n",
      "Rep: 294 Cost: 0.1857 Fidelity: 0.8442 Trace: 0.8474\n",
      "Rep: 295 Cost: 0.1396 Fidelity: 0.8605 Trace: 0.8608\n",
      "Rep: 296 Cost: 0.1838 Fidelity: 0.8377 Trace: 0.8448\n",
      "Rep: 297 Cost: 0.1884 Fidelity: 0.8308 Trace: 0.8393\n",
      "Rep: 298 Cost: 0.1562 Fidelity: 0.8507 Trace: 0.8524\n",
      "Rep: 299 Cost: 0.1917 Fidelity: 0.8457 Trace: 0.8497\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.2967e-05\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0228e-04\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6706e-04\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6005e-04\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0628e-06\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.6633e-04\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3743e-04\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8808e-04\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.6409e-06\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2713e-04\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4737e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5299e-04\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4226e-06\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1038e-04\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2228e-04\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2980e-04\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6275e-06\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.3212e-05\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3343e-04\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1006e-04\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0841e-05\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8841e-06\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9392e-05\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3801e-05\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0882e-05\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8105e-07\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9894e-05\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5659e-05\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3342e-05\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5932e-06\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.2053e-06\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2311e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9235e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.8706e-06\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2936e-06\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5693e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1769e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.8166e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6988e-07\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8926e-06\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3720e-05\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7222e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3121e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5216e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4894e-06\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.9628e-06\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4932e-06\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0120e-07\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3821e-06\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5939e-06\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9811e-06\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2683e-07\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1609e-06\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3571e-06\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7210e-06\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0818e-07\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5287e-07\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7584e-06\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0979e-06\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6104e-07\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1577e-08\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7628e-07\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4226e-06\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.2448e-07\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9662e-08\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5754e-07\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3968e-07\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1726e-07\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4598e-07\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.6076e-08\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.3937e-07\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.3933e-07\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0007e-07\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5378e-09\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9911e-07\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.6435e-07\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0702e-07\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0452e-08\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0820e-08\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1889e-07\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7919e-07\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2261e-08\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7996e-08\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1847e-07\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3561e-07\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4578e-08\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1081e-09\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7692e-08\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.3561e-08\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6570e-08\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7773e-10\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3501e-08\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8790e-08\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.1622e-08\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9137e-09\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6983e-09\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3817e-08\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2645e-08\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.9560e-09\n",
      "[[-0.34459    -0.49320465]]\n",
      "[1. 0. 0.]\n",
      "Rep: 0 Cost: 1.3035 Fidelity: 0.0061 Trace: 0.0497\n",
      "Rep: 1 Cost: 1.2952 Fidelity: 0.0070 Trace: 0.0437\n",
      "Rep: 2 Cost: 1.1458 Fidelity: 0.0114 Trace: 0.0182\n",
      "Rep: 3 Cost: 1.1968 Fidelity: 0.0087 Trace: 0.0184\n",
      "Rep: 4 Cost: 1.0723 Fidelity: 0.0125 Trace: 0.0146\n",
      "Rep: 5 Cost: 0.9989 Fidelity: 0.0091 Trace: 0.0138\n",
      "Rep: 6 Cost: 1.0008 Fidelity: 0.0081 Trace: 0.0103\n",
      "Rep: 7 Cost: 1.0353 Fidelity: 0.0072 Trace: 0.0099\n",
      "Rep: 8 Cost: 1.0039 Fidelity: 0.0071 Trace: 0.0088\n",
      "Rep: 9 Cost: 0.9816 Fidelity: 0.0062 Trace: 0.0077\n",
      "Rep: 10 Cost: 0.9583 Fidelity: 0.0065 Trace: 0.0072\n",
      "Rep: 11 Cost: 0.9801 Fidelity: 0.0069 Trace: 0.0077\n",
      "Rep: 12 Cost: 0.9838 Fidelity: 0.0067 Trace: 0.0075\n",
      "Rep: 13 Cost: 0.9652 Fidelity: 0.0065 Trace: 0.0075\n",
      "Rep: 14 Cost: 0.9526 Fidelity: 0.0072 Trace: 0.0083\n",
      "Rep: 15 Cost: 0.9449 Fidelity: 0.0086 Trace: 0.0093\n",
      "Rep: 16 Cost: 0.9303 Fidelity: 0.0108 Trace: 0.0109\n",
      "Rep: 17 Cost: 0.9257 Fidelity: 0.0122 Trace: 0.0127\n",
      "Rep: 18 Cost: 0.9196 Fidelity: 0.0151 Trace: 0.0164\n",
      "Rep: 19 Cost: 0.9133 Fidelity: 0.0203 Trace: 0.0213\n",
      "Rep: 20 Cost: 0.8924 Fidelity: 0.0263 Trace: 0.0268\n",
      "Rep: 21 Cost: 0.8529 Fidelity: 0.0310 Trace: 0.0315\n",
      "Rep: 22 Cost: 0.8784 Fidelity: 0.0351 Trace: 0.0369\n",
      "Rep: 23 Cost: 0.8597 Fidelity: 0.0419 Trace: 0.0435\n",
      "Rep: 24 Cost: 0.8185 Fidelity: 0.0502 Trace: 0.0513\n",
      "Rep: 25 Cost: 0.7956 Fidelity: 0.0571 Trace: 0.0578\n",
      "Rep: 26 Cost: 0.8263 Fidelity: 0.0610 Trace: 0.0634\n",
      "Rep: 27 Cost: 0.7761 Fidelity: 0.0709 Trace: 0.0717\n",
      "Rep: 28 Cost: 0.7895 Fidelity: 0.0745 Trace: 0.0764\n",
      "Rep: 29 Cost: 0.7905 Fidelity: 0.0786 Trace: 0.0808\n",
      "Rep: 30 Cost: 0.7258 Fidelity: 0.0897 Trace: 0.0898\n",
      "Rep: 31 Cost: 0.7632 Fidelity: 0.0926 Trace: 0.0948\n",
      "Rep: 32 Cost: 0.7263 Fidelity: 0.0998 Trace: 0.1006\n",
      "Rep: 33 Cost: 0.7847 Fidelity: 0.0984 Trace: 0.1022\n",
      "Rep: 34 Cost: 0.7661 Fidelity: 0.1013 Trace: 0.1052\n",
      "Rep: 35 Cost: 0.7232 Fidelity: 0.1083 Trace: 0.1091\n",
      "Rep: 36 Cost: 0.7745 Fidelity: 0.1023 Trace: 0.1058\n",
      "Rep: 37 Cost: 0.7489 Fidelity: 0.1097 Trace: 0.1130\n",
      "Rep: 38 Cost: 0.7183 Fidelity: 0.1247 Trace: 0.1265\n",
      "Rep: 39 Cost: 0.7695 Fidelity: 0.1142 Trace: 0.1198\n",
      "Rep: 40 Cost: 0.7745 Fidelity: 0.1120 Trace: 0.1178\n",
      "Rep: 41 Cost: 0.7010 Fidelity: 0.1252 Trace: 0.1267\n",
      "Rep: 42 Cost: 0.7293 Fidelity: 0.1249 Trace: 0.1281\n",
      "Rep: 43 Cost: 0.7614 Fidelity: 0.1202 Trace: 0.1260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 44 Cost: 0.6922 Fidelity: 0.1315 Trace: 0.1333\n",
      "Rep: 45 Cost: 0.6628 Fidelity: 0.1401 Trace: 0.1408\n",
      "Rep: 46 Cost: 0.6994 Fidelity: 0.1387 Trace: 0.1413\n",
      "Rep: 47 Cost: 0.6764 Fidelity: 0.1445 Trace: 0.1460\n",
      "Rep: 48 Cost: 0.6715 Fidelity: 0.1515 Trace: 0.1534\n",
      "Rep: 49 Cost: 0.7014 Fidelity: 0.1531 Trace: 0.1568\n",
      "Rep: 50 Cost: 0.6579 Fidelity: 0.1587 Trace: 0.1604\n",
      "Rep: 51 Cost: 0.6732 Fidelity: 0.1530 Trace: 0.1544\n",
      "Rep: 52 Cost: 0.6609 Fidelity: 0.1580 Trace: 0.1596\n",
      "Rep: 53 Cost: 0.6343 Fidelity: 0.1715 Trace: 0.1725\n",
      "Rep: 54 Cost: 0.6559 Fidelity: 0.1726 Trace: 0.1752\n",
      "Rep: 55 Cost: 0.6437 Fidelity: 0.1796 Trace: 0.1823\n",
      "Rep: 56 Cost: 0.6071 Fidelity: 0.1896 Trace: 0.1905\n",
      "Rep: 57 Cost: 0.6113 Fidelity: 0.1924 Trace: 0.1938\n",
      "Rep: 58 Cost: 0.5766 Fidelity: 0.2012 Trace: 0.2015\n",
      "Rep: 59 Cost: 0.5958 Fidelity: 0.2119 Trace: 0.2136\n",
      "Rep: 60 Cost: 0.5704 Fidelity: 0.2250 Trace: 0.2260\n",
      "Rep: 61 Cost: 0.5606 Fidelity: 0.2321 Trace: 0.2332\n",
      "Rep: 62 Cost: 0.5504 Fidelity: 0.2448 Trace: 0.2465\n",
      "Rep: 63 Cost: 0.5370 Fidelity: 0.2645 Trace: 0.2658\n",
      "Rep: 64 Cost: 0.5566 Fidelity: 0.2853 Trace: 0.2894\n",
      "Rep: 65 Cost: 0.4887 Fidelity: 0.3258 Trace: 0.3277\n",
      "Rep: 66 Cost: 0.5126 Fidelity: 0.3477 Trace: 0.3527\n",
      "Rep: 67 Cost: 0.4432 Fidelity: 0.3780 Trace: 0.3798\n",
      "Rep: 68 Cost: 0.4259 Fidelity: 0.4256 Trace: 0.4286\n",
      "Rep: 69 Cost: 0.3929 Fidelity: 0.4898 Trace: 0.4949\n",
      "Rep: 70 Cost: 0.3676 Fidelity: 0.5301 Trace: 0.5352\n",
      "Rep: 71 Cost: 0.3294 Fidelity: 0.5366 Trace: 0.5385\n",
      "Rep: 72 Cost: 0.2968 Fidelity: 0.5907 Trace: 0.5930\n",
      "Rep: 73 Cost: 0.2949 Fidelity: 0.6530 Trace: 0.6583\n",
      "Rep: 74 Cost: 0.2722 Fidelity: 0.6831 Trace: 0.6876\n",
      "Rep: 75 Cost: 0.2501 Fidelity: 0.6949 Trace: 0.6975\n",
      "Rep: 76 Cost: 0.1909 Fidelity: 0.7187 Trace: 0.7195\n",
      "Rep: 77 Cost: 0.2642 Fidelity: 0.7065 Trace: 0.7088\n",
      "Rep: 78 Cost: 0.2130 Fidelity: 0.7436 Trace: 0.7458\n",
      "Rep: 79 Cost: 0.1514 Fidelity: 0.7636 Trace: 0.7639\n",
      "Rep: 80 Cost: 0.3040 Fidelity: 0.7088 Trace: 0.7156\n",
      "Rep: 81 Cost: 0.2034 Fidelity: 0.7891 Trace: 0.7913\n",
      "Rep: 82 Cost: 0.1254 Fidelity: 0.8107 Trace: 0.8110\n",
      "Rep: 83 Cost: 0.2002 Fidelity: 0.8294 Trace: 0.8412\n",
      "Rep: 84 Cost: 0.1641 Fidelity: 0.8528 Trace: 0.8572\n",
      "Rep: 85 Cost: 0.2294 Fidelity: 0.8457 Trace: 0.8581\n",
      "Rep: 86 Cost: 0.1917 Fidelity: 0.8614 Trace: 0.8712\n",
      "Rep: 87 Cost: 0.1942 Fidelity: 0.8840 Trace: 0.8913\n",
      "Rep: 88 Cost: 0.2480 Fidelity: 0.8628 Trace: 0.8713\n",
      "Rep: 89 Cost: 0.1410 Fidelity: 0.8704 Trace: 0.8737\n",
      "Rep: 90 Cost: 0.2418 Fidelity: 0.9085 Trace: 0.9183\n",
      "Rep: 91 Cost: 0.1550 Fidelity: 0.9079 Trace: 0.9149\n",
      "Rep: 92 Cost: 0.1484 Fidelity: 0.9245 Trace: 0.9309\n",
      "Rep: 93 Cost: 0.1549 Fidelity: 0.9382 Trace: 0.9428\n",
      "Rep: 94 Cost: 0.2318 Fidelity: 0.8965 Trace: 0.9049\n",
      "Rep: 95 Cost: 0.1843 Fidelity: 0.8861 Trace: 0.8937\n",
      "Rep: 96 Cost: 0.2180 Fidelity: 0.9256 Trace: 0.9304\n",
      "Rep: 97 Cost: 0.1944 Fidelity: 0.9362 Trace: 0.9450\n",
      "Rep: 98 Cost: 0.1076 Fidelity: 0.9299 Trace: 0.9301\n",
      "Rep: 99 Cost: 0.2452 Fidelity: 0.8612 Trace: 0.8746\n",
      "Rep: 100 Cost: 0.2082 Fidelity: 0.8545 Trace: 0.8679\n",
      "Rep: 101 Cost: 0.1219 Fidelity: 0.9263 Trace: 0.9282\n",
      "Rep: 102 Cost: 0.3237 Fidelity: 0.8930 Trace: 0.9199\n",
      "Rep: 103 Cost: 0.3009 Fidelity: 0.8883 Trace: 0.9200\n",
      "Rep: 104 Cost: 0.2358 Fidelity: 0.9362 Trace: 0.9502\n",
      "Rep: 105 Cost: 0.1747 Fidelity: 0.9280 Trace: 0.9329\n",
      "Rep: 106 Cost: 0.2901 Fidelity: 0.8934 Trace: 0.9135\n",
      "Rep: 107 Cost: 0.2293 Fidelity: 0.9060 Trace: 0.9214\n",
      "Rep: 108 Cost: 0.1945 Fidelity: 0.8784 Trace: 0.8842\n",
      "Rep: 109 Cost: 0.2223 Fidelity: 0.8737 Trace: 0.8815\n",
      "Rep: 110 Cost: 0.1844 Fidelity: 0.9108 Trace: 0.9207\n",
      "Rep: 111 Cost: 0.1548 Fidelity: 0.9122 Trace: 0.9217\n",
      "Rep: 112 Cost: 0.2047 Fidelity: 0.8617 Trace: 0.8692\n",
      "Rep: 113 Cost: 0.1774 Fidelity: 0.8485 Trace: 0.8532\n",
      "Rep: 114 Cost: 0.2063 Fidelity: 0.8648 Trace: 0.8727\n",
      "Rep: 115 Cost: 0.1880 Fidelity: 0.9046 Trace: 0.9152\n",
      "Rep: 116 Cost: 0.0854 Fidelity: 0.9379 Trace: 0.9386\n",
      "Rep: 117 Cost: 0.1705 Fidelity: 0.9060 Trace: 0.9170\n",
      "Rep: 118 Cost: 0.2506 Fidelity: 0.8666 Trace: 0.8881\n",
      "Rep: 119 Cost: 0.2100 Fidelity: 0.8877 Trace: 0.9024\n",
      "Rep: 120 Cost: 0.1185 Fidelity: 0.9292 Trace: 0.9317\n",
      "Rep: 121 Cost: 0.1554 Fidelity: 0.9352 Trace: 0.9431\n",
      "Rep: 122 Cost: 0.2072 Fidelity: 0.9331 Trace: 0.9477\n",
      "Rep: 123 Cost: 0.1287 Fidelity: 0.9361 Trace: 0.9412\n",
      "Rep: 124 Cost: 0.1775 Fidelity: 0.9093 Trace: 0.9137\n",
      "Rep: 125 Cost: 0.1597 Fidelity: 0.9031 Trace: 0.9083\n",
      "Rep: 126 Cost: 0.1431 Fidelity: 0.9367 Trace: 0.9415\n",
      "Rep: 127 Cost: 0.1306 Fidelity: 0.9224 Trace: 0.9264\n",
      "Rep: 128 Cost: 0.1814 Fidelity: 0.9167 Trace: 0.9225\n",
      "Rep: 129 Cost: 0.1345 Fidelity: 0.9400 Trace: 0.9450\n",
      "Rep: 130 Cost: 0.1545 Fidelity: 0.9368 Trace: 0.9437\n",
      "Rep: 131 Cost: 0.1110 Fidelity: 0.9411 Trace: 0.9455\n",
      "Rep: 132 Cost: 0.1391 Fidelity: 0.9548 Trace: 0.9583\n",
      "Rep: 133 Cost: 0.1256 Fidelity: 0.9474 Trace: 0.9528\n",
      "Rep: 134 Cost: 0.1713 Fidelity: 0.9486 Trace: 0.9533\n",
      "Rep: 135 Cost: 0.1336 Fidelity: 0.9359 Trace: 0.9371\n",
      "Rep: 136 Cost: 0.0855 Fidelity: 0.9322 Trace: 0.9333\n",
      "Rep: 137 Cost: 0.1348 Fidelity: 0.9360 Trace: 0.9399\n",
      "Rep: 138 Cost: 0.1008 Fidelity: 0.9510 Trace: 0.9536\n",
      "Rep: 139 Cost: 0.1115 Fidelity: 0.9526 Trace: 0.9547\n",
      "Rep: 140 Cost: 0.1101 Fidelity: 0.9509 Trace: 0.9529\n",
      "Rep: 141 Cost: 0.0751 Fidelity: 0.9515 Trace: 0.9527\n",
      "Rep: 142 Cost: 0.1108 Fidelity: 0.9549 Trace: 0.9561\n",
      "Rep: 143 Cost: 0.0509 Fidelity: 0.9570 Trace: 0.9572\n",
      "Rep: 144 Cost: 0.1531 Fidelity: 0.9310 Trace: 0.9345\n",
      "Rep: 145 Cost: 0.1611 Fidelity: 0.9260 Trace: 0.9306\n",
      "Rep: 146 Cost: 0.1053 Fidelity: 0.9439 Trace: 0.9471\n",
      "Rep: 147 Cost: 0.0957 Fidelity: 0.9491 Trace: 0.9520\n",
      "Rep: 148 Cost: 0.1187 Fidelity: 0.9424 Trace: 0.9463\n",
      "Rep: 149 Cost: 0.0651 Fidelity: 0.9456 Trace: 0.9459\n",
      "Rep: 150 Cost: 0.1220 Fidelity: 0.9267 Trace: 0.9316\n",
      "Rep: 151 Cost: 0.1511 Fidelity: 0.9213 Trace: 0.9296\n",
      "Rep: 152 Cost: 0.1114 Fidelity: 0.9383 Trace: 0.9424\n",
      "Rep: 153 Cost: 0.0694 Fidelity: 0.9552 Trace: 0.9561\n",
      "Rep: 154 Cost: 0.1124 Fidelity: 0.9534 Trace: 0.9554\n",
      "Rep: 155 Cost: 0.0741 Fidelity: 0.9492 Trace: 0.9502\n",
      "Rep: 156 Cost: 0.0862 Fidelity: 0.9462 Trace: 0.9479\n",
      "Rep: 157 Cost: 0.0927 Fidelity: 0.9468 Trace: 0.9493\n",
      "Rep: 158 Cost: 0.0512 Fidelity: 0.9544 Trace: 0.9548\n",
      "Rep: 159 Cost: 0.1199 Fidelity: 0.9566 Trace: 0.9590\n",
      "Rep: 160 Cost: 0.0854 Fidelity: 0.9534 Trace: 0.9548\n",
      "Rep: 161 Cost: 0.0695 Fidelity: 0.9523 Trace: 0.9532\n",
      "Rep: 162 Cost: 0.0453 Fidelity: 0.9581 Trace: 0.9583\n",
      "Rep: 163 Cost: 0.1092 Fidelity: 0.9650 Trace: 0.9687\n",
      "Rep: 164 Cost: 0.1274 Fidelity: 0.9613 Trace: 0.9657\n",
      "Rep: 165 Cost: 0.0677 Fidelity: 0.9620 Trace: 0.9631\n",
      "Rep: 166 Cost: 0.1129 Fidelity: 0.9566 Trace: 0.9596\n",
      "Rep: 167 Cost: 0.1214 Fidelity: 0.9520 Trace: 0.9555\n",
      "Rep: 168 Cost: 0.0894 Fidelity: 0.9514 Trace: 0.9533\n",
      "Rep: 169 Cost: 0.1576 Fidelity: 0.9539 Trace: 0.9595\n",
      "Rep: 170 Cost: 0.1386 Fidelity: 0.9633 Trace: 0.9703\n",
      "Rep: 171 Cost: 0.1232 Fidelity: 0.9558 Trace: 0.9594\n",
      "Rep: 172 Cost: 0.1038 Fidelity: 0.9278 Trace: 0.9298\n",
      "Rep: 173 Cost: 0.1091 Fidelity: 0.9276 Trace: 0.9300\n",
      "Rep: 174 Cost: 0.0677 Fidelity: 0.9571 Trace: 0.9591\n",
      "Rep: 175 Cost: 0.0881 Fidelity: 0.9616 Trace: 0.9637\n",
      "Rep: 176 Cost: 0.1007 Fidelity: 0.9603 Trace: 0.9636\n",
      "Rep: 177 Cost: 0.1128 Fidelity: 0.9626 Trace: 0.9650\n",
      "Rep: 178 Cost: 0.0722 Fidelity: 0.9681 Trace: 0.9694\n",
      "Rep: 179 Cost: 0.1245 Fidelity: 0.9609 Trace: 0.9619\n",
      "Rep: 180 Cost: 0.1248 Fidelity: 0.9471 Trace: 0.9514\n",
      "Rep: 181 Cost: 0.1754 Fidelity: 0.9299 Trace: 0.9413\n",
      "Rep: 182 Cost: 0.0816 Fidelity: 0.9566 Trace: 0.9583\n",
      "Rep: 183 Cost: 0.1950 Fidelity: 0.9520 Trace: 0.9646\n",
      "Rep: 184 Cost: 0.1230 Fidelity: 0.9624 Trace: 0.9708\n",
      "Rep: 185 Cost: 0.1785 Fidelity: 0.9432 Trace: 0.9474\n",
      "Rep: 186 Cost: 0.1982 Fidelity: 0.9127 Trace: 0.9190\n",
      "Rep: 187 Cost: 0.1414 Fidelity: 0.9191 Trace: 0.9242\n",
      "Rep: 188 Cost: 0.1692 Fidelity: 0.9447 Trace: 0.9530\n",
      "Rep: 189 Cost: 0.0636 Fidelity: 0.9664 Trace: 0.9677\n",
      "Rep: 190 Cost: 0.1797 Fidelity: 0.9450 Trace: 0.9536\n",
      "Rep: 191 Cost: 0.2079 Fidelity: 0.9267 Trace: 0.9351\n",
      "Rep: 192 Cost: 0.1357 Fidelity: 0.9137 Trace: 0.9193\n",
      "Rep: 193 Cost: 0.2089 Fidelity: 0.9040 Trace: 0.9115\n",
      "Rep: 194 Cost: 0.1694 Fidelity: 0.9229 Trace: 0.9276\n",
      "Rep: 195 Cost: 0.1656 Fidelity: 0.9200 Trace: 0.9302\n",
      "Rep: 196 Cost: 0.0891 Fidelity: 0.9525 Trace: 0.9530\n",
      "Rep: 197 Cost: 0.1137 Fidelity: 0.9551 Trace: 0.9568\n",
      "Rep: 198 Cost: 0.0718 Fidelity: 0.9442 Trace: 0.9449\n",
      "Rep: 199 Cost: 0.1591 Fidelity: 0.9152 Trace: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep: 200 Cost: 0.1316 Fidelity: 0.9511 Trace: 0.9536\n",
      "Rep: 201 Cost: 0.0518 Fidelity: 0.9642 Trace: 0.9649\n",
      "Rep: 202 Cost: 0.1764 Fidelity: 0.9610 Trace: 0.9673\n",
      "Rep: 203 Cost: 0.1569 Fidelity: 0.9572 Trace: 0.9596\n",
      "Rep: 204 Cost: 0.2230 Fidelity: 0.9028 Trace: 0.9170\n",
      "Rep: 205 Cost: 0.1509 Fidelity: 0.9414 Trace: 0.9466\n",
      "Rep: 206 Cost: 0.2409 Fidelity: 0.9238 Trace: 0.9520\n",
      "Rep: 207 Cost: 0.2764 Fidelity: 0.9113 Trace: 0.9469\n",
      "Rep: 208 Cost: 0.0810 Fidelity: 0.9743 Trace: 0.9759\n",
      "Rep: 209 Cost: 0.2461 Fidelity: 0.9201 Trace: 0.9466\n",
      "Rep: 210 Cost: 0.2801 Fidelity: 0.9198 Trace: 0.9508\n",
      "Rep: 211 Cost: 0.1569 Fidelity: 0.9705 Trace: 0.9759\n",
      "Rep: 212 Cost: 0.2250 Fidelity: 0.9148 Trace: 0.9287\n",
      "Rep: 213 Cost: 0.2393 Fidelity: 0.8893 Trace: 0.9071\n",
      "Rep: 214 Cost: 0.1623 Fidelity: 0.9481 Trace: 0.9499\n",
      "Rep: 215 Cost: 0.2045 Fidelity: 0.9537 Trace: 0.9585\n",
      "Rep: 216 Cost: 0.1060 Fidelity: 0.9469 Trace: 0.9487\n",
      "Rep: 217 Cost: 0.1376 Fidelity: 0.9391 Trace: 0.9437\n",
      "Rep: 218 Cost: 0.1333 Fidelity: 0.9539 Trace: 0.9580\n",
      "Rep: 219 Cost: 0.1360 Fidelity: 0.9632 Trace: 0.9693\n",
      "Rep: 220 Cost: 0.0749 Fidelity: 0.9681 Trace: 0.9701\n",
      "Rep: 221 Cost: 0.1615 Fidelity: 0.9296 Trace: 0.9407\n",
      "Rep: 222 Cost: 0.1495 Fidelity: 0.9397 Trace: 0.9486\n",
      "Rep: 223 Cost: 0.0896 Fidelity: 0.9694 Trace: 0.9708\n",
      "Rep: 224 Cost: 0.0699 Fidelity: 0.9683 Trace: 0.9689\n",
      "Rep: 225 Cost: 0.1522 Fidelity: 0.9444 Trace: 0.9524\n",
      "Rep: 226 Cost: 0.1178 Fidelity: 0.9454 Trace: 0.9523\n",
      "Rep: 227 Cost: 0.0921 Fidelity: 0.9608 Trace: 0.9627\n",
      "Rep: 228 Cost: 0.1504 Fidelity: 0.9439 Trace: 0.9530\n",
      "Rep: 229 Cost: 0.1258 Fidelity: 0.9526 Trace: 0.9592\n",
      "Rep: 230 Cost: 0.1034 Fidelity: 0.9627 Trace: 0.9645\n",
      "Rep: 231 Cost: 0.0685 Fidelity: 0.9598 Trace: 0.9612\n",
      "Rep: 232 Cost: 0.1531 Fidelity: 0.9665 Trace: 0.9716\n",
      "Rep: 233 Cost: 0.1457 Fidelity: 0.9684 Trace: 0.9727\n",
      "Rep: 234 Cost: 0.0741 Fidelity: 0.9642 Trace: 0.9661\n",
      "Rep: 235 Cost: 0.1098 Fidelity: 0.9585 Trace: 0.9613\n",
      "Rep: 236 Cost: 0.1052 Fidelity: 0.9655 Trace: 0.9684\n",
      "Rep: 237 Cost: 0.0827 Fidelity: 0.9646 Trace: 0.9671\n",
      "Rep: 238 Cost: 0.0890 Fidelity: 0.9597 Trace: 0.9620\n",
      "Rep: 239 Cost: 0.0884 Fidelity: 0.9564 Trace: 0.9586\n",
      "Rep: 240 Cost: 0.0666 Fidelity: 0.9633 Trace: 0.9651\n",
      "Rep: 241 Cost: 0.0842 Fidelity: 0.9729 Trace: 0.9745\n",
      "Rep: 242 Cost: 0.0572 Fidelity: 0.9736 Trace: 0.9747\n",
      "Rep: 243 Cost: 0.1078 Fidelity: 0.9584 Trace: 0.9606\n",
      "Rep: 244 Cost: 0.0882 Fidelity: 0.9628 Trace: 0.9646\n",
      "Rep: 245 Cost: 0.0699 Fidelity: 0.9733 Trace: 0.9756\n",
      "Rep: 246 Cost: 0.0837 Fidelity: 0.9630 Trace: 0.9655\n",
      "Rep: 247 Cost: 0.0646 Fidelity: 0.9650 Trace: 0.9658\n",
      "Rep: 248 Cost: 0.0985 Fidelity: 0.9677 Trace: 0.9717\n",
      "Rep: 249 Cost: 0.0810 Fidelity: 0.9725 Trace: 0.9734\n",
      "Rep: 250 Cost: 0.1482 Fidelity: 0.9641 Trace: 0.9673\n",
      "Rep: 251 Cost: 0.0698 Fidelity: 0.9740 Trace: 0.9748\n",
      "Rep: 252 Cost: 0.1909 Fidelity: 0.9587 Trace: 0.9676\n",
      "Rep: 253 Cost: 0.2027 Fidelity: 0.9571 Trace: 0.9659\n",
      "Rep: 254 Cost: 0.0867 Fidelity: 0.9630 Trace: 0.9638\n",
      "Rep: 255 Cost: 0.1481 Fidelity: 0.9435 Trace: 0.9495\n",
      "Rep: 256 Cost: 0.1542 Fidelity: 0.9433 Trace: 0.9482\n",
      "Rep: 257 Cost: 0.1010 Fidelity: 0.9464 Trace: 0.9482\n",
      "Rep: 258 Cost: 0.1000 Fidelity: 0.9380 Trace: 0.9410\n",
      "Rep: 259 Cost: 0.1244 Fidelity: 0.9426 Trace: 0.9470\n",
      "Rep: 260 Cost: 0.1015 Fidelity: 0.9583 Trace: 0.9601\n",
      "Rep: 261 Cost: 0.0876 Fidelity: 0.9605 Trace: 0.9632\n",
      "Rep: 262 Cost: 0.1077 Fidelity: 0.9622 Trace: 0.9643\n",
      "Rep: 263 Cost: 0.1111 Fidelity: 0.9603 Trace: 0.9606\n",
      "Rep: 264 Cost: 0.0950 Fidelity: 0.9607 Trace: 0.9623\n",
      "Rep: 265 Cost: 0.0737 Fidelity: 0.9632 Trace: 0.9644\n",
      "Rep: 266 Cost: 0.1067 Fidelity: 0.9502 Trace: 0.9532\n",
      "Rep: 267 Cost: 0.0877 Fidelity: 0.9494 Trace: 0.9521\n",
      "Rep: 268 Cost: 0.0831 Fidelity: 0.9526 Trace: 0.9542\n",
      "Rep: 269 Cost: 0.0859 Fidelity: 0.9524 Trace: 0.9535\n",
      "Rep: 270 Cost: 0.1243 Fidelity: 0.9538 Trace: 0.9568\n",
      "Rep: 271 Cost: 0.0564 Fidelity: 0.9670 Trace: 0.9683\n",
      "Rep: 272 Cost: 0.1442 Fidelity: 0.9715 Trace: 0.9756\n",
      "Rep: 273 Cost: 0.1497 Fidelity: 0.9701 Trace: 0.9739\n",
      "Rep: 274 Cost: 0.0789 Fidelity: 0.9631 Trace: 0.9649\n",
      "Rep: 275 Cost: 0.1119 Fidelity: 0.9673 Trace: 0.9697\n",
      "Rep: 276 Cost: 0.0965 Fidelity: 0.9732 Trace: 0.9751\n",
      "Rep: 277 Cost: 0.1082 Fidelity: 0.9641 Trace: 0.9683\n",
      "Rep: 278 Cost: 0.0549 Fidelity: 0.9678 Trace: 0.9686\n",
      "Rep: 279 Cost: 0.0475 Fidelity: 0.9697 Trace: 0.9701\n",
      "Rep: 280 Cost: 0.0711 Fidelity: 0.9731 Trace: 0.9747\n",
      "Rep: 281 Cost: 0.0687 Fidelity: 0.9670 Trace: 0.9675\n",
      "Rep: 282 Cost: 0.1285 Fidelity: 0.9646 Trace: 0.9683\n",
      "Rep: 283 Cost: 0.0620 Fidelity: 0.9781 Trace: 0.9795\n",
      "Rep: 284 Cost: 0.1015 Fidelity: 0.9791 Trace: 0.9812\n",
      "Rep: 285 Cost: 0.0458 Fidelity: 0.9782 Trace: 0.9789\n",
      "Rep: 286 Cost: 0.0910 Fidelity: 0.9687 Trace: 0.9704\n",
      "Rep: 287 Cost: 0.0389 Fidelity: 0.9791 Trace: 0.9796\n",
      "Rep: 288 Cost: 0.0380 Fidelity: 0.9760 Trace: 0.9763\n",
      "Rep: 289 Cost: 0.0599 Fidelity: 0.9760 Trace: 0.9770\n",
      "Rep: 290 Cost: 0.0463 Fidelity: 0.9791 Trace: 0.9798\n",
      "Rep: 291 Cost: 0.0556 Fidelity: 0.9758 Trace: 0.9766\n",
      "Rep: 292 Cost: 0.0607 Fidelity: 0.9786 Trace: 0.9797\n",
      "Rep: 293 Cost: 0.0432 Fidelity: 0.9757 Trace: 0.9761\n",
      "Rep: 294 Cost: 0.0682 Fidelity: 0.9792 Trace: 0.9809\n",
      "Rep: 295 Cost: 0.0520 Fidelity: 0.9815 Trace: 0.9822\n",
      "Rep: 296 Cost: 0.0374 Fidelity: 0.9823 Trace: 0.9828\n",
      "Rep: 297 Cost: 0.0371 Fidelity: 0.9835 Trace: 0.9842\n",
      "Rep: 298 Cost: 0.0526 Fidelity: 0.9823 Trace: 0.9826\n",
      "Rep: 299 Cost: 0.0542 Fidelity: 0.9837 Trace: 0.9848\n"
     ]
    }
   ],
   "source": [
    "fid_progress = [[], [], [], [], [], [], []]\n",
    "loss_progress = [[], [], [], [], [], [], []]\n",
    "\n",
    "\n",
    "for j in range(sctm_iterations):\n",
    "    history = autoencoder.fit(x_train, x_train,epochs=classical_epochs)\n",
    "    \n",
    "    encoded_st = autoencoder.encoder(np.array([target_state])).numpy()\n",
    "    print(encoded_st)\n",
    "    print(target_state)\n",
    "    #decoded_st = autoencoder.decoder(encoded_st).numpy()\n",
    "    \n",
    "    def layer(params, q):\n",
    "        \"\"\"CV quantum neural network layer acting on ``N`` modes.\n",
    "\n",
    "        Args:\n",
    "            params (list[float]): list of length ``2*(max(1, N-1) + N**2 + n)`` containing\n",
    "                the number of parameters for the layer\n",
    "            q (list[RegRef]): list of Strawberry Fields quantum registers the layer\n",
    "                is to be applied to\n",
    "        \"\"\"\n",
    "        ops.Dgate(tf.clip_by_value(encoded_st[0][0], clip_value_min = -alpha_clip, clip_value_max = alpha_clip), encoded_st[0][1]) | q[0]\n",
    "\n",
    "        N = len(q)\n",
    "        M = int(N * (N - 1)) + max(1, N - 1)\n",
    "\n",
    "        rphi = params[-N+1:]\n",
    "        s = params[M:M+N]\n",
    "        dr = params[2*M+N:2*M+2*N]\n",
    "        dp = params[2*M+2*N:2*M+3*N]\n",
    "        k = params[2*M+3*N:2*M+4*N]\n",
    "\n",
    "        ops.Rgate(rphi[0]) | q[0]\n",
    "\n",
    "        for i in range(N):\n",
    "            ops.Sgate(s[i]) | q[i]\n",
    "\n",
    "        ops.Rgate(rphi[0]) | q[0]\n",
    "\n",
    "        for i in range(N):\n",
    "            ops.Dgate(dr[i], dp[i]) | q[i]\n",
    "            ops.Kgate(k[i]) | q[i]\n",
    "    \n",
    "    # initialize engine and program\n",
    "    eng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": cutoff_dim})\n",
    "    qnn = sf.Program(modes)\n",
    "\n",
    "    # initialize QNN weights\n",
    "    weights = init_weights(modes, Qlayers) # our TensorFlow weights\n",
    "    num_params = np.prod(weights.shape)   # total number of parameters in our model\n",
    "\n",
    "    # Create array of Strawberry Fields symbolic gate arguments, matching\n",
    "    # the size of the weights Variable.\n",
    "    sf_params = np.arange(num_params).reshape(weights.shape).astype(np.str)\n",
    "    sf_params = np.array([qnn.params(*i) for i in sf_params])\n",
    "\n",
    "\n",
    "    # Construct the symbolic Strawberry Fields program by\n",
    "    # looping and applying layers to the program.\n",
    "    with qnn.context as q:\n",
    "        for k in range(Qlayers):\n",
    "            layer(sf_params[k], q)\n",
    "            \n",
    "    def cost(weights):\n",
    "        # Create a dictionary mapping from the names of the Strawberry Fields\n",
    "        # free parameters to the TensorFlow weight values.\n",
    "        mapping = {p.name: w for p, w in zip(sf_params.flatten(), tf.reshape(weights, [-1]))}\n",
    "\n",
    "        # Run engine\n",
    "        state = eng.run(qnn, args=mapping).state\n",
    "\n",
    "        # Extract the statevector\n",
    "        ket = state.ket()\n",
    "\n",
    "        # Compute the fidelity between the output statevector\n",
    "        # and the target state.\n",
    "        fidelity = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state)) ** 2\n",
    "\n",
    "        # Objective function to minimize\n",
    "        #cost = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state) - 1)\n",
    "        #return cost, fidelity, ket\n",
    "        # Instead of the Cost function, maybe it is better to break it down to components\n",
    "        # at least, when the Fock basis is insufficent, it will be visible\n",
    "        difference = tf.reduce_sum(tf.abs(ket - target_state))\n",
    "        fidelity = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state)) ** 2\n",
    "        return difference, fidelity, ket, tf.math.real(state.trace())\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    fidp = []\n",
    "    lossp = []\n",
    "    best_fid = 0\n",
    "\n",
    "    for i in range(reps):\n",
    "        # reset the engine if it has already been executed\n",
    "        if eng.run_progs:\n",
    "            eng.reset()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, fid, ket, trace = cost(weights)\n",
    "\n",
    "        # Stores fidelity at each step\n",
    "        fidp.append(fid.numpy())\n",
    "        lossp.append(loss)\n",
    "\n",
    "        if fid > best_fid:\n",
    "            # store the new best fidelity and best state\n",
    "            best_fid = fid.numpy()\n",
    "            learnt_state = ket.numpy()\n",
    "\n",
    "        # one repetition of the optimization\n",
    "        gradients = tape.gradient(loss, weights)\n",
    "        opt.apply_gradients(zip([gradients], [weights]))\n",
    "         # Prints progress at every rep\n",
    "        if i % 1 == 0:\n",
    "            print(\"Rep: {} Cost: {:.4f} Fidelity: {:.4f} Trace: {:.4f}\".format(i, loss, fid, trace))\n",
    "            \n",
    "    fid_progress[j] = fidp\n",
    "    loss_progress[j] = lossp\n",
    "\n",
    "    x_train = np.array([learnt_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3840b76",
   "metadata": {},
   "source": [
    "### Produce fidelity plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12), dpi = 100)\n",
    "\n",
    "# Set tick font size\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "\n",
    "for i in range(sctm_iterations):\n",
    "    plt.plot(fid_progress[i], label = \"Fidelity of iteration \" + str(i), linewidth = 3)\n",
    "\n",
    "plt.ylabel(\"Fidelity\", fontsize = '20')\n",
    "ax.yaxis.set_ticks(np.arange(0, 1, 0.4))\n",
    "plt.xlabel(\"Epoch\", fontsize = '20')\n",
    "ax.xaxis.set_ticks(np.arange(0, reps, 40))\n",
    "plt.title('SCTM Fidelity', fontsize = '24')\n",
    "plt.legend()\n",
    "plt.savefig('SCTM_fidelity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e783394",
   "metadata": {},
   "source": [
    "### Produce loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12), dpi = 100)\n",
    "\n",
    "# Set tick font size\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "\n",
    "for i in range(sctm_iterations):\n",
    "    plt.plot(loss_progress[i], label = \"Training loss of iteration \" + str(i), linewidth = 3)\n",
    "\n",
    "plt.ylabel(\"Training Loss\", fontsize = '20')\n",
    "ax.yaxis.set_ticks(np.arange(0, 1, 0.4))\n",
    "plt.xlabel(\"Epoch\", fontsize = '20')\n",
    "ax.xaxis.set_ticks(np.arange(0, reps, 40))\n",
    "plt.title('SCTM Training Loss', fontsize = '24')\n",
    "plt.legend()\n",
    "plt.savefig('SCTM_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284197f1",
   "metadata": {},
   "source": [
    "### Definition of the function that plots Wigner functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def wigner(rho):\n",
    "    \"\"\"This code is a modified version of the 'iterative' method\n",
    "    of the wigner function provided in QuTiP, which is released\n",
    "    under the BSD license, with the following copyright notice:\n",
    "\n",
    "    Copyright (C) 2011 and later, P.D. Nation, J.R. Johansson,\n",
    "    A.J.G. Pitchford, C. Granade, and A.L. Grimsmo.\n",
    "\n",
    "    All rights reserved.\"\"\"\n",
    "    import copy\n",
    "\n",
    "    # Domain parameter for Wigner function plots\n",
    "    l = 5.0\n",
    "    cutoff = rho.shape[0]\n",
    "\n",
    "    # Creates 2D grid for Wigner function plots\n",
    "    x = np.linspace(-l, l, 100)\n",
    "    p = np.linspace(-l, l, 100)\n",
    "\n",
    "    Q, P = np.meshgrid(x, p)\n",
    "    A = (Q + P * 1.0j) / (2 * np.sqrt(2 / 2))\n",
    "\n",
    "    Wlist = np.array([np.zeros(np.shape(A), dtype=complex) for k in range(cutoff)])\n",
    "\n",
    "    # Wigner function for |0><0|\n",
    "    Wlist[0] = np.exp(-2.0 * np.abs(A) ** 2) / np.pi\n",
    "\n",
    "    # W = rho(0,0)W(|0><0|)\n",
    "    W = np.real(rho[0, 0]) * np.real(Wlist[0])\n",
    "\n",
    "    for n in range(1, cutoff):\n",
    "        Wlist[n] = (2.0 * A * Wlist[n - 1]) / np.sqrt(n)\n",
    "        W += 2 * np.real(rho[0, n] * Wlist[n])\n",
    "\n",
    "    for m in range(1, cutoff):\n",
    "        temp = copy.copy(Wlist[m])\n",
    "        # Wlist[m] = Wigner function for |m><m|\n",
    "        Wlist[m] = (2 * np.conj(A) * temp - np.sqrt(m) * Wlist[m - 1]) / np.sqrt(m)\n",
    "\n",
    "        # W += rho(m,m)W(|m><m|)\n",
    "        W += np.real(rho[m, m] * Wlist[m])\n",
    "\n",
    "        for n in range(m + 1, cutoff):\n",
    "            temp2 = (2 * A * Wlist[n - 1] - np.sqrt(m) * temp) / np.sqrt(n)\n",
    "            temp = copy.copy(Wlist[n])\n",
    "            # Wlist[n] = Wigner function for |m><n|\n",
    "            Wlist[n] = temp2\n",
    "\n",
    "            # W += rho(m,n)W(|m><n|) + rho(n,m)W(|n><m|)\n",
    "            W += 2 * np.real(rho[m, n] * Wlist[n])\n",
    "\n",
    "    return Q, P, W / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784a383",
   "metadata": {},
   "source": [
    "### Obtain the target and learnt states from the quantum decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aef1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_target = np.outer(target_state, target_state.conj())\n",
    "rho_learnt = np.outer(learnt_state, learnt_state.conj())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c178d",
   "metadata": {},
   "source": [
    "### Plot the target state as a Wigner function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "X, P, W = wigner(rho_target)\n",
    "ax.plot_surface(X, P, W, cmap=\"Spectral\", lw=0.5, rstride=1, cstride=1)\n",
    "ax.contour(X, P, W, 10, cmap=\"Spectral\", linestyles=\"solid\", offset=-0.17)\n",
    "ax.set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983923f",
   "metadata": {},
   "source": [
    "### Plot the learnt state as a Wigner function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b78c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "X, P, W = wigner(rho_learnt)\n",
    "ax.plot_surface(X, P, W, cmap=\"Spectral\", lw=0.5, rstride=1, cstride=1)\n",
    "ax.contour(X, P, W, 10, cmap=\"Spectral\", linestyles=\"solid\", offset=-0.17)\n",
    "ax.set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c46f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e9c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
